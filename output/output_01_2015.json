{
    "time_window": "01/2015",
    "articles": [
        {
            "article_published": "01/2015",
            "source": "https://techcrunch.com/2015/01/31/silicon-valley-reinvents-the-invisible-hand/",
            "relevance_score": 0.10686248540878296,
            "content": "Jobs are the lifeblood of the economy. Whether self-employed or company-employed, workers rely on steady incomes to consume goods and pay for all of the necessities of life. When jobs are plentiful, friction decreases between job applicants and employers, ensuring that income is earned earlier and more frequently. Adam Smith\u2019s invisible hand is the guiding force of the market in a capitalist system. By allowing everyone to make independent decisions in whatever way they define their self-interest, the market is presumed to automatically clear, moving goods and services from those who can offer them to those who most desire them. It\u2019s ultimately a statement about market efficiency. That same invisible hand also rules the labor markets. Salaries are commensurate with skills, and employers compete for the workers they desire. If one company is able to create more profit from a talented individual than another company, then they theoretically will offer greater rewards (since they can) and poach that worker away. That\u2019s the theory anyway. In reality, labor markets are filled with a viscous friction that continues to bedevil hiring managers and workers alike. Even in the best of times, it can take weeks from the moment someone starts seeking a job until they have secured a new income (and of course, it can take another several weeks for the first paycheck to actually arrive). In worse times, those weeks can quickly drag out to months and even years, creating a permanent class of unemployed workers with little hope of securing a job. Labor markets are filled with friction, which means that losing a job can easily destroy the financial security of an entire family. To get a sense of this friction, just note how easy it is to buy a product versus hiring an employee, and more importantly, how much easier the former has become over the years compared to the latter. Buying a product from Amazon today can be as simple as a single click, and it arrives on time with great customer service in case something goes wrong. Labor markets seem like they are stuck in Adam Smith\u2019s era. We still apply for jobs the way we did decades ago, with resumes and cover sheets. We still conduct interviews, even though there is increasing proof not to mention plentiful anecdata that such practices are not a good judge of a worker\u2019s abilities. Frankly, the biggest improvement to the process has been the use of machine learning to read resumes, but that has only led to keyword gaming and other useless activities which fail to actually make hiring more effective. Put simply, the invisible hand has been broken in the labor markets for years, even if we haven\u2019t fully realized it. The reason is that labor markets are thin, with few available workers and even fewer employers. That\u2019s why algorithmic marketplaces are so important. Startups are completely transforming the nature of labor markets, changing our notions of what employment can be and how income security functions. In the process, they are not just making our economy more efficient, but improving the lives of workers. Job Security and Thick Labor Markets To understand the context for these changes to our labor markets, we have to peer back into history to understand their development. For centuries, work revolved around the land. Nearly everyone in Europe following the fall of the Roman Empire was related to the land, whether working it directly, or in the case of feudal lords, sitting and waiting for the land to be tilled. What if looking at the past is precisely the wrong way to solve the problem though? What would happen if we actually had thick labor marketplaces without the friction that exists today? Our modern notion of income security is completely alien in this context. Income was the harvest, and variations in weather patterns could ensure a bountiful crop or a disastrous winter. There were some means of absorbing these shocks, but ultimately the market could force even the most careful families to succumb to its will. The growth in cities started to change this dynamic. For the first time, a worker was able to move between jobs, freed from a direct connection to the soil. These markets were rudimentary, and they remained so for centuries until the rapid growth of the industrial age in the early 1800s transformed them. From then on, we observe steady progress toward our modern notion of labor. What are some of those qualities? A safe work environment is one key component of the modern workplace, as well as various mechanisms for rest like weekends and vacation. But probably the most important element of the modern development of labor markets is the protections afforded workers from being fired. Whether tenure laws, anti-discrimination laws, human resources rules, lawsuits, or other institutions, safety from job termination is arguably one of the most fought after rights of workers over the past two centuries of development. I\u2019ve already alluded to the reason for this focus. Labor markets are filled with friction, which means that losing a job can easily destroy the financial security of an entire family. Since it can take weeks or even months to find new employment, workers desire to stay in their current jobs as long as possible. As employment protections have decreased in the United States over the past few decades, families have compensated by relying on dual-incomes to ensure steady finances. In short, job security in our current labor markets is about avoiding job switches. Fundamentally, the challenge is one of the \u201cthickness\u201d of the market. Market thickness can be defined as the simultaneous number of buyers and sellers in a market. When there are only a handful of consumers and producers, it is hard for prices to be set and thus, for transactions to be completed. As both sides increase in volume, markets have more information, and thus are able to operate more efficiently and more rapidly. This is one of the reasons why it is easier to buy clothes than a house \u2013 the high price of houses and thus limited number of transactions makes it difficult for buyers and sellers to understand the market and make a deal. This is even more acute in the labor market. Outside of a handful of tech hubs where employee turnover is rapid, hiring is not at all robust. That makes labor markets quite thin, with limited numbers of employees moving from company to company to provide information on current market conditions to other participants. In these locales across the United States and really the whole world, the entire goal is to hold on to employment as tightly as possible and avoid the labor market as much as possible. For all of our progress, so little has changed. What Happens If We Have Thick Labor Markets? Actually, there has been some progress, most of it terrible. If you want to see what a somewhat thick labor market looks like, take a look at the service jobs in areas like food preparation and building maintenance. Due to innovations like franchising and increased usage of independent contractor terms for employment, these workers have none of the job protections of their predecessors and all the downside of friction-filled labor marketplaces. Employers have the upper hand in these transactions, and that has led to a disintegration of living wages and stable work arrangements. The schedules for service workers are anything but stable from week-to-week, creating vast problems for scheduling multiple jobs and child care for families. That has led to even more focus on holding onto a job no matter what the costs are, since switching jobs when you are living on the razor\u2019s edge is impossible without financial disaster. We can see these trends reflected in all of the statistics about inequality that have been discussed in the media and academia the past few years. Middle-class jobs that used to provide job security and decent pay are increasingly being replaced by low-wage contract labor without any job protections at all. Wealth is being accumulated by a small percentage of the population, while the sons and daughters of middle-class families discover that the hollowing out of the American economy is going to force them to slide down the economic ladder. That has led to movements like Occupy Wall Street to demand changes to the ways that companies handle workers. Looking back at the 1960s and 70s, there is an increasing desire to return to the way things were, with secure employment in companies with greater job protections (of course, with women actually in the workplace this time). The cry from all of these movements is the same and it is clear: they want greater protection from the vagaries of the market, and they want everyone to be placed on an equal footing in the economy. That is one approach, but what if looking at the past is precisely the wrong way to solve the problem though? What would happen if we actually had thick labor marketplaces without the friction that exists today? Take, for instance, the service worker who can\u2019t risk changing their job because they need their wages to arrive absolutely on time otherwise everything will fall apart. Since their shift schedule is changing so often, this individual also can\u2019t switch jobs because they can\u2019t find the time to interview. With a thick labor market, suddenly this person is able to seek out employment and instantly know who is hiring. Wages would be clearly available since information in the market is plentiful, and shift schedules would have to be equally spelled out. If the market was thick enough, it would even be possible to switch jobs within a single day. The ability of a software engineer in Silicon Valley to be working at Google in the morning and then switch to Facebook by the afternoon would be available to everyone in the market. It is our current system that is dehumanizing, whether it is applying for hundreds of jobs and never receiving a response or doing stupid exercises during interviews that are completely unrelated to the job that we applied for in the first place. For the first time, those service workers who have been powerless to resist their employers\u2019 caprices would suddenly find that they have new leverage to use: they can actually leave since the market can provide them immediate employment. Their employer would be forced to comprehend their loss of power, or otherwise they may soon find that they have no workers at all. Suddenly, job security isn\u2019t about avoiding the market, but rather fully embracing it. Knowing that you always have another job available can provide a psychological relief that no rule or law will ever fully offer. That also ensures that those who leave the market, say for maternity leave, can easily find their way back into the market again. It isn\u2019t just job security that benefits with this development, but also job flexibility. Thick labor markets can also facilitate greater flexibility with shift schedules and work hours. A worker who wants to take a Wednesday off can now just choose to do so, knowing that the market will work its invisible hand to find another worker to take the shift. Similarly, workers with an extra hour or two may be able to find something to do productively in that time, providing them an extra bit of income. All of this sounds great, but there is an obvious concern. Markets only work when the price mechanism is allowed to be fully determined by market dynamics, and that means that wages will fluctuate until an equilibrium is established. Doesn\u2019t that mean wages will plunge as a great mass of workers seek out these newly accessible service jobs that were previously difficult to get due to market friction? It is an obvious criticism, and an important one, but it makes one fatal flaw: it takes its data from thin labor markets, and not thick ones. With thick labor markets, employers could no longer miss wage payments because they want to make an extra buck \u2013 they would be forced out of the market due to their terrible reputation. Employers for the first time would actually have to compete for workers, since workers would have the leverage to leave at will at the prevailing wage. Perhaps most powerfully, consumers would know the general pay of individual establishments, because again, the market is able to provide that information to all participants. All of these forces would be strong, and all would push wages higher. We shouldn\u2019t assume that the dynamics of our current labor market would carry over to a thick labor market. It\u2019s entirely different, and in terms of job security, is vastly superior to our current model. The question of wages is vitally important, but we shouldn\u2019t immediately dismiss thick labor markets as wage destroyers. There are other forces at work they may actually make them higher. Rebuilding The Invisible Hand The idea of thick labor markets is not unknown to Silicon Valley \u2013 Uber is the obvious example that comes up here. According to their own research, the company has created 160,000 flexible jobs for workers, and many thousands more are on the way. But Uber is just one small piece of the overall labor market, and thus its effect is still not enough to thicken the overall market. The idea of thick labor markets may be a bit of a thought experiment. Certainly their effects are a bit of a mystery \u2013 markets work in complex ways, and emergent properties of these markets may be difficult if not impossible to predict without building them in the first place. However, it is not hard to believe that technology and startups are going to increasingly take the friction out of the marketplace. Better algorithms can align the right workers with the right employers almost instantaneously, and reputation systems ensures that workers are more consistent in their work (and employers act responsibly as well). There are concerns that workers are dehumanized in such algorithmic assignment of work, a criticism that I strongly disagree with. It is our current system that is dehumanizing, whether it is applying for hundreds of jobs and never receiving a response or doing stupid exercises during interviews that are completely unrelated to the job that we applied for in the first place. Thicker labor markets would be an incredible improvement over our existing arrangement. I want to make finding work as easy as buying a pair of socks on Amazon. I want to hit a button, and be at my job in 20 minutes ready to go. When technology has allowed that to happen, we will have finally have given the power to workers to shape their lives how they want to. What a revolution indeed."
        },
        {
            "article_published": "01/2015",
            "source": "https://techcrunch.com/2015/01/31/narrow-ai-cant-do-that-or-can-it/",
            "relevance_score": 0.1023608148097992,
            "content": "More posts by this contributor Editor\u2019s note: David Senior is the CEO and co-founder of Lowdownapp. With industry pundits, including Stephen Hawking, Elon Musk and others, hotly debating the dangers of artificial intelligence and Hollywood priming the public for the release of a slew of new movies \u2014 including Terminator 5 \u2014 that warn what can happen when software and hardware evolve to the point that they are capable of human feats of intelligence, it\u2019s little wonder that the popular view of AI has become confused and convoluted. The assumption that all AI is about systems designed to autonomously learn new tasks, adapt to changing environments and perhaps, like HAL, outwit their creators in the end, skims over the many important differences between classic AI (the one movies are made of) and its over one dozen subdisciplines. These AI subsets, ranging from the speech recognition and natural language understanding we know from personal assistants like Siri and Cortana, to the machine learning and deep learning capabilities core to business analytics and systems designed to make sense of big data, are \u2014 and will remain \u2014 where the action and opportunity is. VCs need no convincing. The last few months have seen a flurry of activity and a wave of investment as startups in Silicon Valley and beyond raise substantial funding for AI approaches and innovations that emphasize the business benefits of AI and narrow AI, a technology subset focused on solving specific, reasonably well-defined problems. It\u2019s a stampede as many of Silicon Valley\u2019s leading venture capital firms, including Khosla Ventures and Greylock Partners, as well as financial institutions, such as Goldman Sachs, flock to the space and invest big dollars in companies using narrow AI technologies to tackle tough business tasks like taming big data. A prime example is Seattle-based Context Relevant, a provider of automated predictive analytics software for big data 2.0 applications. It raised $13.5 million in series B-1 funding with participation by Goldman Sachs, Bank of America Merrill Lynch, Formation 8, New York Life and Bloomberg Beta in September 2014. The round of funding came just months after it closed $21 million in a Series B round, bringing total funding for Context Relevant to $42 million. With an estimated 170 startups in the starting gate ready to recast themselves as AI companies, or simply jump on the bandwagon, you can bet this year will see AI (in all its flavors and forms) lead the list of mega-trends. But before you dismiss this as hype, consider that the rise of AI, and specifically weak or narrow AI, is also inextricably linked with the growth of big data. Simply put, it\u2019s the explosive rate of information growth that creates the requirement for narrow AI. Add to that the recent avalanche of user-generated content \u2014 the nearly 300,000 tweets, 220,000 Instagram photos, 72 hours of YouTube video content and the 2.5 million pieces of content shared by Facebook users every single minute that businesses must monitor and acknowledge \u2014 and it\u2019s clear that no organization (or human) can cope without the aid of narrow AI. But the business and personal benefits of narrow AI go far beyond the ability to trawl through massive amounts of information and automate routine knowledge work. Some narrow AI approaches sift through the data to pull together and expose what is relevant and valuable to the individual user and their \u201cneed\u201d state. An early and rather primitive example of this is Apple\u2019s Siri. I give it credit for bringing narrow AI to the mainstream. But I also side with Robert Scoble, who has repeatedly remarked that the fatal flaw in this and other personal assistant services is a lack of context. Successful narrow AI services, to deliver value and benefit, must be aware of the user\u2019s environment and factor this into the equation before delivering answers or advice or simply taking action. A great sandbox for ideas and innovation is the smartphone calendar, the fiercely personal device most people regard as a digital extension of their physical \u201cself.\u201d The calendar and contacts is not only where people live and record their lives; it\u2019s a living microcosm of their social graph device that grows and evolves as people and their networks do. And so it makes sense that this space \u2014 where the calendar, contacts and context come together \u2014 is where startups are staging a new battle. The stakes are high, which is why competition is also high, as companies conduct a digital battle as fierce as Arnie and as determined as HAL."
        },
        {
            "article_published": "01/2015",
            "source": "https://techcrunch.com/2015/01/25/the-human-impact-of-the-industrial-internet-of-things/?ncid=rss",
            "relevance_score": 0.09944818913936615,
            "content": "Bruno Berthon Contributor Editor\u2019s note: Bruno Berthon is the managing director of digital strategy at Accenture. Will digital technology be positive for workers and jobs? Amid today\u2019s public debate about the consequences of artificial intelligence and advanced robotics, along comes the industrial Internet of things (IIoT). Little understood but potentially very significant for multiple industry sectors, this next wave of technology will create more jobs than it will destroy, according to the majority of business leaders Accenture has surveyed. The industrial Internet of things is a fast-growing network of intelligent connected devices, machines and objects. It will certainly automate and drive efficiencies, but the optimism of employers reflects their recognition that, more importantly, it will enable the creation of entirely new products and services and markets. Indeed, Accenture Strategy estimates the IIoT could boost the gross domestic product (GDP) of 20 of the world\u2019s largest economies by an additional US$14 trillion by 2030. Where some technology advances have simply improved the quality and price competitiveness of products\u2014mainly through automation\u2014the IIoT breaks new ground in helping use vast volumes of data from those products and other physical objects to offer tailored outcomes to customers. Take the example of the agrochemical sector. By integrating climatic, geological and other data, companies can go beyond selling products to earning revenues from guaranteed yields for specific crops in certain locations. Similarly, engine manufacturers could be rewarded for delivering reduced air travel delays by pre-empting maintenance issues through the real-time monitoring of engine performance in flight. In short, the outcome economy has arrived, where partnerships between companies and their respective workers inspire more bespoke and varied solutions for customers. Small wonder, then, that 86 percent of the 1,400 business leaders we polled think the industrial Internet of things will be a net creator of jobs. There is already evidence that workforce transformation is happening. A Maryland steel company used automation and robotics coupled with analytics, which led to more knowledge-intensive work. The result? A safer, more engaging work experience alongside higher productivity and quality. By making digital investments, the steel company was able to significantly increase hourly pay and experienced growing demand that led to an increase in hires. It is not just about new jobs but the content of those new roles. Many businesses will demand new skills and reward workers with more interesting work. Accenture and Royal Philips\u2019 proof-of-concept demonstration uses a Google Glass head-mounted display to research ways to improve the effectiveness and efficiency of performing surgical procedures. Theoretically, hands-free access to critical clinical information could also be applied in the utilities or communications industries, helping field engineers repair complex equipment in more difficult stations than they can today. These digital enhancements augment skills as employees blend their skills with those of digital labor. The IIoT can also empower workers. By sharing data about how customers are interacting with products, employees can use 3D printing and other technologies to experiment in virtual teams, produce prototypes more quickly and tweak product design. Innovation is not only more spontaneous and synchronized; it is autonomous, liberating employees from traditional research and development structures. At the core of this change is the way workers will be freed from volume activity to address individual exceptions revealed by data. In this way, they can resolve challenges faced by particular customers and design more tailored solutions for them. This shift in focus from delivering mass products to delivering outcomes for customers places greater emphasis on talent. The benefits are not guaranteed, however. Seventy two percent of the CEOs and business leaders Accenture surveyed say their company has yet to make concrete plans for the industrial Internet of things. Only seven percent have developed a comprehensive IIoT strategy with investments to match. What do leaders need to do to ensure digital technology brings advantages to workers? Leaders will have to take risks by collapsing hierarchies and permitting new levels of autonomy so that workers can use data and intelligent connected devices to collaborate more with counterparts in other companies. Some companies will need to get ahead of organizational change that will otherwise be forced on them, as digital technology reverses recent trends by centralizing manufacturing while decentralizing services\u2019 delivery. The IIoT depends on significant investments in developing ad hoc skills and breeding new talent. New jobs, from digital robot design and healthcare analytics to transport network engineering and software development, can only be created if businesses and governments and the education sector work together to redesign education curricula. Talent and skills are the most important determinant of whether countries and companies use this new digital era to secure growth and boost their competitiveness. And workers and employability could also be the greatest beneficiaries. That tantalizing prospect depends on business and governments doing more to recognize the generational transformation in the workforce that could result from embracing the industrial Internet of things."
        },
        {
            "article_published": "01/2015",
            "source": "https://www.nytimes.com/2015/01/29/technology/personaltech/uber-a-rising-business-model.html?ref=business",
            "relevance_score": 0.12235559523105621,
            "content": "State of the ArtCredit...Stuart GoldenbergJan. 28, 2015As Uber has grown to become one of the world\u2019s most valuable start-ups, its ambitions often seem limitless.But of all the ways that Uber could change the world, the most far-reaching may be found closest at hand: your office. Uber, and more broadly the app-driven labor market it represents, is at the center of what could be a sea change in work, and in how people think about their jobs. You may not be contemplating becoming an Uber driver any time soon, but the Uberization of work may soon be coming to your chosen profession.Just as Uber is doing for taxis, new technologies have the potential to chop up a broad array of traditional jobs into discrete tasks that can be assigned to people just when they\u2019re needed, with wages set by a dynamic measurement of supply and demand, and every worker\u2019s performance constantly tracked, reviewed and subject to the sometimes harsh light of customer satisfaction. Uber and its ride-sharing competitors, including Lyft and Sidecar, are the boldest examples of this breed, which many in the tech industry see as a new kind of start-up \u2014 one whose primary mission is to efficiently allocate human beings and their possessions, rather than information.Various companies are now trying to emulate Uber\u2019s business model in other fields, from daily chores like grocery shopping and laundry to more upmarket products like legal services and even medicine.\u201cI do think we are defining a new category of work that isn\u2019t full-time employment but is not running your own business either,\u201d said Arun Sundararajan, a professor at New York University\u2019s business school who has studied the rise of the so-called on-demand economy, and who is mainly optimistic about its prospects.Uberization will have its benefits: Technology could make your work life more flexible, allowing you to fit your job, or perhaps multiple jobs, around your schedule, rather than vice versa. Even during a time of renewed job growth, Americans\u2019 wages are stubbornly stagnant, and the on-demand economy may provide novel streams of income.ImageCredit...Todd Heisler/The New York Times\u201cWe may end up with a future in which a fraction of the work force would do a portfolio of things to generate an income \u2014 you could be an Uber driver, an Instacart shopper, an Airbnb host and a Taskrabbit,\u201d Dr. Sundararajan said.But the rise of such work could also make your income less predictable and your long-term employment less secure. And it may relegate the idea of establishing a lifelong career to a distant memory.\u201cI think it\u2019s nonsense, utter"
        },
        {
            "article_published": "01/2015",
            "source": "https://www.nytimes.com/2015/01/18/books/review/among-the-disrupted.html?referrer=&_r=0",
            "relevance_score": 0.11083629727363586,
            "content": "https://www.nytimes.com/2015/01/18/books/review/among-the-disrupted.html?referrer=&_r=0 Among the Disrupted 2015-01-18 Leon Wieseltier AdvertisementSKIP ADVERTISEMENTEssayJan. 7, 2015Amid the bacchanal of disruption, let us pause to honor the disrupted. The streets of American cities are haunted by the ghosts of bookstores and record stores, which have been destroyed by the greatest thugs in the history of the culture industry. Writers hover between a decent poverty and an indecent one; they are expected to render the fruits of their labors for little and even for nothing, and all the miracles of electronic dissemination somehow do not suffice for compensation, either of the fiscal or the spiritual kind. Everybody talks frantically about media, a second-order subject if ever there was one, as content disappears into \u201ccontent.\u201d What does the understanding of media contribute to the understanding of life? Journalistic institutions slowly transform themselves into silent sweatshops in which words cannot wait for thoughts, and first responses are promoted into best responses, and patience is a professional liability. As the frequency of expression grows, the force of expression diminishes: Digital expectations of alacrity and terseness confer the highest prestige upon the twittering cacophony of one-liners and promotional announcements. It was always the case that all things must pass, but this is ridiculous.Meanwhile the discussion of culture is being steadily absorbed into the discussion of business. There are \u201cmetrics\u201d for phenomena that cannot be metrically measured. Numerical values are assigned to things that cannot be captured by numbers. Economic concepts go rampaging through noneconomic realms: Economists are our experts on happiness! Where wisdom once was, quantification will now be. Quantification is the most overwhelming influence upon the contemporary American understanding of, well, everything. It is enabled by the idolatry of data, which has itself been enabled by the almost unimaginable data-generating capabilities of the new technology. The distinction between knowledge and information is a thing of the past, and there is no greater disgrace than to be a thing of the past. Beyond its impact upon culture, the new technology penetrates even deeper levels of identity and experience, to cognition and to consciousness. Such transformations embolden certain high priests in the church of tech to espouse the doctrine of \u201ctranshumanism\u201d and to suggest, without any recollection of the bankruptcy of utopia, without any consideration of the cost to human dignity, that our computational ability will carry us magnificently beyond our humanity and \u201callow us to transcend these limitations of our biological bodies and brains. . . . There will be no distinction, post-Singularity, between human and machine.\u201d (The author of that updated mechanistic nonsense is a director of engineering at Google.)And even as technologism, which is not the same as technology, asserts itself over more and more precincts of human life, so too does scientism, which is not the same as science. The notion that the nonmaterial dimensions of life must be explained in terms of the material dimensions, and that nonscientific understandings must be translated into scientific understandings if they are to qualify as knowledge, is increasingly popular inside and outside the university, where the humanities are disparaged as soft and impractical and insufficiently new. The contrary insistence that the glories of art and thought are not evolutionary adaptations, or that the mind is not the brain, or that love is not just biology\u2019s bait for sex, now amounts to a kind of heresy. So, too, does the view that the strongest defense of the humanities lies not in the appeal to their utility \u2014 that literature majors may find good jobs, that theaters may economically revitalize neighborhoods \u2014 but rather in the appeal to their defiantly nonutilitarian character, so that individuals can know more than how things work, and develop their powers of discernment and judgment, their competence in matters of truth and goodness and beauty, to equip themselves adequately for the choices and the crucibles of private and public life.\u25c6This gloomy inventory of certain tendencies in contemporary American culture \u2014 it is not the whole story, but it is an alarmingly large part of the story \u2014 is offered for the purpose of proposing an accurate name for our moment. We are not becoming transhumanists, obviously. We are too singular for the Singularity. But are we becoming posthumanists?No culture is philosophically monolithic, or promotes a single conception of the human. A culture is an internecine contest between alternative conceptions of the human. Which culture is free of contradictions between first principles? This is no less true of religious cultures than of secular ones, of closed societies than of open ones. Popular culture may be as soaked in ideas as high culture: A worldview can be found in a song. Wherever mortal beings are thoughtful about their mortality, and finite beings ponder their finitude, at whatever level of intellectual articulation, there is philosophy. Philosophy is ubiquitous and inalienable; even the discourse about the end of philosophy is philosophy. A culture may be regarded as the sum of all the philosophies, all the reflective approaches to living, that are manifestly or latently expressed in a society. It is a gorgeous anarchy, even if it contains illusions and errors. There are worse things than being wrong.Within a culture, however, some views may come to prevail over others, for intellectual or social reasons. The war between the worldviews has winners and losers, though none of the worldviews are ever erased and there is honor also in loss. In American culture right now, as I say, the worldview that is ascendant may be described as posthumanism. We have been here before, and not too long ago, but for different reasons. The posthumanism of the 1970s and 1980s was more insular, an academic affair of \u201ctheory,\u201d an insurgency of professors; our posthumanism is a way of life, a social fate. An important book, a brilliant book, an exasperating book has just been written about the origins of that previous posthumanist moment. In \u201cThe Age of the Crisis of Man: Thought and Fiction in America, 1933-1973,\u201d the gifted essayist Mark Greif, who reveals himself to be also a skillful historian of ideas, charts the history of the 20th-century reckonings with the definition of \u201cman.\u201d Strangely, he seems to regret the entire enterprise. Here is his conclusion: \u201cAnytime your inquiries lead you to say, \u2018At this moment we must ask and decide who we fundamentally are, our solution and salvation must lie in a new picture of ourselves and humanity, this is our profound responsibility and a new opportunity\u2019 \u2014 just stop.\u201d Greif seems not to realize that his own book is a lasting monument to precisely such inquiry, and to its grandeur. \u201cAnswer, rather, the practical matters,\u201d he counsels, in accordance with the current pragmatist orthodoxy. \u201cFind the immediate actions necessary to achieve an aim.\u201d But before an aim is achieved, should it not be justified? And the activity of justification may require a \u201cpicture of ourselves.\u201d Don\u2019t just stop. Think harder. Get it right. (Why are liberals so afraid of their own philosophy?)Greif\u2019s book is a prehistory of our predicament, of our own \u201ccrisis of man.\u201d (The \u201cman\u201d is archaic, the \u201ccrisis\u201d is not.) It recognizes that the intellectual history of modernity may be written in part as the epic tale of a series of rebellions against humanism. Humanism has been savaged by theists and atheists, conservatives and progressives, fascists and socialists, scientists and philosophers, though it has also been propounded by the same diversity of thinkers. Who has not felt superior to humanism? It is the cheapest target of all: Humanism is sentimental, flabby, bourgeois, hypocritical, complacent, middlebrow, liberal, sanctimonious, constricting and often an alibi for power. The abusers of humanism, of course, are guilty of none of those sins. From Heidegger to Althusser, they come as emancipators. I think we should emancipate ourselves from their emancipations.But what is humanism? For a start, humanism is not the antithesis of religion, as Pope Francis is exquisitely demonstrating. The most common understanding of humanism is that it denotes a pedagogy and a worldview. The pedagogy consists in the traditional Western curriculum of literary and philosophical classics, beginning in Greek and Roman antiquity and \u2014 after an unfortunate banishment of medieval culture from any pertinence to our own \u2014 erupting in the rediscovery of that antiquity in Europe in the early modern centuries, and in the ideals of personal cultivation by means of textual study and aesthetic experience that it bequeathed, or that were developed under its inspiration, in the \u201cenlightened\u201d 18th and 19th centuries, and eventually culminated in programs of education in the humanities in modern universities. The worldview takes many forms: a philosophical claim about the centrality of humankind to the universe, and about the irreducibility of the human difference to any aspect of our animality; a methodological claim about the most illuminating way to explain history and human affairs, and about the essential inability of the natural sciences to offer a satisfactory explanation; a moral claim about the priority, and the universal nature, of certain values, not least tolerance and compassion. It is all a little inchoate \u2014 human, humane, humanities, humanism, humanitarianism; but there is nothing shameful or demeaning about any of it.And posthumanism? It elects to understand the world in terms of impersonal forces and structures, and to deny the importance, and even the legitimacy, of human agency. It certainly does not mean, as Greif correctly notes about antihumanism, a \u201chatred of the human.\u201d There have been humane posthumanists and there have been inhumane humanists. But the inhumanity of humanists may be refuted on the basis of their own worldview, whereas the condemnation of cruelty toward \u201cman the machine,\u201d to borrow the old but enduring notion of an 18th-century French materialist, requires the importation of another framework of judgment. The same is true about universalism, which every critic of humanism has arraigned for its failure to live up to the promise of a perfect inclusiveness. It is a melancholy fact of history that there has never been a universalism that did not exclude. Yet the same is plainly the case about every particularism, which is nothing but a doctrine of exclusion; and the correction of particularism, the extension of its concept and its care, cannot be accomplished in its own name. It requires an idea from outside, an idea external to itself, a universalistic idea, a humanistic idea. Asking universalism to keep faith with its own principles is a perennial activity of moral life. Asking particularism to keep faith with its own principles is asking for trouble.\u25c6Aside from issues of life and death, there is no more urgent task for American intellectuals and writers than to think critically about the salience, even the tyranny, of technology in individual and collective life. All revolutions exaggerate, and the digital revolution is no different. We are still in the middle of the great transformation, but it is not too early to begin to expose the exaggerations, and to sort out the continuities from the discontinuities. The burden of proof falls on the revolutionaries, and their success in the marketplace is not sufficient proof. Presumptions of obsolescence, which are often nothing more than the marketing techniques of corporate behemoths, need to be scrupulously examined. By now we are familiar enough with the magnitude of the changes in all the spheres of our existence to move beyond the futuristic rhapsodies that characterize much of the literature on the subject. We can no longer roll over and celebrate and shop. Every phone in every pocket contains a \u201cpicture of ourselves,\u201d and we must ascertain what that picture is and whether we should wish to resist it. Here is a humanist proposition for the age of Google: The processing of information is not the highest aim to which the human spirit can aspire, and neither is competitiveness in a global economy. The character of our society cannot be determined by engineers.\u201cOur very mastery seems to escape our mastery,\u201d Michel Serres has anxiously remarked. \u201cHow can we dominate our domination; how can we master our own mastery?\u201d Every technology is used before it is completely understood. There is always a lag between an innovation and the apprehension of its consequences. We are living in that lag, and it is a right time to keep our heads and reflect. We have much to gain and much to lose. In the media, for example, the general inebriation about the multiplicity of platforms has distracted many people from the scruple that questions of quality on the new platforms should be no different from questions of quality on the old platforms. Otherwise a quantitative expansion will result in a qualitative contraction. The new devices do not in themselves authorize a revision of the standards of evidence and argument and style that we championed in the old devices. (What a voluptuous device paper is!) Such revisions may be made on other grounds \u2014 out of commercial ambition, for example; but there is nothing innovative about pandering for the sake of a profit. The decision to prefer the requirements of commerce to the requirements of culture cannot be exonerated by the thrills of the digital revolution.And therein lies a consoling irony of our situation. The machines may be more neutral about their uses than the propagandists and the advertisers want us to believe. We can leave aside the ideology of digitality and its aggressions, and regard the devices as simply new means for old ends. Tradition \u201ctravels\u201d in many ways. It has already flourished in many technologies \u2014 but only when its flourishing has been the objective. I will give an example from the humanities. The day is approaching when the dream of the democratization of knowledge \u2014 Borges\u2019s fantasy of \u201cthe total library\u201d \u2014 will be realized. Soon all the collections in all the libraries and all the archives in the world will be available to everyone with a screen. Who would not welcome such a vast enfranchisement? But universal accessibility is not the end of the story, it is the beginning. The humanistic methods that were practiced before digitalization will be even more urgent after digitalization, because we will need help in navigating the unprecedented welter. Searches for keywords will not provide contexts for keywords. Patterns that are revealed by searches will not identify their own causes and reasons. The new order will not relieve us of the old burdens, and the old pleasures, of erudition and interpretation.Is all this \u2014 is humanism \u2014 sentimental? But sentimentality is not always a counterfeit emotion. Sometimes sentiment is warranted by reality. The persistence of humanism through the centuries, in the face of formidable intellectual and social obstacles, has been owed to the truth of its representations of our complexly beating hearts, and to the guidance that it has offered, in its variegated and conflicting versions, for a soulful and sensitive existence. There is nothing soft about the quest for a significant life. And a complacent humanist is a humanist who has not read his books closely, since they teach disquiet and difficulty. In a society rife with theories and practices that flatten and shrink and chill the human subject, the humanist is the dissenter. Never mind the platforms. Our solemn responsibility is for the substance.LEON WIESELTIER is a contributing editor at The Atlantic and the author of \u201cKaddish.\u201dA version of this article appears in print on , Page 1 of the Sunday Book Review with the headline: Among the DisruptedAdvertisementSKIP ADVERTISEMENT||||I|||| Skip to contentSkip to site index Book Review Today\u2019s Paper What to Read * January Releases * Your Next Read * Critics\u2019 Reviews * Editors\u2019 Choice * 100 Notable Books * U.S. * World * Business * Arts * Lifestyle * Opinion * Audio * Games * Cooking * Wirecutter * The Athletic What to Read * January Releases * Your Next Read * Critics\u2019 Reviews * Editors\u2019 Choice * 100 Notable Books Advertisement SKIP ADVERTISEMENT What to Read * January Releases * Your Next Read * Critics\u2019 Reviews * Editors\u2019 Choice * 100 Notable Books Supported by SKIP ADVERTISEMENT Essay Among the Disrupted * Share full article * * Credit... Joon Mo Kang By Leon Wieseltier * Jan. 7, 2015 Amid the bacchanal of disruption, let us pause to honor the disrupted. The streets of American cities are haunted by the ghosts of bookstores and record stores, which have been destroyed by the greatest thugs in the history of the culture industry. Writers hover between a decent poverty and an indecent one; they are expected to render the fruits of their labors for little and even for nothing, and all the miracles of electronic dissemination somehow do not suffice for compensation, either of the fiscal or the spiritual kind. Everybody talks frantically about media, a second-order subject if ever there was one, as content disappears into \u201ccontent.\u201d What does the understanding of media contribute to the understanding of life? Journalistic institutions slowly transform themselves into silent sweatshops in which words cannot wait for thoughts, and first responses are promoted into best responses, and patience is a professional liability. As the frequency of expression grows, the force of expression diminishes: Digital expectations of alacrity and terseness confer the highest prestige upon the twittering cacophony of one-liners and promotional announcements. It was always the case that all things must pass, but this is ridiculous. Meanwhile the discussion of culture is being steadily absorbed into the discussion of business. There are \u201cmetrics\u201d for phenomena that cannot be metrically measured. Numerical values are assigned to things that cannot be captured by numbers. Economic concepts go rampaging through noneconomic realms: Economists are our experts on happiness! Where wisdom once was, quantification will now be. Quantification is the most overwhelming influence upon the contemporary American understanding of, well, everything. It is enabled by the idolatry of data, which has itself been enabled by the almost unimaginable data-generating capabilities of the new technology. The distinction between knowledge and information is a thing of the past, and there is no greater disgrace than to be a thing of the past. Beyond its impact upon culture, the new technology penetrates even deeper levels of identity and experience, to cognition and to consciousness. Such transformations embolden certain high priests in the church of tech to espouse the doctrine of \u201ctranshumanism\u201d and to suggest, without any recollection of the bankruptcy of utopia, without any consideration of the cost to human dignity, that our computational ability will carry us magnificently beyond our humanity and \u201callow us to transcend these limitations of our biological bodies and brains. . . . There will be no distinction, post-Singularity, between human and machine.\u201d (The author of that updated mechanistic nonsense is a director of engineering at Google.) And even as technologism, which is not the same as technology, asserts itself over more and more precincts of human life, so too does scientism, which is not the same as science. The notion that the nonmaterial dimensions of life must be explained in terms of the material dimensions, and that nonscientific understandings must be translated into scientific understandings if they are to qualify as knowledge, is increasingly popular inside and outside the university, where the humanities are disparaged as soft and impractical and insufficiently new. The contrary insistence that the glories of art and thought are not evolutionary adaptations, or that the mind is not the brain, or that love is not just biology\u2019s bait for sex, now amounts to a kind of heresy. So, too, does the view that the strongest defense of the humanities lies not in the appeal to their utility \u2014 that literature majors may find good jobs, that theaters may economically revitalize neighborhoods \u2014 but rather in the appeal to their defiantly nonutilitarian character, so that individuals can know more than how things work, and develop their powers of discernment and judgment, their competence in matters of truth and goodness and beauty, to equip themselves adequately for the choices and the crucibles of private and public life. \u25c6 This gloomy inventory of certain tendencies in contemporary American culture \u2014 it is not the whole story, but it is an alarmingly large part of the story \u2014 is offered for the purpose of proposing an accurate name for our moment. We are not becoming transhumanists, obviously. We are too singular for the Singularity. But are we becoming posthumanists? No culture is philosophically monolithic, or promotes a single conception of the human. A culture is an internecine contest between alternative conceptions of the human. Which culture is free of contradictions between first principles? This is no less true of religious cultures than of secular ones, of closed societies than of open ones. Popular culture may be as soaked in ideas as high culture: A worldview can be found in a song. Wherever mortal beings are thoughtful about their mortality, and finite beings ponder their finitude, at whatever level of intellectual articulation, there is philosophy. Philosophy is ubiquitous and inalienable; even the discourse about the end of philosophy is philosophy. A culture may be regarded as the sum of all the philosophies, all the reflective approaches to living, that are manifestly or latently expressed in a society. It is a gorgeous anarchy, even if it contains illusions and errors. There are worse things than being wrong. Within a culture, however, some views may come to prevail over others, for intellectual or social reasons. The war between the worldviews has winners and losers, though none of the worldviews are ever erased and there is honor also in loss. In American culture right now, as I say, the worldview that is ascendant may be described as posthumanism. We have been here before, and not too long ago, but for different reasons. The posthumanism of the 1970s and 1980s was more insular, an academic affair of \u201ctheory,\u201d an insurgency of professors; our posthumanism is a way of life, a social fate. An important book, a brilliant book, an exasperating book has just been written about the origins of that previous posthumanist moment. In \u201cThe Age of the Crisis of Man: Thought and Fiction in America, 1933-1973,\u201d the gifted essayist Mark Greif, who reveals himself to be also a skillful historian of ideas, charts the history of the 20th-century reckonings with the definition of \u201cman.\u201d Strangely, he seems to regret the entire enterprise. Here is his conclusion: \u201cAnytime your inquiries lead you to say, \u2018At this moment we must ask and decide who we fundamentally are, our solution and salvation must lie in a new picture of ourselves and humanity, this is our profound responsibility and a new opportunity\u2019 \u2014 just stop.\u201d Greif seems not to realize that his own book is a lasting monument to precisely such inquiry, and to its grandeur. \u201cAnswer, rather, the practical matters,\u201d he counsels, in accordance with the current pragmatist orthodoxy. \u201cFind the immediate actions necessary to achieve an aim.\u201d But before an aim is achieved, should it not be justified? And the activity of justification may require a \u201cpicture of ourselves.\u201d Don\u2019t just stop. Think harder. Get it right. (Why are liberals so afraid of their own philosophy?) Image Credit... Joon Mo Kang Greif\u2019s book is a prehistory of our predicament, of our own \u201ccrisis of man.\u201d (The \u201cman\u201d is archaic, the \u201ccrisis\u201d is not.) It recognizes that the intellectual history of modernity may be written in part as the epic tale of a series of rebellions against humanism. Humanism has been savaged by theists and atheists, conservatives and progressives, fascists and socialists, scientists and philosophers, though it has also been propounded by the same diversity of thinkers. Who has not felt superior to humanism? It is the cheapest target of all: Humanism is sentimental, flabby, bourgeois, hypocritical, complacent, middlebrow, liberal, sanctimonious, constricting and often an alibi for power. The abusers of humanism, of course, are guilty of none of those sins. From Heidegger to Althusser, they come as emancipators. I think we should emancipate ourselves from their emancipations. But what is humanism? For a start, humanism is not the antithesis of religion, as Pope Francis is exquisitely demonstrating. The most common understanding of humanism is that it denotes a pedagogy and a worldview. The pedagogy consists in the traditional Western curriculum of literary and philosophical classics, beginning in Greek and Roman antiquity and \u2014 after an unfortunate banishment of medieval culture from any pertinence to our own \u2014 erupting in the rediscovery of that antiquity in Europe in the early modern centuries, and in the ideals of personal cultivation by means of textual study and aesthetic experience that it bequeathed, or that were developed under its inspiration, in the \u201cenlightened\u201d 18th and 19th centuries, and eventually culminated in programs of education in the humanities in modern universities. The worldview takes many forms: a philosophical claim about the centrality of humankind to the universe, and about the irreducibility of the human difference to any aspect of our animality; a methodological claim about the most illuminating way to explain history and human affairs, and about the essential inability of the natural sciences to offer a satisfactory explanation; a moral claim about the priority, and the universal nature, of certain values, not least tolerance and compassion. It is all a little inchoate \u2014 \u00adhuman, humane, humanities, humanism, humanitarianism; but there is nothing shameful or demeaning about any of it. And posthumanism? It elects to understand the world in terms of impersonal forces and structures, and to deny the importance, and even the legitimacy, of human agency. It certainly does not mean, as Greif correctly notes about antihumanism, a \u201chatred of the human.\u201d There have been humane posthumanists and there have been inhumane humanists. But the inhumanity of humanists may be refuted on the basis of their own worldview, whereas the condemnation of cruelty toward \u201cman the machine,\u201d to borrow the old but enduring notion of an 18th-century French materialist, requires the importation of another framework of judgment. The same is true about universalism, which every critic of humanism has arraigned for its failure to live up to the promise of a perfect inclusiveness. It is a melancholy fact of history that there has never been a universalism that did not exclude. Yet the same is plainly the case about every particularism, which is nothing but a doctrine of exclusion; and the correction of particularism, the extension of its concept and its care, cannot be accomplished in its own name. It requires an idea from outside, an idea external to itself, a universalistic idea, a humanistic idea. Asking universalism to keep faith with its own principles is a perennial activity of moral life. Asking particularism to keep faith with its own principles is asking for trouble. \u25c6 Aside from issues of life and death, there is no more urgent task for American intellectuals and writers than to think critically about the salience, even the tyranny, of technology in individual and collective life. All revolutions exaggerate, and the digital revolution is no different. We are still in the middle of the great transformation, but it is not too early to begin to expose the exaggerations, and to sort out the continuities from the discontinuities. The burden of proof falls on the revolutionaries, and their success in the marketplace is not sufficient proof. Presumptions of obsolescence, which are often nothing more than the marketing techniques of corporate behemoths, need to be scrupulously examined. By now we are familiar enough with the magnitude of the changes in all the spheres of our existence to move beyond the futuristic rhapsodies that characterize much of the literature on the subject. We can no longer roll over and celebrate and shop. Every phone in every pocket contains a \u201cpicture of ourselves,\u201d and we must ascertain what that picture is and whether we should wish to resist it. Here is a humanist proposition for the age of Google: The processing of information is not the highest aim to which the human spirit can aspire, and neither is competitiveness in a global economy. The character of our society cannot be determined by engineers. \u201cOur very mastery seems to escape our mastery,\u201d Michel Serres has anxiously remarked. \u201cHow can we dominate our domination; how can we master our own mastery?\u201d Every technology is used before it is completely understood. There is always a lag between an innovation and the apprehension of its consequences. We are living in that lag, and it is a right time to keep our heads and reflect. We have much to gain and much to lose. In the media, for example, the general inebriation about the multiplicity of platforms has distracted many people from the scruple that questions of quality on the new platforms should be no different from questions of quality on the old platforms. Otherwise a quantitative expansion will result in a qualitative contraction. The new devices do not in themselves authorize a revision of the standards of evidence and argument and style that we championed in the old devices. (What a voluptuous device paper is!) Such revisions may be made on other grounds \u2014 out of commercial ambition, for example; but there is nothing innovative about pandering for the sake of a profit. The decision to prefer the requirements of commerce to the requirements of culture cannot be exonerated by the thrills of the digital revolution. And therein lies a consoling irony of our situation. The machines may be more neutral about their uses than the propagandists and the advertisers want us to believe. We can leave aside the ideology of digitality and its aggressions, and regard the devices as simply new means for old ends. Tradition \u201ctravels\u201d in many ways. It has already flourished in many technologies \u2014 but only when its flourishing has been the objective. I will give an example from the humanities. The day is approaching when the dream of the democratization of knowledge \u2014 Borges\u2019s fantasy of \u201cthe total library\u201d \u2014 will be realized. Soon all the collections in all the libraries and all the archives in the world will be available to everyone with a screen. Who would not welcome such a vast enfranchisement? But universal accessibility is not the end of the story, it is the beginning. The humanistic methods that were practiced before digitalization will be even more urgent after digitalization, because we will need help in navigating the unprecedented welter. Searches for keywords will not provide contexts for keywords. Patterns that are revealed by searches will not identify their own causes and reasons. The new order will not relieve us of the old burdens, and the old pleasures, of erudition and interpretation. Is all this \u2014 is humanism \u2014 sentimental? But sentimentality is not always a counterfeit emotion. Sometimes sentiment is warranted by reality. The persistence of humanism through the centuries, in the face of formidable intellectual and social obstacles, has been owed to the truth of its representations of our complexly beating hearts, and to the guidance that it has offered, in its variegated and conflicting versions, for a soulful and sensitive existence. There is nothing soft about the quest for a significant life. And a complacent humanist is a humanist who has not read his books closely, since they teach disquiet and difficulty. In a society rife with theories and practices that flatten and shrink and chill the human subject, the humanist is the dissenter. Never mind the platforms. Our solemn responsibility is for the substance. LEON WIESELTIER is a contributing editor at The Atlantic and the author of \u201cKaddish.\u201d A version of this article appears in print on , Page 1 of the Sunday Book Review with the headline: Among the Disrupted . Order Reprints | Today\u2019s Paper | Subscribe * Share full article * * Advertisement SKIP ADVERTISEMENT Site Index Site Information Navigation * \u00a9 2024 The New York Times Company * NYTCo * Contact Us * Accessibility * Work with us * Advertise * T Brand Studio * Your Ad Choices * Privacy Policy * Terms of Service * Terms of Sale * Site Map * Canada * International * Help * Subscriptions * Manage Privacy Preferences"
        },
        {
            "article_published": "01/2015",
            "source": "https://www.nytimes.com/2015/01/16/opinion/the-cruel-waste-of-americas-tech-talent.html",
            "relevance_score": 0.10828440636396408,
            "content": "The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you\u2019d like. nytimes.com/subscription"
        },
        {
            "article_published": "01/2015",
            "source": "https://www.foxnews.com/science/want-to-call-aliens-keep-it-short-and-simple-scientists-say.print",
            "relevance_score": 0.050767723470926285,
            "content": "By , Published January 13, 2015 If extraterrestrials ever phoned us, they would be more likely to send narrowly directed bursts than constantly blaring signals \u2014 more text-messaging than novel-writing, some scientists now suggest. Such a strategy also would be more practical for anyone on Earth to reach out to aliens \u2014 if one had at least a billion dollars to spend. How cheap is talk? For 50 years, people have watched the skies with radio telescopes, hoping to detect signals of intelligent alien life. So far, however, the efforts of SETI (the search for extraterrestrial intelligence ) have proved fruitless. Now some scientists suggest the kind of signals SETI has hoped to find for decades might not be what aliens would broadcast. In a way, the form of communication might come down to cost. \"Our grandfather used to say, 'Talk is cheap, but whiskey costs money,'\" said researcher Gregory Benford, an astrophysicist at the University of California, Irvine, and an award-winning science fiction novelist. \"Whatever the life form, evolution selects for economy of resources. Broadcasting is expensive, and transmitting signals across light-years would require considerable resources.\" Assuming that aliens would strive to optimize costs, limit waste and make their signaling technology more efficient, Benford and his twin, James \u2014 a fellow physicist who specializes in high-powered microwave technology \u2014 suggest the signals would not be steadily blasted out in all directions. Extraterrestrials would be more likely to send narrow \"searchlight\" beams delivered in pulses. \"This approach is more like Twitter and less like 'War and Peace,'\" said James Benford, founder and president of Microwave Sciences Inc., in Lafayette, Calif. The Benford twins, along with James' son Dominic, a NASA scientist, detailed their findings in two studies appearing in the June issue of the journal Astrobiology. The Benfords suggest a continuous signal blared at thousands of stars would simply cost too much energy. They say aliens might use short bursts \u2014 say, anywhere from a second to an hour long \u2014 and point these signals in narrow beams at one star and then another in a cycle involving up to thousands of stars that repeats over days or years. For civilizations that constantly watch the skies, the bursts would convey enough data to be recognized as undeniably artificial. As observant civilizations concentrated on this simple beacon, other beacons could broadcast more complex data at lower power (assuming the aliens were still pursuing a frugal strategy). The Benfords suggested looking at a broad range of radio signals in the 1- to 10-gigahertz range, where the travel of light is relatively unimpeded by interstellar matter. Currently SETI is focused on just the 1-to-2 gigahertz region , where components of water such as hydrogen and hydroxyl (a compound of hydrogen and oxygen) emit radio signals. \"The idea is actually based mostly on how all the electronics of the 1960s, when SETI was first starting out, only operated in the range of a few gigahertz,\" Gregory Benford explained. \"Now they operate in a range of up to 100 or 200 gigahertz, so it's a reason to revisit our assumptions. Looking to 10 gigahertz makes sense, since it's cheaper by a factor of 10 to build a transmitter at 10 gigahertz than at 1 gigahertz.\" Looking inward The Benfords also said that instead of gazing at stars within 500 or so light-years, as most SETI efforts have done for decades, observers should point more toward our galaxy's center to distances up to 1,000 light-years from Earth, where 90 percent of the galaxy's stars are clustered. \"The stars there are a billion years older than our sun, which suggests a greater possibility of contact with an advanced civilization than does pointing SETI receivers outward to the newer and less crowded edge of our galaxy,\" Gregory Benford said. Although the galactic center is home to many bursting stars whose explosions are expected to sterilize the space around them, \"the vast bulk of stars in the 28,000 light-years between Earth and the galactic center are not in sterilizing environments,\" Benford told SPACE.com. \"For an analogy, you wouldn't want to hang out in Times Square all the time, but if you live in New Jersey it's obvious that Manhattan is the place to look for the action.\" \"Will searching for distant messages work? Is there intelligent life out there? The SETI effort is worth continuing, but our common-sense beacons approach seems more likely to answer those questions,\" Benford said. Have we seen a beacon? One possibility of an extraterrestrial beacon is a puzzling transient radio source some 26,000 light-years from Earth that was discovered in 2002 in the direction of the galactic center. It sends out radio waves in bursts lasting up to 10 minutes in a 77-minute cycle. Scientists have suggested the source, labeled GCRT J17445-3009, is a flare star, an extrasolar planet, a pulsar or a brown dwarf, but none of these explanations fits well, the Benfords said. Based on the burst length and the short cycle, the Benfords doubt GCRT J17445-3009 is a beacon aimed at possible civilizations among a large crowd of stars. Still, if GCRT J17445-3009 is artificial in nature, it could be a signal that aliens pointed just at us, having detected signs of life from our planet. If that is the case, the Benfords said, we may want to pay closer attention to its signals to look for hidden details. On the other hand, GCRT J17445-3009 could be one link in an interstellar communications network. If so, it would make sense to look in the opposite direction, to see if another beam was communicating at it. \"We studied GCRT not because we really think it's a beacon, but because it's an interesting way to look at similar bursting sources,\" Gregory Benford said. \"There's the famous 'Wow' signal from 1977, for instance, that involved an enormous amount of power, and that there's still no good explanation for.\" Calling aliens Instead of just searching for extraterrestrial intelligence, the Benfords also considered messaging to extraterrestrial intelligences , or METI. They calculated that a galactic-scale beacon, with an antenna roughly a half-mile (0.9 km) wide with a range of a little more than 1,000 light-years, could be built for $1.3 billion. It would cost $200 million annually to operate. To work economically, it would use only narrow, high-power microwave beams and 35-second bursts aimed at each target star. \"Of course, if you want to send a message, first you have to find a billionaire for this,\" Gregory Benford told SPACE.com. He noted he has spoken with a number of billionaires, including former Microsoft chief technology officer Paul Allen and Amazon.com founder Jeff Bezos, \"and everyone has the same remark \u2014 that they would rather spend a billion dollars a different way.\" Copyright \u00a9 2010 Space.com. All Rights Reserved. This material may not be published, broadcast, rewritten or redistributed. URL https://www.foxnews.com/science/want-to-call-aliens-keep-it-short-and-simple-scientists-say"
        },
        {
            "article_published": "01/2015",
            "source": "https://www.foxnews.com/video/4002759760001",
            "relevance_score": 0.045530401170253754,
            "content": "Home Watch Live Shows Topics Join the conversation Log in to comment on videos and join in on the fun. Watch Live TV Watch the live stream of Fox News and full episodes. Reduce eye strain and focus on the content that matters. This video is playing in picture-in-picture. Live Now All times eastern NOW - 3:30 AM 3:30 AM 4:00 AM 4:30 AM 5:00 AM 5:30 AM Fox News Channel The Ingraham Angle 3:00 AM - 4:00 AM Fox & Friends First 4:00 AM - 5:00 AM Fox & Friends First 5:00 AM - 6:00 AM Fox Business Channel Paid Programming 3:00 AM - 3:30 AM Paid Programming 3:30 AM - 4:00 AM American Dream Home 4:00 AM - 4:30 AM American Dream Home 4:30 AM - 5:00 AM Fox News Radio FOX News Radio Live Stream"
        },
        {
            "article_published": "01/2015",
            "source": "https://www.foxnews.com/science/worlds-most-powerful-laser-fires-most-powerful-laser-blast-in-history.print",
            "relevance_score": 0.042691707611083984,
            "content": "By , Published January 08, 2015 next Before each experiment, a positioner precisely centers the target inside the target chamber and serves as a reference to align the laser beams. (Lawrence Livermore National Laboratory) prev The final optics assemblies, shown here mounted on the lower hemisphere of the target chamber, contain special optics for beam conditioning, color conversion, and color separation. They also focus the beams from 40- by 40-centimeter squares of light to a spot on the target only 0.2 to 2 millimeters in diameter. (Lawrence Livermore National Laboratory) The largest laser in the world was turned on for a fraction of a second last week -- and it unleashed the most powerful laser blast in history. The National Ignition Facility (NIF) -- a laser test facility at Lawrence Livermore National Laboratory in Livermore, Calif. -- turned on its 192 laser beams for a brief instant on March 15, unleashing a record-setting 1.875-megajoule blast into a target chamber. The lasers were combined, gathered and focused through a series of lens into a 2.03-megajoule shot, said Ed Moses, NIF director -- a record for the facility. That pulse of energy lasted for just 23 billionths of a second, yet it generated 411 trillion watts of power, NIF said -- 1,000 times more than the entire United States consumes at any given instant. \u201cIt\u2019s a remarkable demonstration of the laser from the standpoint of its energy, its precision, its power, and its availability,\u201d Moses told Nature magazine. But it\u2019s barely half the battle. NIF hopes to dramatically increase the power of the laser shots by the end of year, intending to ultimately use the facility to harness the energy reaction that occurs naturally within the sun: fusion. \u201cThis event marks a key milestone in the National Ignition Campaign\u2019s drive toward fusion ignition,\u201d Moses said. In fission, atoms are split and the massive energy released is captured. The NIF aims for fusion, the ongoing energy process in the sun and other stars where hydrogen and helium nuclei are continually fusing and releasing enormous amounts of energy. In the ignition facility, beams of light converge on pellets of hydrogen isotopes to create a similar, though controlled, micro-explosion. As the beams move through a series of amplifiers, their energy increases exponentially. From beginning to end, the beams' total energy grows from one-billionth of a joule to a potential high of four million joules, NIF said -- a factor of more than a quadrillion. And it all happens in about five millionths of a second. Because the laser is on for the merest fraction of a second, it costs little to operate -- between $5 and $20 per blast, said spokeswoman Lynda Seaver. But the potential is enormous. NIF\u2019s managers hope by the end of the year to reach a break-even point, where the energy released is equal to if not greater than the energy that went into the blast. \u201cWe have all the capability to make it happen in fiscal year 2012,\u201d Moses told Nature. Experts aren't so sure, citing challenges that NIF and other types of fusion have had in the past. Glen Wurden, a plasma physicist at Los Alamos National Laboratory in New Mexico, told Nature scientists should be wary of putting all their eggs in the laser basket. \u201cIt\u2019s premature right now,\u201d he told the magazine, citing the troubles that have plagued a competing approach to fusion and its flagship project in France. URL https://www.foxnews.com/science/worlds-most-powerful-laser-fires-most-powerful-laser-blast-in-history"
        },
        {
            "article_published": "01/2015",
            "source": "https://www.wired.com/2015/01/ai-arrived-really-worries-worlds-brightest-minds/",
            "relevance_score": 0.13933342695236206,
            "content": "On the first Sunday afternoon of 2015, Elon Musk took to the stage at a closed-door conference at a Puerto Rican resort to discuss an intelligence explosion. This slightly scary theoretical term refers to an uncontrolled hyper-leap in the cognitive ability of AI that Musk and physicist Stephen Hawking worry could one day spell doom for the human race. That someone of Musk's considerable public stature was addressing an AI ethics conference\u2014long the domain of obscure academics\u2014was remarkable. But the conference, with the optimistic title \"The Future of AI: Opportunities and Challenges,\" was an unprecedented meeting of the minds that brought academics like Oxford AI ethicist Nick Bostrom together with industry bigwigs like Skype founder Jaan Tallinn and Google AI expert Shane Legg. Musk and Hawking fret over an AI apocalypse, but there are more immediate threats. In the past five years, advances in artificial intelligence\u2014in particular, within a branch of AI algorithms called deep neural networks\u2014are putting AI-driven products front-and-center in our lives. Google, Facebook, Microsoft and Baidu, to name a few, are hiring artificial intelligence researchers at an unprecedented rate, and putting hundreds of millions of dollars into the race for better algorithms and smarter computers. AI problems that seemed nearly unassailable just a few years ago are now being solved. Deep learning has boosted Android's speech recognition, and given Skype Star Trek-like instant translation capabilities. Google is building self-driving cars, and computer systems that can teach themselves to identify cat videos. Robot dogs can now walk very much like their living counterparts. \"Things like computer vision are starting to work; speech recognition is starting to work There's quite a bit of acceleration in the development of AI systems,\" says Bart Selman, a Cornell professor and AI ethicist who was at the event with Musk. \"And that's making it more urgent to look at this issue.\" Given this rapid clip, Musk and others are calling on those building these products to carefully consider the ethical implications. At the Puerto Rico conference, delegates signed an open letter pledging to conduct AI research for good, while \"avoiding potential pitfalls.\" Musk signed the letter too. \"Here are all these leading AI researchers saying that AI safety is important,\" Musk said yesterday. \"I agree with them.\" Google Gets on Board Nine researchers from DeepMind, the AI company that Google acquired last year, have also signed the letter. The story of how that came about goes back to 2011, however. That's when Jaan Tallinn introduced himself to Demis Hassabis after hearing him give a presentation at an artificial intelligence conference. Hassabis had recently founded the hot AI startup DeepMind, and Tallinn was on a mission. Since founding Skype, he'd become an AI safety evangelist, and he was looking for a convert. The two men started talking about AI and Tallinn soon invested in DeepMind, and last year, Google paid $400 million for the 50-person company. In one stroke, Google owned the largest available talent pool of deep learning experts in the world. Google has kept its DeepMind ambitions under wraps\u2014the company wouldn't make Hassabis available for an interview\u2014but DeepMind is doing the kind of research that could allow a robot or a self-driving car to make better sense of its surroundings. That worries Tallinn, somewhat. In a presentation he gave at the Puerto Rico conference, Tallinn recalled a lunchtime meeting where Hassabis showed how he'd built a machine learning system that could play the classic '80s arcade game Breakout. Not only had the machine mastered the game, it played it a ruthless efficiency that shocked Tallinn. While \"the technologist in me marveled at the achievement, the other thought I had was that I was witnessing a toy model of how an AI disaster would begin, a sudden demonstration of an unexpected intellectual capability,\" Tallinn remembered. Deciding the dos and don'ts of scientific research is the kind of baseline ethical work that molecular biologists did during the 1975 Asilomar Conference on Recombinant DNA, where they agreed on safety standards designed to prevent manmade genetically modified organisms from posing a threat to the public. The Asilomar conference had a much more concrete result than the Puerto Rico AI confab. At the Puerto Rico conference, attendees signed a letter outlining the research priorities for AI\u2014study of AI's economic and legal effects, for example, and the security of AI systems. And yesterday, Elon Musk kicked in $10 million to help pay for this research. These are significant first steps toward keeping robots from ruining the economy or generally running amok. But some companies are already going further. Last year, Canadian roboticists Clearpath Robotics promised not to build autonomous robots for military use. \"To the people against killer robots: we support you,\" Clearpath Robotics CTO Ryan Gariepy wrote on the company's website."
        },
        {
            "article_published": "01/2015",
            "source": "https://www.wired.com/insights/2015/01/google-and-elon-musk-good-for-humanity/",
            "relevance_score": 0.1372062712907791,
            "content": "Skip Article Header. Skip to: Start of Article. esenkartal/Getty The recently published Future of Life Institute (FLI) letter \u201cResearch Priorities for Robust and Beneficial Artificial Intelligence,\u201d signed by hundreds of AI researchers in addition to Elon Musk and Stephen Hawking, many representing government regulators and some sitting on committees with names like \u201cPresidential Panel on Long Term AI future,\u201d offers a program professing to protect the mankind from the threat of \u201csuper-intelligent AIs.\u201d In a contrarian view, I believe that should they succeed, rather than upcoming salvation we will see a 21st century version of 17th century Salem Witch trials instead, where technologies competing with AI will be tried and burned at stake with much fanfare and applause from mainstream press. Before I proceed to my concerns, here\u2019s some background on AI. For the last 50 years AI researchers have promised to deliver intelligent computers, which always seem to be five years in the future. For example, Dharmendra Modha, in charge of IBM\u2019s Synapse \u201cneuromorphic\u201d chips, claimed two or three years ago that IBM \u201cwill deliver computer equivalent of human brain\u201d by 2018. I have heard this echoed of in statements of virtually all recently funded AI and Deep Learning companies. The press accepts these claims with the same gullibility it displayed during Apple Siri\u2019s launch, and hails arrival of the \u201cbrain like\u201d computing as a fait accompli. This is very far from the truth. The investments on the other hand are real, with old AI technologies dressed up in new clothes of \u201cDeep Learning.\u201d In addition to acquiring Deep Mind, Google hired Geoffrey Hinton\u2019s University of Toronto team as well as Ray Kurzweil, whose primary motivation for joining Google Brain seems to be the opportunity to upload his brain into vast Google supercomputer. Baidu invested $300M in Stanford University\u2019s Andrew Ng Deep Learning lab, Facebook and Zuckerberg personally invested $55M in Vicarious and hired Yann LeCun, the \u201cother\u201d deep learning guru. Samsung and Intel invested in Expect labs and Reactor, and Qualcomm made a sizable investment in BrainCorp. While some progress in speech processing and image recognition will be made, it will not be sufficient to justify lofty valuations of recent funding events. While my background is in fact in AI, I worked for last few years closely with the preeminent neural scientist Walter Freeman at UC Berkeley on a new kind of wearable personal assistant, one based not on AI but on neural science. During this time, I came to the conclusion that symbol-based computing technologies, including point-to-point \u201cdeep\u201d neural networks (not neural science) can not possibly deliver on claims made by many of these well funded AI labs and startups. Here are just three of the reasons: Every single innovation in evolution of vertebrate brains was due to advances in organism locomotion, and none of the new formations indicate the emergence of symbol processing in cortex. Human intelligence is a product of resonating, coupled electric fields produced by massive population of neurons, synapses and ion channels of cortex resulting in dynamic, AM modulated waves in gamma and beta range, not static point-to-point neural networks. Human memories are formed in hippocampus via \u201cphase precession\u201d of theta waves which transform time events into spatial domain without use of symbols like time stamps. Each of the above three empirical findings invalidates AI\u2019s symbolic, computation approach. I could provide more but it is hard to fight prevalent cultural myths perpetuated by mass media. Movies are a good example. At the beginning of the movie Transcendence, Johnny Depp\u2019s character, an AI researcher (from Berkeley) makes the bold claim that \u201cjust one AI will be smarter than the entire population of humans that ever lived on earth.\u201d By my calculation this estimate is incorrect today by almost 20 orders of magnitude \u2014 it will take more than a few years to bridge this gap. Which brings me back to the FLI letter. While individual investors have every right to lose their assets, the problem gets much more complicated when government regulators are involved. Here are the the main claims of the letter I have a problem with (quotes from the letter in italics): Statements like: \u201cThere is a broad consensus that AI research is progressing steadily,\u201d even \u201cprogressing dramatically\u201d (Google Brain signatories on FLI web site), are just not true. In the last 50 years there has been very little AI progress (more stasis like than \u201csteady\u201d) and not a single major AI based breakthrough commercial product, unless you count iPhone\u2019s infamous Siri. In short, despite the overwhelming media push, AI simply does not work. \u201cAI systems must do what we want them to do\u201d begs the question of who is \u201cwe?\u201d There are 92 references included in this letter, all of them from CS, AI and political scientists, there are many references to approaching, civilization threatening \u201csingularity,\u201d several references to possibilities for \u201cmind uploading,\u201d but not a single reference from a biologist or a neural scientist. To call such an approach to study of intellect \u201cinterdisciplinary\u201d is just not credible. \u201cIdentify research directions that can maximize societal benefits\u201d is outright chilling. Again, who decides whether research is \u201csocially desirable?\u201d \u201cAI super-intelligence will not act with human wishes and will threaten the humanity\u201d is just a cover for justification of the attempted power grab of AI group over the competing approaches to study of intellect. Why should government regulators support technology that has failed to deliver on its promises repeatedly for 50 years? Newly emerging branches of neural science, which made major breakthroughs in recent years, are of much greater promise, in many cases exposing glaring weaknesses of AI approach. So it is precisely these groups which will suffer if AI is allowed to \u201cregulate\u201d the direction of future research of intellect, whether human or \u201cartificial.\u201d Neural scientists study actual brains with imaging techniques such as fMRI, EEG, ECOG, etc and then postulate predictions about their structure and function from the empirical data they gathered. The more neural research progresses, the clearer it becomes that brain is vastly more complex than we thought just a few decades ago. AI researchers, on the other hand, start with the a priori assumption that the brain is quite simple, really just a carbon version of a Von Neumann CPU. As Google Brain AI researcher and FLI letter signatory, Illya Sutskever, recently told me, \u201c[The] brain absolutely is just a CPU and further study of brain would be a waste of my time.\u201d This is almost word for word repetition of famous statement of Noam Chomsky made decades ago \u201cpredicting\u201d the existence of a language \u201cgenerator\u201d in the brain. FLI letter signatories say: Do not to worry, \u201cwe\u201d will allow \u201cgood\u201d AI and \u201cidentify research directions\u201d in order to maximize societal benefits and eradicate diseases and poverty. I believe that it would be precisely the newly emerging neural science groups which would suffer if AI is allowed to regulate research direction in this field. Why should \u201cevidence\u201d like this allow AI scientists to control what biologists and neural scientists can and cannot do? It is quite possible that signatories\u2019 motives are pure. But at the moment the AI lobby has a near monopoly on forming public opinion and attracting government dollars through the influence of compliant media. Indeed government regulators in this space are all AI researchers, often funding AI startups with taxpayer dollars, and later taking up jobs with the very same companies they funded and were supposed to regulate. And often, when government regulators lead, private VC funds follow in a \u201cDon\u2019t fight the Fed,\u201d sheep-like movement. There is yet another dimension to this story: In addition to threat of upcoming \u201csingularity\u201d FLI letter reference section has many references to \u201cmind uploading.\u201d After life-time of immersion in Von Neumann architectures, from Ray Kurzweill and Peter Thiel on, many silicon valley prodigies are obsesses with the idea of becoming immortal via mind upload into silicon. Threat of death is a powerful emotion indeed, but it belongs in realm of religious thinking rather than \u201cdispassionate and objective science\u201d they profess to advocate. Let me conclude with another movie quote: At the end of movie Beautiful Mind, mathematician John Nash played by Russell Crowe, recovering from mental illness, lectures a group of students in a cafeteria: \u201cTrust mathematics, trust your teachers,\u201d then pauses and adds with a wink: \u201cJust stay away from biologists, do not trust those guys.\u201d Indeed, today\u2019s AI researchers are all children of Rene Descartes, trusting in absolute power of logic and mathematics as they push their religion of Cartesian Dualism on the rest of us. Inadvertently, they tell us all to drink AI Kool-Aid in their \u201cSkyNet is Coming\u201d sermon. I believe that neural science and biology utilizing wearable sensors is already much more fruitful than AI in delivering personal assistants guiding us through daily life, keeping us healthier and stress free, based on better understanding of brain, rather than logic of CPU programing and algorithms of AI focused on weapons and robotics. I hope US press will arise to a defense of scientists rights to continue to perform such free research rather then limiting their research to work on \u201cdesirable\u201d results. As famous US journalist once said: \u201cThose who sacrifice liberty for security deserve neither.\u201d Roman Ormandy is the founder of Embody Corp. Go Back to Top. Skip To: Start of Article."
        },
        {
            "article_published": "01/2015",
            "source": "https://www.wired.com/insights/2015/01/innovation-takes-the-exponential-express/",
            "relevance_score": 0.1294884979724884,
            "content": "We\u2019re all familiar with Moore\u2019s Law: the observation that the number of transistors in an integrated circuit doubles every two years or so. We\u2019re also familiar with the plethora of corollaries to the law, pertaining to everything from network speed to hard drive capacity to the number of pixels in our digital cameras. It seems that the natural behavior for technology advancement follows an exponential growth curve. However, not all innovation follows such a curve. Organizational and process improvements, in particular, seem to proceed at a glacial pace. Management fads come and go, and they don\u2019t even seem to be getting much better, let alone better at a faster rate. In development shops, the Agile Manifesto is now fifteen years old or so, and yet organizations still struggle with it. Where\u2019s Moore\u2019s Law when you need it, eh? The problem today\u2019s organizations face is that digital transformation relies both upon technology advancement as well as organizational innovation \u2013 and yet, it seems our inability to accelerate the latter may be putting the whole kit and caboodle in jeopardy. If only we could bottle up the secret to Moore\u2019s Law and pass the bottle around. It\u2019s true that Moore\u2019s law is about fifty years old, more than a lifetime for most of the people who have been riding the exponential express for their entire careers. Yet, while such exponential behavior is now all around us, one question strangely appears to be as yet unanswered: why. Kurzweil\u2019s Key Why? What is it about technology innovation that naturally proceeds along an exponential path? Note first of all that calling Moore\u2019s Law a law is a misnomer; in reality it is simply an observation. An observation so ingrained now in the planning of technology innovators that perhaps it has become self-fulfilling \u2013 but such a sui generis explanation is certainly no explanation at all. I had the great fortune of asking Ray Kurzweil this very question at a recent conference. Kurzweil, of course, is a noted inventor, futurist, and deep thinker who has been writing about the exponential pattern of technology innovation for many years now, what he calls the law of accelerating returns . He has extended his argument well into the future, leading him to theorize about computers that get so smart they completely swamp our human intelligence, or perhaps to technology so advanced we\u2019re able to augment our brains with it. Fascinating stuff, to be sure, especially to us lifelong science fiction fans. And while some of his prognostications sound outlandish, there\u2019s no arguing with the exponential behavior of technology innovation \u2013 and once you get your head around the wheat on the chessboard lessons of such growth, one would be hard pressed not to allow Kurzweil a good measure of leeway in his predictions. Our focus here at Intellyx, however, is more short term. Planning for the next year or two is difficult enough without worrying about whether we\u2019ll be able to back up our consciousnesses to the cloud before we die of old age. Let us therefore return to the question of the day: why? Kurzweil\u2019s answer is essentially to point to the feedback loop intrinsic to all exponential innovation. Essentially, each generation leverages the one before. Intel 386 processors enabled the brilliant minds at Intel to create the 486, and then the Pentium, and so on. The same with memory capacity and Ethernet speeds and so on ad infinitum. Perhaps. Yes, feedback loops do seem to be part of the answer, but they don\u2019t give me the warm fuzzies I would require in order to believe they are the whole story. What is it about human innovation that requires such steadfast adherence to each exponential curve? Furthermore, what happens when exponential curves give out? After all, transistors can only get so small. What happens when Moore\u2019s Law drives right past the single-atom transistor and keeps on going? Moore\u2019s Law Before Moore Kurzweil, as you might expect, has thought long and hard about this question. In fact, he extends Moore\u2019s Law into the past, well before the 1971 invention of the integrated circuit, as shown in the following diagram. Moore\u2019s Law Extended Back to 1900 (Source: Kurzweil AI) As electromechanical calculating machines gave way to relay-based computers and then vacuum tubes and eventually transistors, Kurzweil\u2019s favored metric of calculations per second per dollar stuck closely to an exponential curve (represented as a straight line on this logarithmic scale) \u2013 even curving upward a bit. In other words, just as a particular computing approach (what Kurzweil calls a paradigm) runs out of steam, human innovation comes up with another just in the nick of time. Again my question: why? Feedback loops don\u2019t help us answer this question now. The inventors of the transistor didn\u2019t look at the fact that vacuum tube technology had reached its limit in order to schedule the timing of their invention. Human innovation doesn\u2019t work that way. In other words, disruptive innovation and incremental innovation are at opposite ends of a spectrum \u2013 and yet, still adhere to the same curve. Such adherence can\u2019t be coincidence. There must be some underlying principle of human innovation \u2013 or broadly speaking, human behavior \u2013 that drives both ends of this spectrum of innovation. I don\u2019t think Kurzweil has an answer to this question. At least, several days of poking around his writing haven\u2019t uncovered an answer. (Ray, drop me a line if I\u2019ve missed something!) The good news: I believe I\u2019ve cracked this nut. In fact, I approached this question from an entirely different angle in an earlier Cortex where I discussed the nature of disruptive innovation. The insight necessary to answer the question of why depends upon treating any large group of people as a complex adaptive system, where innovativeness is one of the emergent properties of that system. As I explained in that Cortex, shifting such a system from high connectivity to low connectivity increases evolutionary change. Furthermore, disruptions also lead to increasing rates of evolutionary change, in particular when the system exhibits low connectivity. Such change leads to adaptation to the disruptions in the environment, so combining low connectivity with disruptions leads to periods of rapid innovation and adaptation. Emergent behavior essentially appears as patterns out of patternlessness. Take a bunch of independent actors (scientists, business people, engineers, etc.), give them a number of external motivations (the drive for profit, the human need to create something new, and so on), as well as various resources \u2013 without the high connectivity, rigid reporting structures of traditional hierarchical organizations \u2013 and stir. The end results are specific, repeatable patterns we can take to the bank: in this case, the exponential growth of the law of accelerating returns. Extending Exponential Innovation to the Organization The law of accelerating returns applies to many evolutionary processes entirely separate from the behavior of members of Homo sapiens. In fact, Kurzweil points to the evolution of our species itself as an example, from single-celled life to the creation of DNA to the Cambrian Explosion, right on up to the evolution of sentience \u2013 a pattern that also follows an exponential curve. That being said, even for the most technical of innovations, the most important component systems of the complex system of systems we call an organization are people, not technology subsystems. Innovation, after all, is a human endeavor. Technology innovation is itself a set of organizational processes. What all of these processes have in common \u2013 a property, in fact, of all complex systems \u2013 is self-organization. Systems as large as galaxies and as small as the cells in your body self-organize. Sentience isn\u2019t required \u2013 and paradoxically, often gets in the way, as I\u2019ve discussed in a recent article on self-organizing teams. Sometimes, however, these well-dressed primates walking the planet get it right. People ask Google their secret to innovation. The answer: self-organization. The same for NetFlix. And if they can do it, so can you. Whether such self-organization will lead to exponential improvements, however, is an open question. It\u2019s essential to the continuous delivery model that characterizes DevOps \u2013 but progress at even the best-run DevOps organizations can only go so fast. Clearly there\u2019s more to applying the law of accelerating returns to organizational change \u2013 but that discussion will have to wait till a future Cortex newsletter. Stay tuned. Jason Bloomberg is President of Intellyx. Go Back to Top. Skip To: Start of Article."
        },
        {
            "article_published": "01/2015",
            "source": "https://www.breitbart.com/politics/2015/01/05/deep-learning-gaze-into-the-web-abyss-and-it-gazes-also-into-you/",
            "relevance_score": 0.1009039431810379,
            "content": "Americans are accustomed to the dominance of Google, Microsoft, and Yahoo as search engines, but on the global stage, a Chinese service called Baidu is now second only to Google in popularity. Baidu hasn\u2019t announced any firm plans to move into the U.S. market yet, but, in addition to the huge market in China, they\u2019ve expanded services to countries such as Egypt, Thailand, and most recently Brazil. They\u2019ve also recently hired away one of Google\u2019s top researchers, Andrew Ng, a specialist in artificial intelligence who has taught courses at Stanford University. As Ng explained in an interview with VentureBeat, Baidu\u2019s claim to fame is their search engine\u2019s reliance on \u201cdeep learning\u201d algorithms. In other words, their products learn what users want by analyzing their requests and what they do with the information. To some extent, every search engine and social media platform is trying to learn from its users, producing a certain degree of discomfort among those who feel their Internet tools are spying on them. The deep learning movement wants to take this idea of \u201cliving\u201d software further, creating systems that digest enormous amounts of data very quickly, without much human intervention, producing highly customized experiences for each individual user. It is hoped that the benefits from this level of swift, automatic customization, and the fact that it\u2019s all being done by faithful, computerized servants instead of intrusive, human programmers, will overcome consumer unease. One might also suppose that much of the early work is being carried out with customers who are, for better or worse, less nervous about having their online activities monitored than Americans and Europeans. If companies like Baidu can realize the ambitious plans of visionary designers like Ng and bring polished, incredibly useful living software perfected overseas to American audiences, consumer resistance could be minimal. As the old saying goes, nothing succeeds like success. The big American tech companies are also interested in developing this kind of technology, and while power players like Google are certainly swimming in resources, it doesn\u2019t take much reading between the lines to get the idea that Ng left Google because they\u2019ve become too hidebound and bureaucratic. Ng talks about receiving needed resources much faster than his unit at Google provided, getting things done without sitting through tedious committee meetings. He can frolick through Chinese markets with much faster birth-death cycles for online products than is typical in the West, and he has a field day hiring away other top artificial intelligence researchers to join his team. To get an idea of what this deep learning software would be like, the VentureBeat article on Ng references the recent movie \u201cHer,\u201d a near-future science fiction film in which Scarlett Johansson voiced an artificially intelligent digital assistant whose tireless efforts to relate with her user, and enhance the quality of his life, blossomed into an actual romance. (Not to spoil anything for those who have not seen the film, but hopefully deep learning researchers are pondering the ultimate resolution of that romance at great length.) The key feature of these next generation systems over existing smart search engines and voice-activated smartphones is the causal grace of their relationship with users. Computer systems have grown steadily easier to use with each passing decade, long ago reaching the point of widespread consumer acceptance by growing friendly enough for average, non-techie folks to use reflexively on a daily basis. One indicator of how user-friendly consumer systems have become is that no one really talks about \u201cuser friendliness\u201d any more. It was the sizzling-hot buzz phrase of the computer industry not very long ago, but it is now well-understood that if an application isn\u2019t simple enough for most people to figure out with a casual glance and a few curious mouse clicks, it\u2019s not going to break through to a mass audience. Cryptic text-based systems have given way to graphical user interfaces that didn\u2019t really work\u2014they tended to cripple the machines they were running on. Eventually graphical interfaces were perfected, and now they\u2019re ubiquitous, running on handheld devices with incredibly convenient touch screens. Voice command is the next step, but even the most gee-whiz voice apps today, like Apple\u2019s famed Siri, are just a shadow of what they could be. Greater accuracy is the key to making smart search engines and voice applications work, and that\u2019s what Baidu hired Andrew Ng to work on. At the moment, the ability of online systems to adjust themselves automatically to suit user preferences is fairly crude. Much of the learning is based on what users request of a particular system\u2026 but people don\u2019t always know what they really want. A deep learning system would study what they actually do with the data they request, build a network of preferences from many different data sources, consider the preferences of similar users, and learn to interpret the personal subtleties of spoken language. The personal assistant envisioned in \u201cHer\u201d displays these qualities\u2014she can anticipate what her user wants based on very vague requests, she adjusts her behavior to meet his demonstrated preferences, she understands the nuances of spoken conversation (there are some cleverly written early interactions where she asks questions of her user to figure out when he\u2019s being sarcastic, what he really means when he uses certain figures of speech, and so forth.) The ability to accurately pull meaningful data from the random noise of human behavior is crucial to making such features work. Living human beings don\u2019t actually handle these tasks with a very high degree of accuracy, unless they know each other very well. The speed and power of artificially intelligent computer systems, combined with the endless patience of machine intelligence, could make them much better at becoming everyone\u2019s close personal friend. With these capabilities perfected, computer systems would cross the final user-friendliness bridge and begin doing more than half of the work to maintain a relationship with humans. As it stands, even the most easy-to-use interfaces require us to learn how the system thinks\u2014we have to figure out where the menus are, perhaps learn a few shortcut keys, learn the peculiarities of each system\u2019s tools for such routine tasks as file uploads, and learn how we can configure the interface to suit our personal tastes. One reason we tend to think of modern systems as more user-friendly is that the typical user is far more comfortable with learning his way around an interface than his father was; an online generation is coming of age, and it\u2019s more adventuresome and patient than the eighties and nineties executives who howled in frustration at the inscrutable but indispensable machines on their desks. When computer systems are able to shoulder most of the interaction burden, and we can use them as casually as we would request help from a trusted human assistant, the next computer revolution will be at hand. Artificial intelligence is still an argument\u2014there are those who believe it will never be anything but an illusion, that talk of \u201cneural networks\u201d is just slick marketing for really fast computers. We currently have search engines that work extremely well; both user experiences and uninvited advertising have been tailored to individual preferences. Such tasks as piloting an automobile, routinely accomplished by millions of humans without extensive training or individual genius, remain beyond the capability of the smartest computer system; the ability of an incredibly complicated machine to briefly fool a panel of human judges into thinking they are holding a conversation with a human child is celebrated as a landmark A.I. triumph. The practical application of such technology remains elusive, and yesterday\u2019s promised miracles remain science fiction. But then again, many electronic conveniences we currently take for granted were science fiction just twenty years ago. It\u2019s interesting to watch where high-rolling tech companies are placing their bets, and they all seem convinced that deep learning systems are worth investing sizable sums of money in. The problem for American investors is that it doesn\u2019t seem like our domestic giants are nimble and adventurous enough to keep up with the work being done by the big Chinese companies. At least, that\u2019s how Andrew Ng placed his bets."
        },
        {
            "article_published": "01/2015",
            "source": "https://www.breitbart.com/europe/2015/01/21/civil-war-brewing-for-the-cultural-left/",
            "relevance_score": 0.05474191904067993,
            "content": "It has often been remarked that the right won the economic arguments of the twentieth century, while the left won the culture war. Although Thatcher and Reagan succeeded in their quest to overturn the postwar economic consensus and undermine the USSR, the left consistently triumphed over social conservatives in political debates on society and culture. Throughout the 1980s and the 1990s, the left successfully positioned itself as the guardian of liberty and reason against a dogmatic and authoritarian \u201cmoral majority\u201d. Moderates and liberals could not understand why the right wanted to deny gay people the right to marry, or women the right to an abortion. Nor could they understand the conservative quest to pull the theory of evolution from primary schools, or the regular campaigns by conservative moral crusaders against filth, blasphemy and even Satanism (1, 2) in popular culture. Against such opponents, it was relatively easy for the left to position itself as the defenders of academic inquiry, artistic expression and personal freedom. But the sands are beginning to shift. The coalition of moderate liberals, sceptical intellectuals, and radical progressives that once stood together against the conservative \u201cmoral majority\u201d is beginning to fracture. In the absence of a compelling external opponent, the internal tensions of this coalition are becoming more visible. While it is too soon to say if the revolution is about to consume itself, a number of serious divisions have emerged on the cultural left. And they are becoming increasingly bitter. Religion Richard Dawkins, Sam Harris, Daniel Dennett and Christopher Hitchens were once known as the \u201cFour Horsemen\u201d of New Atheism. For a long while, there was nothing more amusing to a young liberal than watching one of them debate against a creationist, or someone who objected to abortion or gay marriage on religious grounds. Dawkins, for a while, was the darling of the British media. Then things started to sour. Christopher Hitchens, in his full-throated defences of the second Iraq war, was the first to lose left-wing support. Notoriously, Feminist Frequency producer Jonathan McIntosh celebrated Hitchens\u2019 death, saying he was a \u201cdespicable, warmongering, hateful human being. Good riddance.\u201d (To put that in perspective, McIntosh had just a few months earlier refused to celebrate the death of Osama Bin Laden.) Dawkins, who recently discovered the joys of deliberately offending people on Twitter, has become an even greater figure of hate for progressives. This is probably due to his indiscriminate rationalism: he is just as willing to poke holes in theories of post-modern feminism as he is to attack religion. And when he does attack religion, he insists that Islam is probably the worst one out there. He has become persona non grata in progressive circles as a result. 2014 saw atheists and progressives embroiled in what looked like an all-out war. Ayaan Hirsi Ali, a female genital mutilation survivor and one of the fiercest critics of Islam in the atheist movement, was disinvited from a planned speaking engagement at Brandeis University for her criticism of Islam, and was stripped of her honorary degree. Salon.com immediately applauded the decision. Students at UC Berkeley attempted to do the same to Bill Maher over his alleged islamophobia, but were stopped by the college administration. Sam Harris, another of the \u201cfour horsemen\u201d, felt compelled to engage in a three-hour debate with progressive commentator Cenk Uygur after enduring a wave of hatchet-jobs from media progressives for his own comments on Islam. Progressives may be overwhelmingly atheist, but there is only so much heresy they can stand. One of their core beliefs is that you do not \u201cpunch down\u201d\u2013that is, attack vulnerable or marginalised communities. Islam, despite being the dominant religion of dozens of nation-states, is said by progressives to fall into this category. But many atheists don\u2019t buy it. And so they continue, with creationists to the right and progressives to the left, blaspheming against the beliefs of both. As Christianity declines and Islam grows, it is progressives, constantly impeding criticism of the latter, who may prove to be a bigger thorn in the side of atheism than conservatives ever were. Due process During the Bush administration, liberals eagerly positioned themselves as champions of the rights of the accused: specifically, those accused of plotting terrorism. For the left, Guantanamo Bay became a byword for a new authoritarian lawlessness, in which jury trials were a thing of the past and punishment was meted out on suspicion alone. These days, however, defenders of due process are more likely to be at loggerheads with radical progressives than Bush-era neocons. Nowadays, it is progressives, not conservatives, who championed the use of campus tribunals to deal with sexual assault on US campuses. These tribunals, conducted by untrained faculty members, with no requirement for defendants to have access to legal representation, have attracted a growing tide of criticism, as well as a number of lawsuits. In the UK, progressives have also lent their support to prosecutors\u2019 efforts to interfere in the jury system, most recently by calling on judges to tackle the \u201cunconscious biases\u201d of juries. While progressives on Twitter and the blogosphere rejoice in the assault on due process, others on the left are not so enthusiastic. Last summer, 28 Harvard lawyers, most of them liberal, came out against the university\u2019s attempts to hand control of sexual assault cases to a single \u201cTitle IX compliance officer.\u201d In the aftermath of Rolling Stone\u2019s disastrous story on a hoax rape claim at the University of Virginia, even more liberals\u2013including Slate\u2019s Emily Yoffe and former Salon writer Richar d Bradley\u2013began to speak out against the new atmosphere of vigilantism and automatic credulity sometimes referred to as \u201clisten and believe.\u201d Meanwhile, progressives continue to call for \u201caffirmative consent\u201d laws that would redefine sexual assault to include any form of sexual contact that is not explicitly, verbally consented to. Ezra Klein, the arch-progressive editor of Vox, accepted that such laws are \u201cterrible\u201d and would convict people for \u201cgenuinely ambiguous situations\u201d \u2013 but, incredibly, also said that this was OK. In order to prevent sexual assault, wrote Klein, it was necessary for the law to \u201ccreate a world in which men are afraid.\u201d Bad laws that could be enforced arbitrarily were, in his view, a great way of accomplishing that. Other liberals understandably recoiled in horror at this line of reasoning. As doubts about the data behind the \u201ccampus rape epidemic\u201d grow, battle lines are being drawn between progressives like Klein and liberals like Yoffe, Bradley, and the Harvard lawyers. The crusade to change culture \u2013 the very heart of the progressive mission \u2013 is on a collision course with due process, and perhaps liberalism itself. Censorship The blood was scarcely cold on the corpses of the murdered Charlie Hebdo cartoonists before progressives began to call them racist. Among them was Jonathan McIntosh, our Hitchens-hating friend from the previous section, leading some bloggers to dub him \u201cJihad Jonathan.\u201d But he wasn\u2019t alone. Around the world, it soon became apparent that a number of news organisations, including Sky, CNN, the New York Times and the Daily Telegraph were refusing to run the cartoons. on the grounds of religious offence. It is understandable for these outlets to be afraid of people with guns. What is less understandable to many liberals is being afraid of people of people with petitions. The left-libertarian journal Spiked, which has become a focal point for people dissatisfied with the mainstream left, recently wrote a biting piece of satire outlining how a mix of student boycotts and Change.org petitions might have ended Charlie Hebdo altogether, had it been published in the UK. The article was shared over four thousand times. But it\u2019s not just Brendan O\u2019Neill who\u2019s noticed the rise of the \u201cStepford Students\u201d. His longtime opponent, the liberal columnist Nick Cohen has been warning about the very same thing. Chris Rock, also a liberal, recently revealed that he no longer performs comedy for student audiences, arguing that they were too \u201cconservative\u201d in the way they handled offensive content. His use of the word \u201cconservative\u201d is telling. For decades, it was social conservatives who put pressure on private institutions to censor material that offended them. Cinemas that screened Monty Python movies were boycotted, panics were stoked about the \u201csatanism\u201d of Dungeons & Dragons, and born-again Christians led campaigns against violent videogames. Today, however, it is progressives who are not just standing up for the right of private censorship, but also actively demand it. It is progressives, not Christian conservatives, who now lead campaigns against sex and violence in the media. And it was progressive students, not middle-aged moral crusaders, who banned a pop song on over 20 university campuses. The #GamerGate uprising was in part a reaction to this culture of censorship, and they have already helped protect two video games \u2013 the controversial shooter Hatred, and the farming simulator Seedscape \u2013 from attempted boycotts. But despite rabid opposition from the left, #GamerGate \u2013 like Nick Cohen, like Bill Maher, and like Richard Dawkins \u2013 continues to identify with liberalism. The pro-censorship left and the anti-censorship left know they will never convince each other. Both sides have begun to dig trenches. Academia I have left the most surprising arena of left-vs-left conflict for last. Academia has long been assumed by those on the right to be an impenetrable fortress of progressive dogma. But times are changing. Most intriguing of all is that it is in the social sciences where the most change is taking place. I remember my own surprise, as a young undergraduate, to find an entire module on evolutionary psychology in the social science reading list. I was also surprised to see professors excitedly recommending Daniel Kahneman\u2019s Thinking, Fast and Slow to students. Most surprising of all was when my sociology tutor \u2013 a leading egalitarian theorist \u2013 informed our class that he thought social construction theory was \u201cmostly wish-wash.\u201d Those who are unfamiliar with the nature-vs-nurture debate will probably not understand why such a statement is controversial. Cognitive and genetic scientists, and evolutionary theorists, have long been viewed with suspicion by sociologists. After all, one of the chief projects of cognitive and evolutionary scientists in the past two decades has been the dismantling of the standard social science model, the theoretical framework that looks to external influences (nurture) to explain human behaviour, as opposed to genes or other innate factors (nature). If my university is anything to go by, however, even social scientists themselves are beginning to see flaws in the old model. This is bad news for progressives. The idea that human minds are infinitely malleable, and that the human behaviour can be altered simply by changing the social environment, underpins almost every progressive campaign \u2013 from No More Page Three to non-selective schooling. This is no accident: anyone who wishes to radically change the world must, on some level, believe that human nature can be altered. Frightened by the growing weakness of their flagship theories, progressives on campus have begun to lash out. One of the biggest controversies was in 2005, when then-President of Harvard University, Larry Summers, was faced with a motion of no confidence after suggesting that innate differences between the genders should be a line of inquiry when analysing the gender pay gap. The motion passed, and it left serious scars in the academic community. \u201cGood grief, shouldn\u2019t everything be within the pale of legitimate academic discourse?\u201d asked Steven Pinker shortly after the controversy. \u201cThat\u2019s the difference between a university and a madrasa.\u201d There have been no comparable controversies since then, but a stream of outrage continues to follow the work of Pinker, Simon Baron-Cohen, Robert Plomin, Nicholas Wade and anyone else who investigates the idea of innate differences between persons and groups. It doesn\u2019t matter how much they stress their commitment to liberalism and egalitarianism (which they do, frequently), nothing can calm their opponents. Academics aren\u2019t usually minded to make snap political judgments on a whim. But there is a growing perception that some on the progressive left are just as committed to their dogmas as creationists are to theirs. With the standard social science model increasingly looking like a relic of the 1960s, the fierceness with which some progressives continue to cling to it will determine the fierceness of future divisions in academia, and, by extension, the left. The future of the culture wars As 21st-century progressives begin to embrace many of the tactics, arguments, and moral panics employed by 20th-century conservatives, the old distinctions in the culture wars begin to lose their relevance. In a number of arenas, the cultural left is ignoring conservatives and has begun to fight itself. The question for the right is: what to do? There is no doubt room for common ground with liberal atheists who want to criticize Islam, with liberal lawyers who want to protect due process and with content producers fighting the new, petition-led censorship. Opposition to political correctness and a respect for individual rights have always been strong traditions on the right. On the other hand, appealing to liberal, atheist, Grand Theft Auto V fans without losing the support of social conservatives could prove difficult. Nevertheless, it increasingly appears that cultural politics, once the great strength of the left-wing movement, is rapidly turning into its Achilles heel. Once a source of unity, it has turned into perhaps the primary source of division. With moderate liberals and radical progressives sharpening their weapons on a number of fronts, a battle for the soul of the left is about to begin."
        },
        {
            "article_published": "01/2015",
            "source": "https://www.breitbart.com/europe/2015/01/02/how-feminist-propaganda-is-destroying-mens-lives/",
            "relevance_score": 0.052497051656246185,
            "content": "Otherwise baffled as to why the STEM fields of science, technology, engineering and maths aren\u2019t crammed full of women like psychology, social work and education, feminist activists have been leading a noisy campaign over the past few years to paint men in these fields, entirely inaccurately, as raging misogynists \u2013 even going to the media with the word \u201cbrogrammer,\u201d to convey their annoyance. While the most gullible media outlets took the bait, even left-leaning Gizmodo hit back with a piece called, \u201cThere\u2019s No Such Thing as a Brogrammer\u201d. Nagging and haranguing about supposed sexism has only been getting louder, the pinnacle of which was reached when scientist Dr Matt Taylor was reduced to apologising in tears after wearing a mildly risqu\u00e9 shirt on camera to celebrate his achievement of landing a spacecraft on a comet. On Twitter, a woman proclaimed that the shirt had signalled that women \u201cweren\u2019t welcome in science,\u201d which over 1,000 of the usual suspects retweeted with feverish enthusiasm. One computer scientist, Scott Aaronson, took issue with the overwhelming narrative being cooked up that men working in the STEM fields were misogynist pigs with no knowledge of the necessary and beautiful art of feminist discourse, and, writing a very personal account of his adolescence and youth, he described reading dozens of feminist books, \u201cstudies and task reports about the \u2018privilege\u2019 and \u2018entitlement\u2019 of the nerdy males that\u2019s keeping women away from science\u201d and feminist blogs. Comment #171 subsequently gained a lot of attention on Twitter, Facebook and Reddit. From when he was 12, up until his mid-twenties, far from the \u201centitled\u201d attitude to women that feminists accuse men working in STEM of having, Aaronson, having read these books, having been to \u201csexual-assault prevention workshops\u201d as an undergrad, was terrified of even talking to women so fearful was he his intentions could be misconstrued and have him deemed a demonic rape fiend. Desperate at this point to have been born a woman or a gay man so he didn\u2019t have to bear the burden of being a heterosexual white male, seeping malevolent privilege and doomed to stalk the planet oppressing all those to cross his path, Aaronson \u201cscoured the feminist literature for any statement to the effect that [his] fears were as silly as [he] hoped they were,\u201d only to find the opposite. \u201cI found reams of text about how even the most ordinary male/female interactions are filled with \u201cmicroaggressions,\u201d and how even the most \u201cenlightened\u201d males\u2014especially the most \u201cenlightened\u201d males, in fact\u2014are filled with hidden entitlement and privilege and a propensity to sexual violence that could burst forth at any moment.\u201d Suicidal, such was Aaronson\u2019s trauma, he even tried to acquire a prescription for chemical castration, so that he could focus on devoting his entire life to maths and have no reason to be seen as a predator. Aaronson ends his piece with the by poignantly noting that while, no, the trauma of sexual assault and the trauma he went through in his 15 years of \u201clife-destroying anxiety\u201d weren\u2019t comparable but that with his, \u201cthere are no academics studying it, no task forces devoted to it, no campus rallies in support of the sufferers, no therapists or activists to tell you that you\u2019re not alone or it isn\u2019t your fault. There are only therapists and activists to deliver the opposite message: that you are alone and it is your privileged, entitled, male fault.\u201d While obviously tragic for him on a personal level, Aaronson\u2019s blog comment has wider, more serious and indeed urgent implications. Universities are rolling out these sexual consent workshops, which will likely become compulsory. Aaronson said he left each of those workshops \u201cwith enough fresh paranoia and self-hatred to last through another year,\u201d and there is a petition by the Everyday Sexism and End Violence Against Women Coalition to have them made compulsory in all schools. Terrifyingly, the petition sets out a curriculum of \u201csexual consent, healthy and respectful relationships, gender stereotypes and online pornography\u201d. Given that for feminists, there is only one possible viewpoint to take on all these things, the petition basically ensures that third-wave feminism is beamed straight into children\u2019s heads. The scariest thing about this petition is that apart from David Cameron and Nigel Farage, all the other party leaders have wholeheartedly backed it. Aaronson isn\u2019t alone in being negatively affected by feminist propaganda. He received many grateful emails from guys in the same position, for giving them hope they too could one day make a success of themselves, and when I read Scott\u2019s piece, I was immediately put in mind of a man in his 20s I knew through the games platform Steam, in 2011. Having lurked on the internet a lot and got sucked into reading feminist blogs and websites for days and weeks on end, this man since his late teens had suffered from such crippling social anxiety he was living on disability benefits and unable to leave the house. If children throughout the country are to be conditioned in schools with this sort of poisonous thinking, this kind of tragedy will become far more commonplace. Yet instead of thinking perhaps feminists would be able to learn from cases such as Aaronson\u2019s, the response he got from them was brutal. Amanda Marcotte claimed his post was a \u201cyalp of entitlement\u201d from someone who saw women only as sex toys, and her piece was met with rapturous applause. Laurie Penny, whose response was deemed by many to be too kind, rather than actually showing any empathy instead managed to talk about how hard it was to be a \u201chorny\u201d, \u201csocially awkward\u201d, nerdy girl growing up, and that as a female, when she wanted to enter a \u201clife of the mind\u201d she found the patriarchy standing in her way. I find this curious, since women are a third more likely to go to university, get well over half of all Master\u2019s degrees and just over half of all PhDs. But anyway, the upshot is, that Aaronson\u2019s problems with social anxiety were caused by, according to Penny\u2026 the patriarchy! Aaronson\u2019s trauma was completely dismissed and replaced with a diatribe about her own experiences growing up, and then she completely ignored what had actually caused his anxiety and twisted it round to pretend the culprit was the ever imaginary \u201cpatriarchy\u201d. Reading Penny\u2019s piece and, especially, Marcotte\u2019s gives an unsettling window into the minds of modern feminists. Even when a man reveals his vulnerabilities and writes with heartbreaking honesty about a cripplingly awful period of his life, they seem pathologically unable to see why he was suffering, and, in the case of Marcotte, that he was suffering at all. It\u2019s as though unless men and boys adhere to their exact way of thinking, they\u2019re inhuman and unworthy of empathy. Rather than filling more young people\u2019s heads with the acrid fantasies of third wave feminists that foster distrust and disquiet between the sexes, venomous ideas like \u201cprivilege theory\u201d and \u201crape culture\u201d should go the way of the woolly mammoth. Only then can we be closer to equality."
        }
    ]
}
