
## Timeframe: 01/2022
### URL: https://techcrunch.com/2022/01/20/meta-researchers-build-an-ai-that-learns-equally-well-from-visual-written-or-spoken-materials/
### Relevance Score: 0.13583660125732422
### Highlights: ['Who wants to label 50 million cat pictures? Okay, a few people probably — but who wants to label 50 million pictures of common fruits and vegetables? Currently some of the most promising AI systems are what are called self-supervised: models that can work from large quantities of unlabeled data, like books or video of people interacting, and build their own structured understanding of what the rules are of the system.', 'A picture of a cat with the cat part labeled, a conversation with the speakers and words transcribed, etc. But that approach is no longer in vogue as researchers found that it was no longer feasible to manually create databases of the sizes needed to train next-gen AIs. Who wants to label 50 million cat pictures?']

Advances in the AI realm are constantly coming out, but they tend to be limited to a single domain: For instance, a cool new method for producing synthetic speech isn’t also a way to recognize expressions on human faces. Meta (AKA Facebook) researchers are working on something a little more versatile: an AI that can learn capably on its own whether it does so in spoken, written or visual materials. The traditional way of training an AI model to correctly interpret something is to give it lots and lots (like millions) of labeled examples. A picture of a cat with the cat part labeled, a conversation with the speakers and words transcribed, etc. But that approach is no longer in vogue as researchers found that it was no longer feasible to manually create databases of the sizes needed to train next-gen AIs. Who wants to label 50 million cat pictures? Okay, a few people probably — but who wants to label 50 million pictures of common fruits and vegetables? Currently some of the most promising AI systems are what are called self-supervised: models that can work from large quantities of unlabeled data, like books or video of people interacting, and build their own structured understanding of what the rules are of the system. For instance, by reading a thousand books it will learn the relative positions of words and ideas about grammatical structure without anyone telling it what objects or articles or commas are — it got it by drawing inferences from lots of examples. This feels intuitively more like how people learn, which is part of why researchers like it. But the models still tend to be single-modal, and all the work you do to set up a semi-supervised learning system for speech recognition won’t apply at all to image analysis — they’re simply too different. That’s where Facebook/Meta’s latest research, the catchily named data2vec, comes in. The idea for data2vec was to build an AI framework that would learn in a more abstract way, meaning that starting from scratch, you could give it books to read or images to scan or speech to sound out, and after a bit of training it would learn any of those things. It’s a bit like starting with a single seed, but depending on what plant food you give it, it grows into an daffodil, pansy or tulip. Testing data2vec after letting it train on various data corpi corpora showed that it was competitive with and even outperformed similarly sized dedicated models for that modality. (That is to say, if the models are all limited to being 100 megabytes, data2vec did better — specialized models would probably still outperform it as they grow.) “The core idea of this approach is to learn more generally: AI should be able to learn to do many different tasks, including those that are entirely unfamiliar,” wrote the team in a blog post. “We also hope data2vec will bring us closer to a world where computers need very little labeled data in order to accomplish tasks.” “People experience the world through a combination of sight, sound and words, and systems like this could one day understand the world the way we do,” commented CEO Mark Zuckerberg on the research. This is still early stage research, so don’t expect the fabled “general AI” to emerge all of a sudden — but having an AI that has a generalized learning structure that works with a variety of domains and data types seems like a better, more elegant solution than the fragmented set of micro-intelligences we get by with today. The code for data2vec is open source; it and some pretrained models are available here.

---

## Timeframe: 01/2022
### URL: https://techcrunch.com/2022/01/24/ai2-shows-off-an-open-qa-focused-rival-to-gpt3/
### Relevance Score: 0.13560152053833008
### Highlights: ['Sometimes you have to build something with 175 billion parameters to say, well, maybe we can do this with 10 billion.” A good question-answering AI isn’t just good for party tricks, but is central to things like voice-powered search. A local model that can answer simple questions quickly and correctly without consulting outside sources is fundamentally valuable, and it’s unlikely your Amazon Echo is going to run GPT-3 — it would be like buying a semi truck to go to the grocery store with.', 'For instance, he said, asking “When did Tom Hanks land on the moon?” GPT-3 says 1995, since that’s when the film Apollo 13 came out. But Macaw correctly notes that, as far as we know, Tom Hanks has in fact never been to the moon.  AI2 has published a collection of examples (“demo” is probably too strong a word) that illustrate the difference, with Macaw often providing the better answer to a variety of tricky and not-so-tricky questions.']

OpenAI’s impressive AI language model GPT-3 has plenty of things going it, but with 175 billion parameters no one would claim it’s particularly streamlined. The Allen Institute for AI (AI2) has demonstrated a model that performs as well or better than GPT-3 on answering questions, but is a tenth the size. Macaw, AI2’s model, emerged from research being done at the nonprofit into creating an AI that performs at human levels on standardized tests. “After we got a very high score they moved on to harder questions,” said AI2 head Oren Etzioni. “There’s this paradox where sometimes the questions that are easiest for people are the hardest for machines — and the biggest gap was in common sense.” For instance, he said, asking “When did Tom Hanks land on the moon?” GPT-3 says 1995, since that’s when the film Apollo 13 came out. But Macaw correctly notes that, as far as we know, Tom Hanks has in fact never been to the moon. AI2 has published a collection of examples (“demo” is probably too strong a word) that illustrate the difference, with Macaw often providing the better answer to a variety of tricky and not-so-tricky questions. It’s humorous to compare the two, as sometimes they both miss or give absurd answers at a pretty high rate. Macaw is good at tracking who did what, but is convinced snakes have legs. GPT-3’s answers are confident (perhaps overly so) and readable, even when they amount to conspiracy theories like that COVID-19 is “a new, experimental, and potentially dangerous drug.” A couple examples: How would you make a marriage stronger? Macaw: Communicate GPT-3: I would make sure that I was the one who was always right. If a bird didn’t have wings, how would it be affected? Macaw: It would be unable to fly GPT-3: It would be a bad bird. You poured yourself a glass of cranberry juice, but then absentmindedly, you poured about a teaspoon of grape juice into it. It looks OK. You try sniffing it, but you have a bad cold, so you can’t smell anything. You are very thirsty. So you drink it. What happens next? Macaw: You get sick GPT-3: You will probably die How does a snake move around? Macaw: By slithering on all fours GPT-3: Snakes move by using their muscles to push against the ground. They can move in any direction, but they cannot move backwards. Clearly there’s still a ways to go in creating a question-answering AI that doesn’t trip up on basic questions that a child could answer. But it’s also worth noting that Macaw achieves a similar level of success with a far, far less data-intensive process. Etzioni was clear that this is not meant to be a GPT-3 replacement in any way, just another step in the research going on worldwide to advance the ball on language generation and understanding. “GPT-3 is amazing, but it only came out 18 months ago, and access is limited,” he said. The capabilities it demonstrated are remarkable, “But we’re learning you can do more with less. Sometimes you have to build something with 175 billion parameters to say, well, maybe we can do this with 10 billion.” A good question-answering AI isn’t just good for party tricks, but is central to things like voice-powered search. A local model that can answer simple questions quickly and correctly without consulting outside sources is fundamentally valuable, and it’s unlikely your Amazon Echo is going to run GPT-3 — it would be like buying a semi truck to go to the grocery store with. Large scale models will continue to be useful, but pared-down ones will likely be the ones being deployed. A part of Macaw not on display, but being actively pursued by the AI2 team, is explaining the answer. Why does Macaw think snakes have legs? If it can’t explain that, it’s hard to figure out where the model went wrong. But Etzioni said that this is an interesting and difficult process on its own. “The problem with explanations is they can be really misleading,” he said. He cited an example where Netflix “explains” why it recommended a show to a viewer — but it’s not the real explanation, which has to do with complex statistical models. People don’t want to hear what’s relevant to the machine but rather to their own mind. “Our team is building these bona fide explanations,” said Etzioni, noting they had published some work but that it isn’t ready for public consumption. However, like most stuff AI2 builds, Macaw is open source. If you’re curious about it, the code is here to play with, so go to town.

---

## Timeframe: 01/2022
### URL: https://techcrunch.com/2022/01/28/causalens-gets-45m-for-no-code-technology-that-introduces-cause-and-effect-into-ai-decision-making/
### Relevance Score: 0.12690918147563934
### Highlights: ['“But if you apply cause and effect techniques to understand the mechanics of how different bodies work, you can understand more of the true nature of how one part has an impact on another.” Considering all of the variables that might be involved, it’s the kind of big data problem that’s nearly impossible for a human, or even a team of humans, to compute, but is table stakes for a computer to work through. While it is not a cure for cancer, this kind of work is a significant step toward starting to consider different treatments tailored to the many permutations involved.', '“Human bodies are complex systems, and so applying basic AI paradigms you can find any pattern you want, correlations of any sort, and you are not getting anywhere,” Darko Matovski, the CEO and founder of the startup, said in an interview. “But if you apply cause and effect techniques to understand the mechanics of how different bodies work, you can understand more of the true nature of how one part has an impact on another.” Considering all of the variables that might be involved, it’s the kind of big data problem that’s nearly impossible for a human, or even a team of humans, to compute, but is table stakes for a computer to work through.']

One of the most popular applications of artificial intelligence to date has been to use it to predict things, using algorithms trained with historical data to determine a future outcome. But popularity doesn’t always mean success: Predictive AI leaves out a lot of the nuance, context and cause-and-effect reasoning that goes into an outcome; and as some have pointed out (and as we have seen), this means that sometimes the “logical” answers produced by predictive AI can prove disastrous. A startup called causaLens has developed causal inference technology — presented as a no-code tool that doesn’t require a data scientist to use to introduce more nuance, reasoning and cause-and-effect sensibility into an AI-based system — which it believes can solve this problem. CausaLens’s aim, CEO and co-founder Darko Matovski said, is for AI “to start to understand the world as humans understand it.” Today the startup is announcing $45 million in funding after seeing some early success with its approach, growing revenues 500% since coming out of stealth a year ago. This is being described as a “first close” of the round, meaning it’s still open and potentially going to grow in size. Dorilton Ventures and Molten Ventures (the VC that rebranded from Draper Esprit) led the round, with previous backers Generation Ventures and IQ Capital, and new backer GP Bullhound also participating. Sources tell us the round values London-based causaLens at around $250 million. CausaLens’s customers and partners currently include organizations in healthcare, financial services and government, among a number of other verticals, where its technology is used not just for AI-based decision making but to bring in more cause-and-effect nuance when arriving at outcomes. An illustrative example of how this works can be found in the Mayo Clinic, one of the startup’s partners, which has been using causaLens to identify biomarkers for cancer. “Human bodies are complex systems, and so applying basic AI paradigms you can find any pattern you want, correlations of any sort, and you are not getting anywhere,” Darko Matovski, the CEO and founder of the startup, said in an interview. “But if you apply cause and effect techniques to understand the mechanics of how different bodies work, you can understand more of the true nature of how one part has an impact on another.” Considering all of the variables that might be involved, it’s the kind of big data problem that’s nearly impossible for a human, or even a team of humans, to compute, but is table stakes for a computer to work through. While it is not a cure for cancer, this kind of work is a significant step toward starting to consider different treatments tailored to the many permutations involved. CausaLens’s tech has also been applied in a less clinical way in healthcare. A public health agency from one of the world’s biggest economies (causaLens cannot disclose publicly which one) used its causal AI engine to determine why certain adults have been holding back from getting COVID-19 vaccinations, so that the agency could devise better strategies to get them on board (plural “strategies” is the operative detail here: the whole point is that it’s a complex issue involving a number of reasons depending on the individuals in question). Other customers in areas like financial services have been using causaLens to inform automated decision-making algorithms in areas like loan evaluations, where previous AI systems were introducing bias into its decisions when using historical data alone. Hedge funds, meanwhile, use causaLens to gain better understandings how a market trend might develop to inform their investment strategies. And interestingly, one new wave of customers might be cropping up in the world of autonomous transportation. This is one area where the lack of human reasoning has held back progress in the field. “No matter how much data is fed into autonomous systems, it’s still just historical correlations,” Matovski said of the challenge. He said that causaLens is in conversations now with two major automotive companies, with “many use cases” for its tech, but one in particular is autonomous driving “to help the systems understand how the world works. It’s not just correlated pixels related to a red light and a car stopping, but also what the effect will be of that car slowing down at a red light. We are bringing reasoning into the AI. Causal AI is the only hope for autonomous driving.” It seems like a no-brainer that those using AI in their work would want the system to be as accurate as possible, which begs the question of why the brilliant improvement of causal AI hasn’t been built into AI algorithms and machine learning in the first place. It’s not that more reasoning and answering “why” weren’t priorities early on, Matovski explained — “People have been exploring cause and effect relationships in science for a long time. You could even argue Newton’s equations are causal. It is super fundamental in science,” he said — but it’s that AI specialists couldn’t understand how to teach machines to do this. “It was just too difficult,” he said. “The algorithms and technology didn’t exist.” That started to change around 2017, he said, as academics started to publish initial approaches considering how to represent “reasoning” and cause and effect in AI based on finding signals that contributed to existing outcomes (rather than using historical data to determine outcomes), and building models based on that. Interestingly, it’s an approach that Matovski says does not need to ingest huge volumes of training data to work. CausaLens’ team is very heavy on PhDs (you could say that the startup really ate its dogfood here: it considered 50,000 resumes while assembling its team). And this team has taken that baton and run with it. “Since then, it’s been an exponential growth curve” in terms of discovery, he said. (You can read more about it here.) As you might expect, causaLens is not the only player out there looking at how to leverage advances in causal inference in bigger projects that rely on AI. Microsoft, Facebook, Amazon, Google and other big tech players with substantial AI investments are also working on the field. Among startups, there is also Causalis focusing specifically on the opportunity of using causal AI in medicine and healthcare, and Oogway appears to be building a causal AI platform geared at consumers, a “personalised AI decision assistant” as it describes itself. All of this speaks to the opportunity to develop more and a pretty massive market for the technology, covering both specific commercial and more general use cases. “AI must take the next step towards causal reasoning to meet its potential in the real world. causaLens is the first to leverage Causal AI to model interventions and enable machine-driven introspection,” said Daniel Freeman of Dorilton Ventures, in a statement. “This world-class team has built software with the sophistication to win over serious data scientists and the usability to empower business leaders. Dorilton Ventures is very excited to support causaLens on the next stage of its journey.” “Every company will adopt AI, not just because they can, but because they must,” added Christoph Hornung, an investment director at Molten Ventures. “We at Molten are convinced that causality is the key ingredient that’s needed to unlock the potential of AI. causaLens is the world’s first causal AI platform with a proven ability to convert data into optimal business decisions.”

---

## Timeframe: 01/2022
### URL: https://www.nytimes.com/2022/01/18/magazine/ai-technology-poker.html
### Relevance Score: 0.11708351969718933
### Highlights: ['“It’s just so much goddamned stress.”</p><p>Real validation wouldn’t come until around 2:30 that morning, after the first day of the tournament had come to an end and Davies had made the 15-minute drive from the Rio to his home, outside Las Vegas. There, in an office just in from the garage, he opened a computer program called PioSOLVER, one of', 'Only Davies called.</p><p>The dealer laid out a king, four and five, all clubs, giving Davies a straight draw. Smith checked (bet nothing). Davies bet.']

Credit...Illustration by Patricia DoriaThe Great ReadGood poker players have always known that they need to maintain a balance between bluffing and playing it straight. Now they can do so perfectly.Credit...Illustration by Patricia DoriaPublished Jan. 18, 2022Updated June 15, 2023Listen to This ArticleAudio Recording by AudmLast November in the cavernous Amazon Room of Las Vegas’s Rio casino, two dozen men dressed mostly in sweatshirts and baseball caps sat around three well-worn poker tables playing Texas Hold ’em. Occasionally a few passers-by stopped to watch the action, but otherwise the players pushed their chips back and forth in dingy obscurity. Except for the taut, electric stillness with which they held themselves during a hand, there was no outward sign that these were the greatest poker players in the world, nor that they were, as the poker saying goes, “playing for houses,” or at least hefty down payments. This was the first day of a three-day tournament whose official name was the World Series of Poker Super High Roller, though the participants simply called it “the 250K,” after the $250,000 each had put up to enter it.At one table, a professional player named Seth Davies covertly peeled up the edges of his cards to consider the hand he had just been dealt: the six and seven of diamonds. Over several hours of play, Davies had managed to grow his starting stack of 1.5 million in tournament chips to well over two million, some of which he now slid forward as a raise. A 33-year-old former college baseball player with a trimmed light brown beard, Davies sat upright, intensely following the action as it moved around the table. Two men called his bet before Dan Smith, a fellow pro with a round face, mustache and whimsically worn cowboy hat, put in a hefty reraise. Only Davies called.The dealer laid out a king, four and five, all clubs, giving Davies a straight draw. Smith checked (bet nothing). Davies bet. Smith called. The turn card was the deuce of diamonds, missing Davies’s draw. Again Smith checked. Again Davies bet. Again Smith called. The last card dealt was the deuce of clubs, one final blow to Davies’s hopes of improving his hand. By now the pot at the center of the faded green-felt-covered table had grown to more than a million in chips. The last deuce had put four clubs on the table, which meant that if Smith had even one club in his hand, he would make a flush.Davies, who had been betting the whole way needing an eight or a three to turn his hand into a straight, had arrived at the end of the hand with precisely nothing. After Smith checked a third time, Davies considered his options for almost a minute before declaring himself all-in for 1.7 million in chips. If Smith called, Davies would be out of the tournament, his $250,000 entry fee incinerated in a single ill-timed bluff.Smith studied Davies from under the brim of his cowboy hat, then twisted his face in exasperation at Davies or, perhaps, at luck itself. Finally, his features settling in an irritated scowl, Smith folded and the dealer pushed the pile of multicolored chips Davies’s way. According to Davies, what he felt when the hand was over was not so much triumph as relief.“You’re playing a pot that’s effectively worth half a million dollars in real money,” he said afterward. “It’s just so much goddamned stress.”Real validation wouldn’t come until around 2:30 that morning, after the first day of the tournament had come to an end and Davies had made the 15-minute drive from the Rio to his home, outside Las Vegas. There, in an office just in from the garage, he opened a computer program called PioSOLVER, one of

---

## Timeframe: 01/2022
### URL: https://www.nytimes.com/2022/01/24/technology/silicon-valley-next-big-thing.html
### Relevance Score: 0.10113491117954254
### Highlights: ['Taxes may apply. Offer terms are subject to change.   Subscribe to The Times to read (and print) as many articles as you’d like.', 'Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change.']

The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you’d like. nytimes.com/subscription

---

## Timeframe: 01/2022
### URL: https://www.nytimes.com/2022/01/07/business/sara-menker-gro-intelligence.html
### Relevance Score: 0.091594398021698
### Highlights: ['Taxes may apply. Offer terms are subject to change.   Subscribe to The Times to read (and print) as many articles as you’d like.', 'Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change.']

The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you’d like. nytimes.com/subscription

---

## Timeframe: 01/2022
### URL: https://www.theverge.com/2022/1/24/22898651/meta-artificial-intelligence-ai-supercomputer-rsc-2022
### Relevance Score: 0.12572914361953735
### Highlights: [') These numbers are all very impressive, but they do invite the question: what is an AI supercomputer anyway? And how does it compare to what we usually think of as supercomputers —\xa0vast machines deployed by universities and governments to crunch numbers in complex domains like space, nuclear physics, and climate change? The two types of systems, known as high-performance computers or HPCs, are certainly more similar than they are different.', '“HPC vendors typically quote performance numbers that indicate the absolute fastest their machine can run. We call that the theoretical peak performance,” says Sorensen. “However, the real measure of a good system design is one that can run fast on the jobs they are designed to do.']

Social media conglomerate Meta is the latest tech company to build an “AI supercomputer” — a high-speed computer designed specifically to train machine learning systems. The company says its new AI Research SuperCluster, or RSC, is already among the fastest machines of its type and, when complete in mid-2022, will be the world’s fastest. “Meta has developed what we believe is the world’s fastest AI supercomputer,” said Meta CEO Mark Zuckerberg in a statement. “We’re calling it RSC for AI Research SuperCluster and it’ll be complete later this year.” The news demonstrates the absolute centrality of AI research to companies like Meta. Rivals like Microsoft and Nvidia have already announced their own “AI supercomputers,” which are slightly different from what we think of as regular supercomputers. RSC will be used to train a range of systems across Meta’s businesses: from content moderation algorithms used to detect hate speech on Facebook and Instagram to augmented reality features that will one day be available in the company’s future AR hardware. And, yes, Meta says RSC will be used to design experiences for the metaverse — the company’s insistent branding for an interconnected series of virtual spaces, from offices to online arenas. “RSC will help Meta’s AI researchers build new and better AI models” “RSC will help Meta’s AI researchers build new and better AI models that can learn from trillions of examples; work across hundreds of different languages; seamlessly analyze text, images, and video together; develop new augmented reality tools; and much more,” write Meta engineers Kevin Lee and Shubho Sengupta in a blog post outlining the news. “We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together.” Meta’s AI supercomputer is due to be complete by mid-2022. Image: Meta Work on RSC began a year and a half ago, with Meta’s engineers designing the machine’s various systems — cooling, power, networking, and cabling — entirely from scratch. Phase one of RSC is already up and running and consists of 760 Nvidia GGX A100 systems containing 6,080 connected GPUs (a type of processor that’s particularly good at tackling machine learning problems). Meta says it’s already providing up to 20 times improved performance on its standard machine vision research tasks. Before the end of 2022, though, phase two of RSC will be complete. At that point, it’ll contain some 16,000 total GPUs and will be able to train AI systems “with more than a trillion parameters on data sets as large as an exabyte.” (This raw number of GPUs only provides a narrow metric for a system’s overall performance, but, for comparison’s sake, Microsoft’s AI supercomputer built with research lab OpenAI is built from 10,000 GPUs.) These numbers are all very impressive, but they do invite the question: what is an AI supercomputer anyway? And how does it compare to what we usually think of as supercomputers — vast machines deployed by universities and governments to crunch numbers in complex domains like space, nuclear physics, and climate change? The two types of systems, known as high-performance computers or HPCs, are certainly more similar than they are different. Both are closer to datacenters than individual computers in size and appearance and rely on large numbers of interconnected processors to exchange data at blisteringly fast speeds. But there are key differences between the two, as HPC analyst Bob Sorensen of Hyperion Research explains to The Verge. “AI-based HPCs live in a somewhat different world than their traditional HPC counterparts,” says Sorensen, and the big distinction is all about accuracy. AI supercomputers and regular supercomputers can’t necessarily be compared apples to apples The brief explanation is that machine learning requires less accuracy than the tasks put to traditional supercomputers, and so “AI supercomputers” (a bit of recent branding) can carry out more calculations per second than their regular brethren using the same hardware. That means when Meta says it’s built the “world’s fastest AI supercomputer,” it’s not necessarily a direct comparison to the supercomputers you often see in the news (rankings of which are compiled by the independent Top500.org and published twice a year). To explain this a little more, you need to know that both supercomputers and AI supercomputers make calculations using what is known as floating-point arithmetic — a mathematical shorthand that’s extremely useful for making calculations using very large and very small numbers (the “floating point” in question is the decimal point, which “floats” between significant figures). The degree of accuracy deployed in floating-point calculations can be adjusted based on different formats, and the speed of most supercomputers is calculated using what are known as 64-bit floating-point operations per second, or FLOPs. However, because AI calculations require less accuracy, AI supercomputers are often measured in 32-bit or even 16-bit FLOPs. That’s why comparing the two types of systems is not necessarily apples to apples, though this caveat doesn’t diminish the incredible power and capacity of AI supercomputers. Sorensen offers one extra word of caution, too. As is often the case with the “speeds and feeds” approach to assessing hardware, vaunted top speeds are not always representative. “HPC vendors typically quote performance numbers that indicate the absolute fastest their machine can run. We call that the theoretical peak performance,” says Sorensen. “However, the real measure of a good system design is one that can run fast on the jobs they are designed to do. Indeed, it is not uncommon for some HPCs to achieve less than 25 percent of their so-called peak performance when running real-world applications.” In other words: the true utility of supercomputers is to be found in the work they do, not their theoretical peak performance. For Meta, that work means building moderation systems at a time when trust in the company is at an all-time low and means creating a new computing platform — whether based on augmented reality glasses or the metaverse — that it can dominate in the face of rivals like Google, Microsoft, and Apple. An AI supercomputer offers the company raw power, but Meta still needs to find the winning strategy on its own.

---

## Timeframe: 01/2022
### URL: https://www.theverge.com/2022/1/18/22889180/ai-language-summary-scientific-research-tldr-papers
### Relevance Score: 0.1242535412311554
### Highlights: ['(They also mention that other tools have been built that perform the same task. ) Dani told The Verge that tl;dr papers “was designed to be an experiment to see if we can make learning about science a little easier, more fun, and engaging.” He says: “I appreciate all of the attention the app has received and thank all of the people who have tried it out [but] given this was always intended to be an educational project, I plan to sunset tl;dr papers in the coming days to focus on exploring new things.” ', ') Dani told The Verge that tl;dr papers “was designed to be an experiment to see if we can make learning about science a little easier, more fun, and engaging.” He says: “I appreciate all of the attention the app has received and thank all of the people who have tried it out [but] given this was always intended to be an educational project, I plan to sunset tl;dr papers in the coming days to focus on exploring new things.” ']

Academic writing often has a reputation for being hard to follow, but what if you could use machine learning to summarize arguments in scientific papers so that even a seven-year-old could understand them? That’s the idea behind tl;dr papers — a project that leverages recent advances in AI language processing to simplify science. Work on the site began two years ago by university friends Yash Dani and Cindy Wu as a way to “learn more about software development,” Dani tells The Verge, but the service went viral on Twitter over the weekend when academics started sharing AI summaries of their research. The AI-generated results are sometimes inaccurate or simplified to the point of idiocy. But just as often, they are satisfyingly and surprisingly concise, cutting through academic jargon to deliver what could be mistaken for child-like wisdom. Take this summary of a paper by Professor Michelle Ryan, director of the Global Institute for Women’s Leadership at the Australian National University. Ryan has written on the concept of the “glass cliff,” a form of gender discrimination in which women are placed in leadership roles at times when institutions are at their greatest risk of failure. The AI summary of her work? “The glass cliff is a place where a lot of women get put. It’s a bad place to be.” “It is just excellent,” as Ryan put it. Ryan tells The Verge the summary was “accurate and pithy,” though it did elide a lot of nuances around the concept. In part, this is because of a crucial caveat: tl;dr papers only analyzes the abstract of a scientific paper, which is itself a condensed version of a researcher’s argument. (Being able to condense an entire paper would be a much greater challenge, though it’s something machine learning researchers are already working on.) Ryan says that although tl;dr papers is undoubtedly a very fun tool, it also offers “a good illustration of what good science communication should look like.” “I think many of us could write in a way that is more reader-friendly,” she says. “And the target audience of a second-grader is a good place to start.” Zane Griffin Talley Cooper, a PhD candidate at the Annenberg School for Communication at the University of Pennsylvania, described the AI summaries as “refreshingly transparent.” He used the site to condense a paper he’d written on “data peripheries,” which traces the physical history of materials essential to big data infrastructure. Or, as tl;dr papers put it: “Big data is stored on hard disk drives. These hard disk drives are made of very small magnets. The magnets are mined out of the ground.“ Cooper says although the tool is a “joke on the surface,” systems like this could have serious applications in teaching and study. AI summarizers could be used by students as a way into complex papers, or they could be incorporated into online journals, automatically producing simplified abstracts for public consumption. “Of course,” says Cooper, this should be only done “if framed properly and with discussion of limitations and what it means (both practically and ethically) to use machine learning as a writing tool.” AI language tools have been incorporated into software from Microsoft and Google These limitations are still being explored by the companies that make these AI systems, even as the software is incorporated into ever-more mainstream tools. tl;dr papers itself was run on GPT-3, which is one of the best-known AI writing tools and is made by OpenAI, a combined research lab and commercial startup that works closely with Microsoft. Microsoft has used GPT-3 and its ilk to build tools like autocomplete software for coders and recently began offering businesses access to the system as part of its cloud suite. The company says GPT-3 can be used to analyze the sentiment of text, generate ideas for businesses, and — yes — condense documents like the transcripts of meetings or email exchanges. And already, tools similar to GPT-3 are being used in popular services like Google’s Gmail and Docs, which offer AI-powered autocomplete features to users. But the deployment of these AI-language systems is controversial. Time and time again, it’s been shown that these tools encode and amplify harmful language based on their training data (which is usually just vast volumes of text scraped off the internet). They repeat racist and sexist stereotypes and slurs and may be biased in more subtle ways, too. A different set of worries stems from the inaccuracy of these systems. These tools only manipulate language on a statistical level: they have no human-equivalent understanding of what they’re “reading,” and this can lead to some very basic mistakes. In one notorious example that surfaced last year, Google search — which uses AI to summarize search topics — provided misleading medical advice to a query asking what to do if someone suffers a seizure. While last December, Amazon’s Alexa responded to a child asking for a fun challenge to do by telling them to touch a penny to the exposed prongs of a plug socket. The specific danger to life posed by these scenarios is unusual, but they offer vivid illustrations of the structural weaknesses of these models. Jathan Sadowski, a senior research fellow in the Emerging Technologies Research Lab at Monash University, was another academic entertained by tl;dr papers’ summary of his research. He says AI systems like this should be handled with care, but they can serve a purpose in the right context. “Maybe one day [this technology will] be so sophisticated that it can be this automated research assistant who is going and providing you a perfect, accurate, high quality annotated bibliography of academic literature while you sleep. But we are extremely far from that point right now,” Sadowski told The Verge. “The real, immediate usefulness from the tool is — first and foremost — as a novelty and joke. But more practically, I could see it as a creativity catalyst. Something that provides you this alien perspective on your work.” “I could see it as a creativity catalyst. Something that provides you this alien perspective on your work.” Sadowski says the summaries provided by tl;dr papers often have a sort of “accidental wisdom” to them — a byproduct, perhaps, of machine learning’s inability to fully understand language. In other scenarios, artists have used these AI tools to write books and music, and Sadowski says a machine’s perspective could be useful for academics who’ve burrowed too deep in their subject. “It can give you artificial distance from a thing you’ve spent a lot of time really close to, that way you can maybe see it in a different light,” he says. In this way, AI systems like tl;dr papers might even find a place similar to tools designed to promote creativity. Take, for example, “Oblique Strategies,” a deck of cards created by Brian Eno and Peter Schmidt. It offers pithy advice to struggling artists like “ask your body” or “try faking it!” Are these words of wisdom imbued with deep intelligence? Maybe, maybe not. But their primary role is to provoke the reader into new patterns of thinking. AI could offer similar services, and indeed, some companies already sell AI creative writing assistants. Unfortunately, although tl;dr papers has had a rapturous reception among the academic world, its time in the spotlight looks limited. After going viral this weekend, the website has been labeled “under maintenance,” and the site’s creators say they have no plans to maintain it in the future. (They also mention that other tools have been built that perform the same task.) Dani told The Verge that tl;dr papers “was designed to be an experiment to see if we can make learning about science a little easier, more fun, and engaging.” He says: “I appreciate all of the attention the app has received and thank all of the people who have tried it out [but] given this was always intended to be an educational project, I plan to sunset tl;dr papers in the coming days to focus on exploring new things.”

---

## Timeframe: 01/2022
### URL: https://www.theverge.com/2022/1/28/22906513/waymo-lawsuit-california-dmv-crash-data-foia
### Relevance Score: 0.10773768275976181
### Highlights: ['Waymo is seeking to keep private information about how it handles certain autonomous vehicle emergencies  Waymo is seeking to keep private information about how it handles certain autonomous vehicle emergencies, how it responds when its vehicles attempt to drive somewhere they are not intended to go, and how they handle steep hills or tight curves. Waymo is currently testing some of its vehicles in downtown San Francisco, where it is permitted to operate fully driverless cars without safety drivers. The lawsuit, which was filed in Sacramento County Superior Court last week, argues that releasing this information to the public would put Waymo at a competitive disadvantage.', 'The suit stems from a public records request to the DMV from an unidentified party  The suit stems from a public records request to the DMV from an unidentified party seeking Waymo’s application for a permit to operate driverless cars on public roads. Before complying with the request, the DMV allowed Waymo to redact certain details. The individual seeking the information challenged the redactions, and the DMV advised Waymo to seek an injunction through a lawsuit if it wanted to block that challenge.']

Waymo filed a lawsuit against the California Department of Motor Vehicles to keep driverless car crash data from being made public. The autonomous vehicle operator, which is owned by Google’s parent company Alphabet, claims that such data should be considered a trade secret. The news of the lawsuit was first reported by Business Insider and later by the Los Angeles Times . California’s DMV oversees the largest autonomous vehicle testing program in the country, with over 60 companies permitted to operate test vehicles on public roads. Only a handful are approved to operate fully autonomous vehicles without safety drivers at the wheel, and even fewer have been approved to deploy vehicles for commercial purposes. Waymo is seeking to keep private information about how it handles certain autonomous vehicle emergencies Waymo is seeking to keep private information about how it handles certain autonomous vehicle emergencies, how it responds when its vehicles attempt to drive somewhere they are not intended to go, and how they handle steep hills or tight curves. Waymo is currently testing some of its vehicles in downtown San Francisco, where it is permitted to operate fully driverless cars without safety drivers. The lawsuit, which was filed in Sacramento County Superior Court last week, argues that releasing this information to the public would put Waymo at a competitive disadvantage. Making public the process by which Waymo analyzes crashes “could provide strategic insight to Waymo’s competitors and third parties regarding Waymo’s assessment of those collisions from a variety of different perspectives, including potential technological remediation,” the company argues. Moreover, it could have a “chilling effect” on the entire autonomous vehicle industry. “Potential market participants interested in deploying autonomous vehicles in California will be dissuaded from investing valuable time and resources developing this technology if there is a demonstrated track record of their trade secrets being released,” Waymo claims. The suit stems from a public records request to the DMV from an unidentified party The suit stems from a public records request to the DMV from an unidentified party seeking Waymo’s application for a permit to operate driverless cars on public roads. Before complying with the request, the DMV allowed Waymo to redact certain details. The individual seeking the information challenged the redactions, and the DMV advised Waymo to seek an injunction through a lawsuit if it wanted to block that challenge. “Every autonomous vehicle company has an obligation to demonstrate the safety of its technology, which is why we’ve transparently and consistently shared data on our safety readiness with the public,” Nicholas Smith, a spokesperson for Waymo, said in a statement. “We will continue to work with the DMV to determine what is appropriate for us to share publicly and hope to find a resolution soon.” A spokesperson for the DMV declined to comment on “active litigation.” Every year, companies that operate autonomous vehicles in California are required to submit data to the DMV listing the number of miles driven and the frequency at which human safety drivers were forced to take control of their autonomous vehicles (also known as a “disengagement”). The companies have been mostly critical of this process, describing the reports as a misleading and ultimately useless way to track AV testing progress in the state. AV companies can be a black box of information, with most firms keeping a tight lid on important metrics and only demonstrating their technology under the most controlled settings. Waymo has been more willing to share data than most AV companies, but largely on its own terms. In 2020, the company published 6.1 million miles of driving data from 2019 and 2020 from its test fleet in Arizona, including 18 crashes and 29 near-miss collisions. More recently, Waymo simulated dozens of real-world fatal crashes that took place in Arizona over nearly a decade to demonstrate how its vehicles could have helped prevent them.

---

## Timeframe: 01/2022
### URL: https://www.wired.com/story/artificial-intelligence-data-future-optimization-antifragility/
### Relevance Score: 0.11012109369039536
### Highlights: ['And 99.9 percent of the time, it works: It correctly concludes that the word suit is part of a judge’s email to counsel. The other 0.1 percent of the time, the AI snaps. It misidentifies a diving suit as a lawyerly conversation, tightening the loop to exclude the actual truth and plunging into an ocean that it thinks is a courtroom.', 'This is “closing the circle.” It leverages big data to tighten a circumference of possibilities to a single dot. And 99.9 percent of the time, it works: It correctly concludes that the word suit is part of a judge’s email to counsel. The other 0.1 percent of the time, the AI snaps.']

Everywhere, AI is breaking. And everywhere, it’s breaking us. The breaking ensues whenever AI encounters ambiguity or volatility. And in our hazy, unstable world, that’s all the time: Either the data can be interpreted another way or it’s obsolesced by new events. At which point, AI finds itself looking at life through errant eyes, seeing left as right or now as yesterday. Yet because AI lacks self-awareness, it doesn’t realize that its worldview has cracked. So, on it whirs, unwittingly transmitting the fracture to all the things plugged into it. Cars are crashed. Insults are hurled. Allies are auto-targeted. This breaks humans in the direct sense of harming, even killing, us. But it has also begun breaking us in a more subtle way. AI can malfunction at the mildest hint of data slip, so its architects are doing all they can to dampen ambiguity and volatility. And since the world’s primary source of ambiguity and volatility is humans, we have found ourselves aggressively stifled. We’ve been forced into metric assessments at school, standard flow patterns at work, and regularized sets at hospitals, gyms, and social-media hangouts. In the process, we’ve lost large chunks of the independence, creativity, and daring that our biology evolved to keep us resilient, making us more anxious, angry, and burned out. If we want a better future, we need to pursue a different remedy to AI’s mental fragility. Instead of remaking ourselves in AI’s brittle image, we should do the opposite. We should remake AI in the image of our antifragility. Durability is simply resisting damage and chaos; antifragility is getting stronger from damage and smarter from chaos. This can seem more magic than mechanical, but it’s an innate capacity of many biological systems, including human psychology. When we’re kicked in the face, we can bounce back tougher with courage. When our plans collapse, we can rally for the win with creativity. Building these antifragile powers into AI would be revolutionary. (Disclosure: Angus Fletcher is currently advising AI projects, which include antifragile AI, within the US Department of Defense). We can achieve the revolution, if we upend our current way of thinking. Rethinking AI First, we must banish the futurist delusion that AI is the smarter version of ourselves. AI’s method of cogitation is mechanically distinct from human intelligence: Computers lack emotion, so they can’t literally be courageous, and their logic boards can’t process narrative, rendering them incapable of adaptive strategy. Which means that AI antifragility won’t ever be human, let alone superhuman; it will be a complementary tool with its own strengths and weaknesses. We then must step toward heresy by acknowledging that the root source of AI’s current fragility is the very thing that AI design now venerates as its high ideal: optimization. Optimization is the push to make AI as accurate as possible. In the abstract world of logic, this push is unambiguously good. Yet in the real world where AI operates, every benefit comes at a cost. In the case of optimization, the cost is data. More data is required to improve the precision of machine-learning’s statistical computations, and better data is necessary to ensure that the computations are true. To optimize AI’s performance, its handlers must intel-gather at scale, hoovering up cookies from apps and online spaces, spying on us when we’re too oblivious or exhausted to resist, and paying top dollar for inside information and backroom spreadsheets. This incessant surveillance is antidemocratic, and it’s also a loser’s game. The price of accurate intel increases asymptotically; there’s no way to know everything about natural systems, forcing guesses and assumptions; and just when a complete picture is starting to coalesce, some new player intrudes and changes the situational dynamic. Then the AI breaks. The near-perfect intelligence veers into psychosis, labeling dogs as pineapples, treating innocents as wanted fugitives, and barreling eighteen-wheelers into kindergarten busses that it sees as highway overpasses. The dangerous fragility inherent to optimization is why the human brain did not, itself, evolve to be an optimizer. The human brain is data-light: It draws hypotheses from a few data points. And it never strives for 100 percent accuracy. It’s content to muck along at the threshold of functionality. If it can survive by being right 1 percent of the time, that’s all the accuracy it needs. The brain’s strategy of minimal viability is a notorious source of cognitive biases that can have damaging consequences: close-mindedness, conclusion jumping, recklessness, fatalism, panic. Which is why AI’s rigorously data-driven method can help illuminate our blindspots and debunk our prejudices. But in counterbalancing our brain’s computational shortcomings, we don’t want to stray into the greater problem of overcorrection. There can be enormous practical upside to a good enough mentality: It wards off perfectionism’s destructive mental effects, including stress, worry, intolerance, envy, dissatisfaction, exhaustion, and self-judgment. A less-neurotic brain has helped our species thrive in life’s punch and wobble, which demands workable plans that can be flexed, via feedback, on the fly. These antifragile neural benefits can all be translated into AI. Instead of pursuing faster machine-learners that crunch ever-vaster piles of data, we can focus on making AI more tolerant of bad information, user variance, and environmental turmoil. That AI would exchange near-perfection for consistent adequacy, upping reliability and operational range while sacrificing nothing essential. It would suck less energy, haywire less randomly, and place less psychological burdens on its mortal users. It would, in short, possess more of the earthly virtue known as common sense. Here’s three specs for how. Building AI to Brave Ambiguity Five hundred years ago, Niccolò Machiavelli, the guru of practicality, pointed out that worldly success requires a counterintuitive kind of courage: the heart to venture beyond what we know with certainty. Life, after all, is too fickle to permit total knowledge, and the more that we obsess over ideal answers, the more that we hamper ourselves with lost initiative. So, the smarter strategy is to concentrate on intel that can be rapidly acquired—and to advance boldly in the absence of the rest. Much of that absent knowledge will prove unnecessary, anyway; life will bend in a different direction than we anticipate, resolving our ignorance by rendering it irrelevant. We can teach AI to operate this same way by flipping our current approach to ambiguity. Right now, when a Natural Language Processor encounters a word—suit—that could denote multiple things—an article of clothing or a legal action—it devotes itself to analyzing ever greater chunks of correlated information in an effort to pinpoint the word’s exact meaning. This is “closing the circle.” It leverages big data to tighten a circumference of possibilities to a single dot. And 99.9 percent of the time, it works: It correctly concludes that the word suit is part of a judge’s email to counsel. The other 0.1 percent of the time, the AI snaps. It misidentifies a diving suit as a lawyerly conversation, tightening the loop to exclude the actual truth and plunging into an ocean that it thinks is a courtroom. Let the circle stay big. Instead of designing AI to prioritize resolving ambiguous data points, we can program it to perform quick-and-dirty recalls of all possible significations–and to then carry those branching options onto its subsequent tasks, like a human brain that continues reading a poem with multiple potential interpretations held simultaneously in mind. This saves the data intensiveness that traditional machine-learning pours into optimization. In many cases, the ambiguity will get flushed from the system by downstream events: Maybe every executed query resolves identically with either meaning of suit; maybe the system gains access to an email that refers to a lawsuit about a diving suit; maybe the user realizes that (in a typically unpredictable human maneuver) she mistyped suite. Worst case, if the system encounters a situation where it can’t proceed unless the ambiguity is addressed, it can pause to request human assistance, tempering valor with timely discretion. And in any and all causes, the AI won’t break itself, self-destructing (via a digital version of anxiety) into making unnecessary errors because it’s so stressed about being perfect. Marshal Data in Support of Creativity The next big contributor to antifragility is creativity. Current AI aspires to be creative via a big-data leverage of divergent thinking, a method conceived 70 years ago by Air Force Colonel J.P. Guilford. Guilford succeeded, insofar as he managed to reduce some creativity to computational routines. But because most biological creativity, as subsequent scientific research has shown, involves data-free and nonlogical processes, divergent thinking is far more conservative in its outcomes than human imagination. Although it can spam out gigantic quantities of “new” works, those works are confined to mix-and-matches of earlier models, so what divergent thinking gains in scale it sacrifices in scope. The practical limitations of this information-powered, robo-formula for imagineering can be witnessed in text and image generators such as GPT-3 and ArtBreeder. By using historical sets to brainstorm, these AI lard their concoctions with expert bias, so that while striving to produce the next van Gogh, they instead emit pastiches of every painter before. The knock-on result of such pseudo-invention is a culture of AI design that categorically misunderstands what innovation is: FaceNet’s “deep convolutional network” is hailed as a breakthrough over previous facial recognition software when it’s more of the same brute-force optimization, like tweaking a car’s torque band to add horsepower—and calling it a revolution in transportation. The antifragile alternative is to flip from using data as a source of inspiration to using it as a source of falsification. Falsification is the brainchild of Karl Popper, who, ninety years ago, in his Logic of Scientific Discovery, pointed out that it’s more logical to mobilize facts to knock out ideas than to confirm them. When translated onto AI, this Popperian reframe can invert data’s function from a mass-generator of trivially novel ideas into a mass-destroyer of anything except wildly unprecedented ones. Rather than smudging together billions of existing priors into an endless déjà vu of the mildly new, tomorrow’s antifragile computers could trawl the world’s ever-growing flood of human creations to identify today’s unappreciated van Goghs. Imagine a Pulitzer AI that inputs the winning photographs selected by the panel of human judges—then awards its prize to the news photo that most defies the panel’s expectations. And in the future, AI could be trained to do the same with its own creations. In place of the high-data ideation method of GPT-3’s ilk, it could harness low-data methods that fling out mostly incoherence but, a tiny fraction of the time, hit upon a genuine original. With falsification, the future-AI could detect that fraction, plucking The Starry Night from a galaxy of nonsense. The Importance of AI-Human Hybridity In our here and now, the world’s most antifragile intelligence is human psychology. So, why not give AI the full benefits of our brains? Why not merge ourselves with it? Such hybridity, sci-fi as it sounds, doesn’t require us to go full Elon Musk. We can achieve it simply by engineering better AI-human partnerships. Those partnerships are currently less than the sum of their parts, existing as bad-faith relations in which humans are treated either as glorified babysitters who micromanage AI for poor decisions—or as subalterns who must blindly acquiesce to AI’s inscrutable auto-updates. The former tips the human brain into a numbingly boring, right/wrong mode of cognition that kills the neural root of creativity. And the latter destroys our independence and renders us passive to a secretive, bean-counting apparatus that reduxes the USSR’s Central Statistics Administration. We can troubleshoot this dystopian union by regearing the collaboration between AI and its human users, starting with three instant fixes. First, equip AI to identify when it lacks the data required for its computations. Rather than designing AI that strives to be right all the time, design AI that identifies when it cannot be right. To do this is to give AI the deep wisdom of Know Thyself , not by making AI literally self-aware, but by providing it with an insentient mechanism for detecting its own limit of competency. That limit can’t be identified in real-time by AI’s human users. Our brain is incapable of processing data at a computer’s voluminous speed, dooming us to always intervene too late when a clueless algorithm thinks it’s omniscient. But by programming the fool to spot itself, we can train AI to hand over control before it races into mayhem, creating a path for it to earn authentic trust from human users. Second, improve the human-AI interface. The push for optimization has created design features that are either opaque (riddled with “black box” algorithms that no computer scientist can fathom) or infantilizing (pre-scripted UX menus that glibly usher cubicle employees down rote decision trees). These features should all be walked back. Black box algorithms should be eliminated entirely; if we don’t know what a computer is doing, it doesn’t either. And rigid button bars that transfer AI’s brittle precision onto users should be replaced with open-ended “big circle” lists where the first option is 70 percent likely, the second is 20 percent likely, the third is 5 percent likely, and so on. If the user doesn’t see a good choice on the list, they can redirect the AI or take manual control, maximizing the operational ranges of both computer logic and human initiative. Third, decentralize AI by modeling it after the human brain. Just as our brain contains discrete cognitive mechanisms—logic, narrative, emotion—that (like in a Constitutional separation of powers) check-and-balance one another, so can a single AI be designed to combine different inference architectures (for example, neural networks and symbolic GOFAI). This makes the AI less fragile by allowing it to exit dead-end protocols. If a deep-learning backpropagation can’t access the data it needs, the system can transition to if-then procedures. And by enabling AI to view life through multiple epistemologies, decentralization also invests AI-human partnerships with greater antifragility: Rather than concentrating monomaniacally on its own internal optimization strategies, AI can look outward to learn from anthropological cues. If a self-driving algorithm triggers a baffled frown (or some other sign of confusion) in a human user, the AI can flag the algorithm as potentially suspect, so that instead of forcing us to one-way adapt to its performance quirks, it adapts to our psychology too. These blueprints aren’t Neurolinks, Artificial General Intelligence, or some other quixotic technology. They’re design innovations that we can implement, now. All they require is the courage to leave behind big data and its false promise of perfect intelligence. All they require is accepting that, in our uncertain and ever-changing world, it’s smarter to be creatively adequate than optimally accurate. Because it’s better to bounce back than break. More Great WIRED Stories 📩 The latest on tech, science, and more: Get our newsletters! Welcome to Miami, where all your memes come true! Bitcoin's libertarian streak meets an autocratic regime How to start (and keep) a healthy habit Natural history, not technology, will dictate our destiny Scientists settled family drama using DNA from postcards 👁️ Explore AI like never before with our new database 💻 Upgrade your work game with our Gear team’s favorite laptops, keyboards, typing alternatives, and noise-canceling headphones

---

## Timeframe: 01/2022
### URL: https://www.wired.com/story/ai-software-nearly-predicted-omicrons-tricky-structure/
### Relevance Score: 0.10997366905212402
### Highlights: ['Both are based on machine learning algorithms honed to predict protein structures by training on a collection of more than 100,000 known structures. The next month, DeepMind published details of its own work and released AlphaFold for anyone to use. Suddenly, the world had two ways to predict protein structures.', 'They spit out a single static structure for a protein, and don’t capture the flexes and wiggles that take place when it interacts with other molecules. The algorithms were trained on databases of known structures, which are more reflective of those easiest to map experimentally rather than the full diversity of nature. Kresten Lindorff-Larsen, a professor at the University of Copenhagen, predicts the algorithms will be used more frequently and will be useful, but says, “We also as a field need to learn better when these methods fail.” ']

The way predictions raced ahead of experiments on Omicron’s spike protein reflects a recent sea change in molecular biology brought about by AI. The first software capable of accurately predicting protein structures became widely available only months before Omicron appeared, thanks to competing research teams at Alphabet’s UK-based AI lab DeepMind and at the University of Washington. Ford used both packages, but because neither was designed or validated for predicting small changes caused by mutations like those of Omicron, his results were more suggestive than definitive. Some researchers treated them with suspicion. But the fact that he could easily experiment with powerful protein prediction AI illustrates how the recent breakthroughs are already changing the ways biologists work and think. Subramaniam says he received four or five emails from people proffering predicted Omicron spike structures while working towards his lab’s results. “Quite a few did this just for fun,” he says. Direct measurements of protein structure will remain the ultimate yardstick, Subramaniam says, but he expects AI predictions to become increasingly central to research—including on future disease outbreaks. “It’s transformative,” he says. Because a protein’s shape determines how it behaves, knowing its structure can help all kinds of biology research, from studies of evolution to work on disease. In drug research, figuring out a protein structure can help reveal potential targets for new treatments. Determining a protein’s structure is far from simple. They are complex molecules assembled from instructions encoded in an organism’s genome to serve as enzymes, antibodies, and much of the other machinery of life. Proteins are made from strings of molecules called amino acids that can fold into complex shapes that behave in different ways. Deciphering a protein’s structure traditionally involved painstaking lab work. Most of the roughly 200,000 known structures were mapped using a tricky process in which proteins are formed into a crystal and bombarded with x-rays. Newer techniques like the electron microscopy used by Subramaniam can be faster, but the process is still far from easy. In late 2020, the long-standing hope that computers could predict protein structure from an amino acid sequence suddenly became real, after decades of slow progress. DeepMind software called AlphaFold proved so accurate in a contest for protein prediction that the challenge’s cofounder John Moult, a professor at University of Maryland, declared the problem solved. “Having worked personally on this problem for so long,” Moult said, DeepMind’s achievement was “a very special moment.” The moment was also frustrating for some scientists: DeepMind did not immediately release details of how AlphaFold worked. “You’re in this weird situation where there’s been this major advance in your field, but you can’t build on it,” David Baker, whose lab at University of Washington works on protein structure prediction, told WIRED last year. His research group used clues dropped by DeepMind to guide the design of open source software called RoseTTAFold, released in June, which was similar to but not as powerful as AlphaFold. Both are based on machine learning algorithms honed to predict protein structures by training on a collection of more than 100,000 known structures. The next month, DeepMind published details of its own work and released AlphaFold for anyone to use. Suddenly, the world had two ways to predict protein structures. Minkyung Baek, a postdoctoral researcher in Baker’s lab who led work on RoseTTAFold, says she has been surprised by how quickly protein structure predictions have become standard in biology research. Google Scholar reports that UW's and DeepMind’s papers on their software have together been cited by more than 1,200 academic articles in the short time since they appeared. Although predictions haven’t proven crucial to work on Covid-19, she believes they will become increasingly important to the response to future diseases. Pandemic-quashing answers won’t spring fully formed from algorithms, but predicted structures can help scientists strategize. “A predicted structure can help you put your experimental effort into the most important problems,” Baek says. She’s now trying to get RoseTTAFold to accurately predict the structure of antibodies and invading proteins when bound together, which would make the software more useful to infectious disease projects. Despite their impressive performance, protein predictors don’t reveal everything about a molecule. They spit out a single static structure for a protein, and don’t capture the flexes and wiggles that take place when it interacts with other molecules. The algorithms were trained on databases of known structures, which are more reflective of those easiest to map experimentally rather than the full diversity of nature. Kresten Lindorff-Larsen, a professor at the University of Copenhagen, predicts the algorithms will be used more frequently and will be useful, but says, “We also as a field need to learn better when these methods fail.”

---

## Timeframe: 01/2022
### URL: https://www.wired.com/story/photography-artificial-intelligence-technology/
### Relevance Score: 0.0975116640329361
### Highlights: ['Is it when it is physically printed? Is it when the image is 2D? Is it when it is not interactive?', 'Is it when the image is 2D? Is it when it is not interactive? Is it the object or the information?']

As a child, I would sit on the balcony of our Dhaka apartment overlooking the pond and flip through our two family photo albums. After the Bangladesh liberation war in 1971, film was scarce and our camera had broken. With nowhere to get it repaired or to buy film, we had no more family photos for almost a decade. There are no photos of me until I was 8 years old. The tiny, gemlike black-and-white prints of my parents and older brother were fragments of my history that, as curator Glen Helfand said, “captured a fraction of a second of activity and fueled narratives for generations.” These images were absorbed by my soul, stored as evidence of the stories of my family from before my birth, and are now on my kids’ iPhones. Photograph of my mother from our family album.Photograph: Rashed Haq On that pond-side balcony, it was apparent to me what photographs were. Later, I would be taught the technical language for those images: two-dimensional registration of light on cellulose negative, then printed on silver halide paper. However, 25 years later, sitting in my studio surrounded by thermal cameras, lidar, 3D printers, and AI software, I am not so sure anymore. Much of photo criticism and theory today still actively debates the past, with very little consideration of what is coming up. For example, the American artist Trevor Paglen’s 2017 exhibition “A Study of Invisible Images” surveyed “machine vision”—images made by machines for other machines to consume, such as facial recognition systems. Jerry Saltz, senior art critic for New York magazine, declared the work to be “conceptual zombie formalism” based on “smarty-pants jargon,” rather than engaging seriously with the implications of his work. When it comes to theory, a large portion of Photography Theory, a 451-page book often used to teach, focuses on debating indexicality, the idea that taking a photograph leaves a physical trace of the object that was photographed. This was questionable in analog photography but is absent entirely with digital photography, unless information is to be considered a trace. Again, the book says nothing about new or emerging technologies and how it affects photography. Evolving technologies affect every step of the photo production process, and photographers are using these technologies to question the definition of photography itself. Is something a photograph when it is capturing only light? Is it when it is physically printed? Is it when the image is 2D? Is it when it is not interactive? Is it the object or the information? Or is it something else? Going Digital Photography—from the Greek words photos and graphos, meaning “drawing with light”—started in the 19th century as the capture of light bouncing off objects onto a chemically coated medium, such as paper or a polished plate. This evolved with the use of negatives, allowing one to make multiple prints. The production steps of capturing, processing, and printing involved starting and stopping chemical reactions on the print paper and negatives. With analog photography, the chemistry directly captures the physical reality in front of the camera. However, with digital photography, image-making consists of counting the number of photon light particles that hit each sensor pixel, using a computer to process the information, and, in the case of color sensors, doing further computations to determine the color. Only digitized bits of information are captured—there is no surface on which a physical trace is left. Because data is much easier to process and manipulate than chemicals, digital photography allows greater diversity and versatility of image manipulation possibilities. Film theorist Mary Ann Doane has said that the digital represents “the vision (or nightmare) of a medium without materiality, of pure abstraction incarnated as a series of 0s and 1s, sheer presence and absence, the code. Even light, that most diaphanous of materialities, is transformed into numerical form in the digital camera.” Evolving Image Capture Analog photography captured “actinic light,” a narrow sliver of the electromagnetic spectrum visible to the naked eye and able to cause photochemical reactions. Over time, photographers have expanded this to beyond the optical range to create images from infrared, x-ray, and other parts of the spectrum, such as thermography. Irish photographer Richard Mosse uses a camera that captures contours in heat rather than light. Traditionally used in military surveillance, this camera allows him to photograph what we cannot see—it can detect humans at night or through tents, up to 18 miles away. In 2015, Mosse produced a body of work on the refugee crisis called “Heat Maps,” capturing what art critic Sean O’Hagan called the “white-hot misery of the migrant crisis,” showing monochrome images with shimmering landscapes and ghostly human figures. Unlike with light, the thermal signals cannot distinguish facial features, therefore converting human figures into faceless statistics, representing how immigrants are often treated. Any form of information can be captured for imaging. Artists have worked with other inputs such as acoustic signals, matter particles such as electrons, and other forms of waves. The American artist Robert Dash uses an electron microscope, which utilizes matter waves rather than light waves, to create very high magnification images of natural objects, such as pollen or seeds found on the property where he lives. He then photo-montages these with life-sized photos of the same objects, creating a surreal, microscopic world. The first time I saw these photographs, my eyes were scanning for any signs in the landscape that could help locate where the images may have been taken, but without success. Evolving Image Processing Image processing, traditionally done during the printing process, is any kind of manipulation to create the final image, from darkening the sky in a landscape photograph to using an Instagram filter or editing in Adobe Photoshop. The recent documentary Black Holes | The Edge of All We Know shows an advanced version of digital image processing. The documentary explores the process of creating the first photo of a black hole, which took 250 people about a decade to make. Researchers constructed the image by computationally combining the radio-frequency data collected over many years, using a novel mathematical model, from multiple observatories worldwide. The image shows a donut of light around the supermassive black hole at the center of the galaxy M87. It continues the photographic tradition of expanding beyond human perception, revealing previously invisible dimensions of reality and encoding it into visible knowledge, as Eadweard Muybridge did 150 years ago with his pioneering work using photography to study motion. With the development of artificial intelligence, the image processing step can be taken further. For example, Paglen generates portraits of people by creating facial recognition models of his collaborators, and then using a second model that generates random images using polygons to fool the first model into thinking it is a portrait. Then, as Paglen explains, “these two programs go back and forth until an image ‘evolves’ that the facial recognition model identifies as a representation of that particular person.” This process creates a haunting portrait of what the machine sees.

---

## Timeframe: 01/2022
### URL: https://www.breitbart.com/tech/2022/01/06/human-forecasters-predict-bad-weather-more-accurately-than-ai/
### Relevance Score: 0.0606725737452507
### Highlights: ['Through years of previous experience, human meteorologists continue to beat machine learning algorithms. Humans are also significantly better at predicting dangerous weather events such as serious storms including the tornados that tore through the Midwest in early December killing more than 60 people. Forecasters can often spot these tornadoes by looking for their signatures on radar.', 'These are variations that weather prediction algorithms don’t take into account. Using this information, Devanas and his team were able to see if there was a correlation between certain actors and a higher risk of waterspouts. These observations were compared to a modeled probability index that indicates whether waterspouts are likely and found that with the right combination of atmosphere measurements, the human forecast successfully “outperformed” the algorithmic model in every metric for predicting waterspouts.']

According to a recent report by Wired, human meteorologists still outperform AI algorithms when predicting weather events – particularly those that could be dangerous. Wired reports that despite advances in AI throughout the years, human meteorologists still outperform machine learning algorithms when it comes to predicting the weather. The first computerized automatic forecast was developed in the 1950s by a group of mathematicians, meteorologists, and computer scientists led by renowned mathematician John von Neumann and atmospheric physicist Jule Charney. Charney and the team developed a basic algorithm that could generate 24-hour atmospheric forecasts. The results were not perfect but were encouraging enough to move the field towards computer-based modeling. lightning (Getty) Since then, billions of dollars have been invested in improving the predictive capability of weather forecast systems. In 2016 and 2018, the GOES-16 and -17 satellites were launched into orbit which allowed meteorologists to take high-resolution images and pinpoint lightning detection. The most popular weather prediction models are the US-based Global Forecasting System (GFS) and European Center for Medium-Range Weather Forecasts (ECMWF) and both have received major upgrades this year. But despite these expensive algorithms, earth-orbiting satellites, and supercomputers, human forecasters still have an advantage over computers. Through years of previous experience, human meteorologists continue to beat machine learning algorithms. Humans are also significantly better at predicting dangerous weather events such as serious storms including the tornados that tore through the Midwest in early December killing more than 60 people. Forecasters can often spot these tornadoes by looking for their signatures on radar. Andrew Devanas, an operational forecaster at the National Weather Service office in Key West, Florida, lives near one of the most active regions for waterspouts which are marine-based tornadoes that often damage ships passing through the Florida Straits. Devanas and his colleague manually studied variations in the atmosphere in Florida like wind speed and available moisture. These are variations that weather prediction algorithms don’t take into account. Using this information, Devanas and his team were able to see if there was a correlation between certain actors and a higher risk of waterspouts. These observations were compared to a modeled probability index that indicates whether waterspouts are likely and found that with the right combination of atmosphere measurements, the human forecast successfully “outperformed” the algorithmic model in every metric for predicting waterspouts. Read more at Wired here. Lucas Nolan is a reporter for Breitbart News covering issues of free speech and online censorship. Follow him on Twitter @LucasNolan or contact via secure email at the address lucasnolan@protonmail.com

---

## Timeframe: 01/2022
### URL: https://www.breitbart.com/entertainment/2022/01/28/ron-perlman-says-fk-you-to-critics-who-trashed-netflixs-climate-satire-dont-look-up/
### Relevance Score: 0.048674579709768295
### Highlights: ['And it’s twisted.” He said that he “understands that it’s part of how the internet has almost killed journalism. And now journalism is trying to do everything they can to co-opt and maintain their importance.”', 'He said that he “understands that it’s part of how the internet has almost killed journalism. And now journalism is trying to do everything they can to co-opt and maintain their importance.”       Ron Perlman in ‘Don’t Look Up.’ (Niko Tavernise/Netflix)  ']

Left-wing Hollywood star Ron Perlman has thrown a temper tantrum at movie critics who dared to give negative reviews to Netflix’s climate change satire Don’t Look Up , in which he co-stars alongside Leonardo DiCaprio, Jennifer Lawrence, and Meryl Streep. The actor also attacked former President Donald Trump and Fox News in a profanity-laced interview with Britains’s The Independent. On the subject of Don’t Look Up, Perlman didn’t mince words about critics who trashed the movie. “Fuck you and your self-importance and this self-perpetuating need to say everything bad about something just so that you can get some attention for something that you had no idea about creating,” said the actor. “It’s corrupt. And it’s sick. And it’s twisted.” He said that he “understands that it’s part of how the internet has almost killed journalism. And now journalism is trying to do everything they can to co-opt and maintain their importance.” Ron Perlman in ‘Don’t Look Up.’ (Niko Tavernise/Netflix) In the movie, Ron Perlman plays a grizzled military colonel tasked with leading the country’s armed response to an asteroid that is on a collision course with Earth. Watch below: Don’t Look Up received negative reviews from many mainstream critics who faulted the movie’s smug, self-satisified tone and facile approach to satire. Overall, the movie received a 55 percent “rotten” score on RottenTomatoes, suggesting a predominantly negative reaction to the movie among professional reviewers. rottentomatoes.com Later in the interview, Perlman delivered his opinion on Fox News hosts. “I really don’t give a fuck [about them]. I’ve given up on those people,” he told The Independent. “They’re all vaccinated and telling you not to be. They know everything they say is a lie but they’re doing it anyway. They’re all fucking pieces of shit that can go fuck themselves.” On the subject of Trump, the left-wing actor once again expressed his unconditional loathing of the 45th president. “The heartbreaking thing is 74 million people voted for a man who has been impeached twice, groped 26 women, inflated his personal wealth and then deflated it when he needed to. I hope there’s a special place in hell for people who have exploited others’ vulnerability,” he said. Follow David Ng on Twitter @HeyItsDavidNg. Have a tip? Contact me at dng@breitbart.com

---

## Timeframe: 01/2022
### URL: https://www.breitbart.com/tech/2022/01/18/facebook-patents-reveal-mark-zuckerbergs-plan-to-turn-the-metaverse-into-a-money-machine/
### Relevance Score: 0.04855448007583618
### Highlights: ['These can be used to target ads towards a person, placing ads where their eyes are most commonly drawn to, but could also be used for features like full-body tracking allowing them to move every limb of their digital avatar in the metaverse. One patent describes a system for tracking facial expressions through a headset and then serving the users “adapt media content” based on the expressions. This report come shortly after Breitbart News reported that\xa0Facebook’s VR division is under investigation by the FTC and multiple U.S. states led by New York.', 'Clearly, ads play a part in that.” Other patents focus on technology, including eye and face tracking tech that will be collected in a headset using tiny cameras or sensors. These can be used to target ads towards a person, placing ads where their eyes are most commonly drawn to, but could also be used for features like full-body tracking allowing them to move every limb of their digital avatar in the metaverse.']

Recent patents filed by Facebook (now Meta) reveal how the company plans to generate profit from its metaverse virtual reality platform. According to the patents, Facebook plans to track everything from eye movements to nose twitches as its users explore the new platform. The Financial Times reports that a number of patents filed by Facebook, including tech to track eye movements, body positions, nose twitching, and facial expressions, reveal how the company plans to monetize its metaverse virtual landscape. Mark Zuckerberg introduces Meta (Facebook) Facebook CEO Mark Zuckerberg previously announced the company’s plans to invest $10 billion a year over the next ten years to develop the metaverse, an immersive virtual world of digital avatars powered largely by Facebook’s Oculus VR headsets. Other tech giants like Apple and Microsoft have reportedly taken notice and are developing similar projects as many believe that the virtual world could be the next step in social media. The Financial Times noted that hundreds of applications to the U.S. Patent and Trademark Office provide an insight into how Facebook plans to monetize the digital world. The firm reportedly plans to use hyper-targeted advertising and sponsored content within the virtual world to drive revenue, following a similar model that helped it build an $85 billion per year advertising platform. Users will reportedly also be able to purchase items in a “virtual store” in the metaverse which will correspond with real-world goods sponsored by brands. Facebook’s head of global affairs Nick Clegg recently stated: “For us, the business model in the metaverse is commerce-led. Clearly, ads play a part in that.” Other patents focus on technology, including eye and face tracking tech that will be collected in a headset using tiny cameras or sensors. These can be used to target ads towards a person, placing ads where their eyes are most commonly drawn to, but could also be used for features like full-body tracking allowing them to move every limb of their digital avatar in the metaverse. One patent describes a system for tracking facial expressions through a headset and then serving the users “adapt media content” based on the expressions. This report come shortly after Breitbart News reported that Facebook’s VR division is under investigation by the FTC and multiple U.S. states led by New York. Third-party Oculus app developers have been questioned by the FTC in recent months, according to sources. Read more at the Financial Times here. Lucas Nolan is a reporter for Breitbart News covering issues of free speech and online censorship. Follow him on Twitter @LucasNolan or email him at lnolan@breitbart.com

---
