
## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/31/narrow-ai-cant-do-that-or-can-it/
### Query Relevance Score: 0.11629411578178406
### Highlights: None

More posts by this contributor Editor’s note: David Senior is the CEO and co-founder of Lowdownapp. With industry pundits, including Stephen Hawking, Elon Musk and others, hotly debating the dangers of artificial intelligence and Hollywood priming the public for the release of a slew of new movies — including Terminator 5 — that warn what can happen when software and hardware evolve to the point that they are capable of human feats of intelligence, it’s little wonder that the popular view of AI has become confused and convoluted. The assumption that all AI is about systems designed to autonomously learn new tasks, adapt to changing environments and perhaps, like HAL, outwit their creators in the end, skims over the many important differences between classic AI (the one movies are made of) and its over one dozen subdisciplines. These AI subsets, ranging from the speech recognition and natural language understanding we know from personal assistants like Siri and Cortana, to the machine learning and deep learning capabilities core to business analytics and systems designed to make sense of big data, are — and will remain — where the action and opportunity is. VCs need no convincing. The last few months have seen a flurry of activity and a wave of investment as startups in Silicon Valley and beyond raise substantial funding for AI approaches and innovations that emphasize the business benefits of AI and narrow AI, a technology subset focused on solving specific, reasonably well-defined problems. It’s a stampede as many of Silicon Valley’s leading venture capital firms, including Khosla Ventures and Greylock Partners, as well as financial institutions, such as Goldman Sachs, flock to the space and invest big dollars in companies using narrow AI technologies to tackle tough business tasks like taming big data. A prime example is Seattle-based Context Relevant, a provider of automated predictive analytics software for big data 2.0 applications. It raised $13.5 million in series B-1 funding with participation by Goldman Sachs, Bank of America Merrill Lynch, Formation 8, New York Life and Bloomberg Beta in September 2014. The round of funding came just months after it closed $21 million in a Series B round, bringing total funding for Context Relevant to $42 million. With an estimated 170 startups in the starting gate ready to recast themselves as AI companies, or simply jump on the bandwagon, you can bet this year will see AI (in all its flavors and forms) lead the list of mega-trends. But before you dismiss this as hype, consider that the rise of AI, and specifically weak or narrow AI, is also inextricably linked with the growth of big data. Simply put, it’s the explosive rate of information growth that creates the requirement for narrow AI. Add to that the recent avalanche of user-generated content — the nearly 300,000 tweets, 220,000 Instagram photos, 72 hours of YouTube video content and the 2.5 million pieces of content shared by Facebook users every single minute that businesses must monitor and acknowledge — and it’s clear that no organization (or human) can cope without the aid of narrow AI. But the business and personal benefits of narrow AI go far beyond the ability to trawl through massive amounts of information and automate routine knowledge work. Some narrow AI approaches sift through the data to pull together and expose what is relevant and valuable to the individual user and their “need” state. An early and rather primitive example of this is Apple’s Siri. I give it credit for bringing narrow AI to the mainstream. But I also side with Robert Scoble, who has repeatedly remarked that the fatal flaw in this and other personal assistant services is a lack of context. Successful narrow AI services, to deliver value and benefit, must be aware of the user’s environment and factor this into the equation before delivering answers or advice or simply taking action. A great sandbox for ideas and innovation is the smartphone calendar, the fiercely personal device most people regard as a digital extension of their physical “self.” The calendar and contacts is not only where people live and record their lives; it’s a living microcosm of their social graph device that grows and evolves as people and their networks do. And so it makes sense that this space — where the calendar, contacts and context come together — is where startups are staging a new battle. The stakes are high, which is why competition is also high, as companies conduct a digital battle as fierce as Arnie and as determined as HAL.

---

## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/17/the-robots-are-coming-arent-they/
### Query Relevance Score: 0.11172334104776382
### Highlights: None

Editor’s note: Rob Daley is the CEO of 4moms, a robotics company that makes high-tech baby gear. Rosie, C-3PO, HAL, WALL-E, Bender, Optimus, Asimo. We have envisioned building humanoid machines in our likeness with the ability to walk, talk and think like us — or better than us — since long before the creation of the microchip. In 322 BC, Aristotle postulated that automatons created from garden statues might bring about the end of slavery. Two-year-olds are captivated by these mostly mythical creatures we call robots. From the time we are little kids, when we dream about the future, we dream about robots. It’s as simple as that. But here’s the thing — humanoid robots are never going to happen. To understand why, you have to understand the shape of the technical problems and compare them to market forces. First, the mechanics of the human body are spectacularly complex. Depending on your view of creationism and evolution, the human design either required the hand of God or it was the output of a one in a gazillion experiment. Either way, we are dealing with a mechanical form that isn’t easy to replicate. Sure, we have built machines with arms and legs, but we are a long way from having a unified platform that can walk the Earth with the dexterity and reliability of a human. Second, we need major advancements in input/output software. Anyone who’s gotten into an argument with Siri can well understand how far we are from having a dependable conversation with a machine. It’s amusing, if frustrating, to ask Siri to “Send Jenn a text” and hear her reply “What song would you like me to play?” You can imagine what might happen when your robot misunderstands you when you say “Take the trash to the curb.” Speaker-independent, open-ended speech recognition with better than 99 percent reliability isn’t part of our near future. Third, artificial intelligence is the biggest software challenge in the universe. The human brain’s ability to process information and learn and make decisions is something we can barely understand, let alone reproduce. Replicating human thought in a bounded problem — like the game of chess — is barely within our grasp. Software that enables machines to act like us, which requires them to think like us, isn’t something we’ll be able to develop for generations, if ever. So all we have to do to build a humanoid robot is build a machine to replicate the human form. Then solve the input/output problem so we always understand each other. And finally, replicate the thought process of the human brain. Any one of those three tasks is nearly impossible. The intersection of all three represents one of the greatest technical challenges known to man. However, it’s not just the scale of the technical challenge that makes a humanoid robotic future impossible; it’s the intersection of those three challenges with simple economics. A humanoid robot would be a multipurpose platform that could do almost anything. It could do things like get a beer from the fridge or a newspaper off the front lawn. But those are sub $20 problems — meaning realistically you wouldn’t pay $20 for someone to solve those problems for you. That same humanoid robot could do more useful things like vacuum your house or soothe your baby or drive your car. Those things are worth more than $20 and we already use robots to solve those problems – they just aren’t humanoid. You can pay $400 to have a robot vacuum your house. For around $250 you can buy an infant seat to soothe your baby. When buying a new car you can select robotic options on the fringe of self-driving like lane departure warning and collision avoidance and dynamic cruise control. It’s easy to see that fully autonomous self-driving cars are on the horizon; it’s just a question of how far off. The drones used by the military are another example of non-humanoid robotics. The point is, we don’t need to build a humanoid robot to do these things. We can build the robots into the things themselves. The computer industry offers an interesting analogy. For decades, computers were multipurpose platforms that got faster and cheaper but otherwise did not change much. However, in the early 2000s, for the first time in history, a few cents’ worth of computing power was really powerful. You could call it the second coming of Moore’s Law. That gave us the ability to start integrating intelligence into devices at a rapid rate, and it reduced the need for super powerful, multipurpose platforms like PCs. Today, tablets and smartphones do much of the work that our PCs used to do and we have loads of new devices, such as DVRs, fitness trackers and intelligent thermostats, that are essentially special-purpose computers. Because computer technology got so powerful and inexpensive, the answer wasn’t to keep extending the reach of the multipurpose PC. We added the intelligence of the PC into the devices around us – a march that is going to continue indefinitely. The idea that we’ll have robots in the future to assist in daily tasks isn’t wrong. The idea that we need a multipurpose humanoid platform is. As costs continue to decline and technology continues to advance, we’ll be able to make lots of special-purpose robots to solve real consumer problems. Unfortunately for Aristotle, and the little kid in all of us, there’s just no need to make them humanoid.

---

## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/31/silicon-valley-reinvents-the-invisible-hand/
### Query Relevance Score: 0.10312476754188538
### Highlights: None

Jobs are the lifeblood of the economy. Whether self-employed or company-employed, workers rely on steady incomes to consume goods and pay for all of the necessities of life. When jobs are plentiful, friction decreases between job applicants and employers, ensuring that income is earned earlier and more frequently. Adam Smith’s invisible hand is the guiding force of the market in a capitalist system. By allowing everyone to make independent decisions in whatever way they define their self-interest, the market is presumed to automatically clear, moving goods and services from those who can offer them to those who most desire them. It’s ultimately a statement about market efficiency. That same invisible hand also rules the labor markets. Salaries are commensurate with skills, and employers compete for the workers they desire. If one company is able to create more profit from a talented individual than another company, then they theoretically will offer greater rewards (since they can) and poach that worker away. That’s the theory anyway. In reality, labor markets are filled with a viscous friction that continues to bedevil hiring managers and workers alike. Even in the best of times, it can take weeks from the moment someone starts seeking a job until they have secured a new income (and of course, it can take another several weeks for the first paycheck to actually arrive). In worse times, those weeks can quickly drag out to months and even years, creating a permanent class of unemployed workers with little hope of securing a job. Labor markets are filled with friction, which means that losing a job can easily destroy the financial security of an entire family. To get a sense of this friction, just note how easy it is to buy a product versus hiring an employee, and more importantly, how much easier the former has become over the years compared to the latter. Buying a product from Amazon today can be as simple as a single click, and it arrives on time with great customer service in case something goes wrong. Labor markets seem like they are stuck in Adam Smith’s era. We still apply for jobs the way we did decades ago, with resumes and cover sheets. We still conduct interviews, even though there is increasing proof not to mention plentiful anecdata that such practices are not a good judge of a worker’s abilities. Frankly, the biggest improvement to the process has been the use of machine learning to read resumes, but that has only led to keyword gaming and other useless activities which fail to actually make hiring more effective. Put simply, the invisible hand has been broken in the labor markets for years, even if we haven’t fully realized it. The reason is that labor markets are thin, with few available workers and even fewer employers. That’s why algorithmic marketplaces are so important. Startups are completely transforming the nature of labor markets, changing our notions of what employment can be and how income security functions. In the process, they are not just making our economy more efficient, but improving the lives of workers. Job Security and Thick Labor Markets To understand the context for these changes to our labor markets, we have to peer back into history to understand their development. For centuries, work revolved around the land. Nearly everyone in Europe following the fall of the Roman Empire was related to the land, whether working it directly, or in the case of feudal lords, sitting and waiting for the land to be tilled. What if looking at the past is precisely the wrong way to solve the problem though? What would happen if we actually had thick labor marketplaces without the friction that exists today? Our modern notion of income security is completely alien in this context. Income was the harvest, and variations in weather patterns could ensure a bountiful crop or a disastrous winter. There were some means of absorbing these shocks, but ultimately the market could force even the most careful families to succumb to its will. The growth in cities started to change this dynamic. For the first time, a worker was able to move between jobs, freed from a direct connection to the soil. These markets were rudimentary, and they remained so for centuries until the rapid growth of the industrial age in the early 1800s transformed them. From then on, we observe steady progress toward our modern notion of labor. What are some of those qualities? A safe work environment is one key component of the modern workplace, as well as various mechanisms for rest like weekends and vacation. But probably the most important element of the modern development of labor markets is the protections afforded workers from being fired. Whether tenure laws, anti-discrimination laws, human resources rules, lawsuits, or other institutions, safety from job termination is arguably one of the most fought after rights of workers over the past two centuries of development. I’ve already alluded to the reason for this focus. Labor markets are filled with friction, which means that losing a job can easily destroy the financial security of an entire family. Since it can take weeks or even months to find new employment, workers desire to stay in their current jobs as long as possible. As employment protections have decreased in the United States over the past few decades, families have compensated by relying on dual-incomes to ensure steady finances. In short, job security in our current labor markets is about avoiding job switches. Fundamentally, the challenge is one of the “thickness” of the market. Market thickness can be defined as the simultaneous number of buyers and sellers in a market. When there are only a handful of consumers and producers, it is hard for prices to be set and thus, for transactions to be completed. As both sides increase in volume, markets have more information, and thus are able to operate more efficiently and more rapidly. This is one of the reasons why it is easier to buy clothes than a house – the high price of houses and thus limited number of transactions makes it difficult for buyers and sellers to understand the market and make a deal. This is even more acute in the labor market. Outside of a handful of tech hubs where employee turnover is rapid, hiring is not at all robust. That makes labor markets quite thin, with limited numbers of employees moving from company to company to provide information on current market conditions to other participants. In these locales across the United States and really the whole world, the entire goal is to hold on to employment as tightly as possible and avoid the labor market as much as possible. For all of our progress, so little has changed. What Happens If We Have Thick Labor Markets? Actually, there has been some progress, most of it terrible. If you want to see what a somewhat thick labor market looks like, take a look at the service jobs in areas like food preparation and building maintenance. Due to innovations like franchising and increased usage of independent contractor terms for employment, these workers have none of the job protections of their predecessors and all the downside of friction-filled labor marketplaces. Employers have the upper hand in these transactions, and that has led to a disintegration of living wages and stable work arrangements. The schedules for service workers are anything but stable from week-to-week, creating vast problems for scheduling multiple jobs and child care for families. That has led to even more focus on holding onto a job no matter what the costs are, since switching jobs when you are living on the razor’s edge is impossible without financial disaster. We can see these trends reflected in all of the statistics about inequality that have been discussed in the media and academia the past few years. Middle-class jobs that used to provide job security and decent pay are increasingly being replaced by low-wage contract labor without any job protections at all. Wealth is being accumulated by a small percentage of the population, while the sons and daughters of middle-class families discover that the hollowing out of the American economy is going to force them to slide down the economic ladder. That has led to movements like Occupy Wall Street to demand changes to the ways that companies handle workers. Looking back at the 1960s and 70s, there is an increasing desire to return to the way things were, with secure employment in companies with greater job protections (of course, with women actually in the workplace this time). The cry from all of these movements is the same and it is clear: they want greater protection from the vagaries of the market, and they want everyone to be placed on an equal footing in the economy. That is one approach, but what if looking at the past is precisely the wrong way to solve the problem though? What would happen if we actually had thick labor marketplaces without the friction that exists today? Take, for instance, the service worker who can’t risk changing their job because they need their wages to arrive absolutely on time otherwise everything will fall apart. Since their shift schedule is changing so often, this individual also can’t switch jobs because they can’t find the time to interview. With a thick labor market, suddenly this person is able to seek out employment and instantly know who is hiring. Wages would be clearly available since information in the market is plentiful, and shift schedules would have to be equally spelled out. If the market was thick enough, it would even be possible to switch jobs within a single day. The ability of a software engineer in Silicon Valley to be working at Google in the morning and then switch to Facebook by the afternoon would be available to everyone in the market. It is our current system that is dehumanizing, whether it is applying for hundreds of jobs and never receiving a response or doing stupid exercises during interviews that are completely unrelated to the job that we applied for in the first place. For the first time, those service workers who have been powerless to resist their employers’ caprices would suddenly find that they have new leverage to use: they can actually leave since the market can provide them immediate employment. Their employer would be forced to comprehend their loss of power, or otherwise they may soon find that they have no workers at all. Suddenly, job security isn’t about avoiding the market, but rather fully embracing it. Knowing that you always have another job available can provide a psychological relief that no rule or law will ever fully offer. That also ensures that those who leave the market, say for maternity leave, can easily find their way back into the market again. It isn’t just job security that benefits with this development, but also job flexibility. Thick labor markets can also facilitate greater flexibility with shift schedules and work hours. A worker who wants to take a Wednesday off can now just choose to do so, knowing that the market will work its invisible hand to find another worker to take the shift. Similarly, workers with an extra hour or two may be able to find something to do productively in that time, providing them an extra bit of income. All of this sounds great, but there is an obvious concern. Markets only work when the price mechanism is allowed to be fully determined by market dynamics, and that means that wages will fluctuate until an equilibrium is established. Doesn’t that mean wages will plunge as a great mass of workers seek out these newly accessible service jobs that were previously difficult to get due to market friction? It is an obvious criticism, and an important one, but it makes one fatal flaw: it takes its data from thin labor markets, and not thick ones. With thick labor markets, employers could no longer miss wage payments because they want to make an extra buck – they would be forced out of the market due to their terrible reputation. Employers for the first time would actually have to compete for workers, since workers would have the leverage to leave at will at the prevailing wage. Perhaps most powerfully, consumers would know the general pay of individual establishments, because again, the market is able to provide that information to all participants. All of these forces would be strong, and all would push wages higher. We shouldn’t assume that the dynamics of our current labor market would carry over to a thick labor market. It’s entirely different, and in terms of job security, is vastly superior to our current model. The question of wages is vitally important, but we shouldn’t immediately dismiss thick labor markets as wage destroyers. There are other forces at work they may actually make them higher. Rebuilding The Invisible Hand The idea of thick labor markets is not unknown to Silicon Valley – Uber is the obvious example that comes up here. According to their own research, the company has created 160,000 flexible jobs for workers, and many thousands more are on the way. But Uber is just one small piece of the overall labor market, and thus its effect is still not enough to thicken the overall market. The idea of thick labor markets may be a bit of a thought experiment. Certainly their effects are a bit of a mystery – markets work in complex ways, and emergent properties of these markets may be difficult if not impossible to predict without building them in the first place. However, it is not hard to believe that technology and startups are going to increasingly take the friction out of the marketplace. Better algorithms can align the right workers with the right employers almost instantaneously, and reputation systems ensures that workers are more consistent in their work (and employers act responsibly as well). There are concerns that workers are dehumanized in such algorithmic assignment of work, a criticism that I strongly disagree with. It is our current system that is dehumanizing, whether it is applying for hundreds of jobs and never receiving a response or doing stupid exercises during interviews that are completely unrelated to the job that we applied for in the first place. Thicker labor markets would be an incredible improvement over our existing arrangement. I want to make finding work as easy as buying a pair of socks on Amazon. I want to hit a button, and be at my job in 20 minutes ready to go. When technology has allowed that to happen, we will have finally have given the power to workers to shape their lives how they want to. What a revolution indeed.

---

## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/02/the-year-of-big-data-is-upon-us/
### Query Relevance Score: 0.0998106449842453
### Highlights: None

Editor’s note: Deven Parekh is a managing director at Insight Venture Partners where he manages investments in e-commerce, consumer Internet data, and application software businesses on a global basis. The great Danish physicist Niels Bohr once observed that “prediction is very difficult, especially if it’s about the future.” Particularly in the ever-changing world of technology, today’s bold prediction is liable to prove tomorrow’s historical artifact. But thinking ahead about wide-ranging technology and market trends is a useful exercise for those of us engaged in the business of partnering with entrepreneurs and executives that are building the next great company. Moreover, let’s face it: gazing into the crystal ball is a time-honored, end-of-year parlor game. And it’s fun. So in the spirit of the season, I have identified five big data themes to watch in 2015. As a marketing term or industry description, big data is so omnipresent these days that it doesn’t mean much. But it is pretty clear that we are at a tipping point. The global scale of the Internet, the ubiquity of mobile devices, the ever-declining costs of cloud computing and storage, and an increasingly networked physical word create an explosion of data unlike anything we’ve seen before. The creation of all of this data isn’t as interesting as the possible uses of it. I think 2015 may well be the year we start to see the true potential (and real risks) of how big data can transform our economy and our lives. Big Data Terrorism The recent Sony hacking case is notable because it appears to potentially be the first state-sponsored act of cyber-terrorism where a company has been successfully threatened under the glare of the national media. I’ll leave it to the pundits to argue whether Sony’s decision to postpone releasing an inane farce was prudent or cowardly. What’s interesting is that the cyber terrorists caused real fear to Sony by publicly releasing internal enterprise data — including salaries, email conversations and information about actual movies. Every Fortune 2000 management team is now thinking: Is my data safe? What could happen if my company’s data is made public and how could my data be used against me? And of course, security software companies are investing in big data analytics to help companies better protect against future attacks. Big Data Becomes a Civil Liberties Issue Data-driven decision tools are not only the domain of businesses but are now helping Americans make better decisions about the school, doctor or employer that is best for them. Similarly, companies are using data-driven software to find and hire the best employees or choose which customers to focus on. But what happens when algorithms encroach on people’s privacy, their lifestyle choices and their health, and get used to make decisions based on their race, gender or age — even inadvertently? Our schools, companies and public institutions all have rules about privacy, fairness and anti-discrimination, with government enforcement as the backstop. Will privacy and consumer protection keep up with the fast-moving world of big data’s reach, especially as people become more aware of the potential encroachment on their privacy and civil liberties? Open Government Data Expect the government to continue to make government data more “liquid” and useful – and for companies to put the data to creative use. The public sector is an important source of data that private companies use in their products and services. Take Climate Corporation, for instance. Open access to weather data powers the company’s insurance products and Internet software, which helps farmers manage risk and optimize their fields. Or take Zillow as another example. The successful real estate media site uses federal and local government data, including satellite photography, tax assessment data and economic statistics to provide potential buyers a more dynamic and informed view of the housing market. Personalized Medicine Even as we engage in a vibrant discussion about the need for personal privacy, “big data” pushes the boundaries of what is possible in health care. Whether we label it “precision medicine” or “personalized medicine,” these two aligned trends — the digitization of the health care system and the introduction of wearable devices — are quietly revolutionizing health and wellness. In the not-too-distant future, doctors will be able to create customized drugs and treatments tailored for your genome, your activity level, and your actual health. After all, how the average patient reacts to a particular treatment regime generically isn’t that relevant; I want the single best course of treatment (and outcome) for me. Health IT is already a booming space for investment, but clinical decisions are still mostly based on guidelines, not on hard data. Big data analytics has the potential to disrupt the way we practice health care and change the way we think about our wellness. Digital Learning, Everywhere With over $1.2 trillion spent annually on public K-12 and higher education, and with student performance failing to meet the expectations of policy makers, educators and employers are still debating how to fix American education. Some reformers hope to apply market-based models, with an emphasis on testing, accountability and performance; others hope to elevate the teaching profession and trigger a renewed investment in schools and resources. Both sides recognize that digital learning, inside and outside the classroom, is an unavoidable trend. From Massive Open Online Courses (MOOCs) to adaptive learning technologies that personalize the delivery of instructional material to the individual student, educational technology thrives on data. From names that you grew up with (McGraw Hill, Houghton Mifflin, Pearson) to some you didn’t (Cengage, Amplify), companies are making bold investments in digital products that do more than just push content online; they’re touting products that fundamentally change how and when students learn and how instructors evaluate individual student progress and aid their development. Expect more from this sector in 2015. Now that we’ve moved past mere adoption to implementation and utilization, 2015 will undoubtedly be big data’s break-out year.

---

## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/25/what-happens-to-privacy-when-the-internet-is-in-everything/
### Query Relevance Score: 0.09409362077713013
### Highlights: None

This week Google’s Eric Schmidt was on a panel at the World Economic Forum in Davos, where he suggested that the future Internet will be, in one sense, invisible — because it will be embedded into everything we interact with. “The Internet will disappear,” he predicted (via The Hollywood Reporter). “There will be so many IP addresses…so many devices, sensors, things that you are wearing, things that you are interacting with that you won’t even sense it. It will be part of your presence all the time. “Imagine you walk into a room, and the room is dynamic. And with your permission and all of that, you are interacting with the things going on in the room.” This is not an especially outlandish forecast, given the trajectory of connected devices. Analyst Gartner calculated there were some 3.8 billion such ‘smart objects’ in use last year, and forecast 4.9 billion this — rising to 25 billion in circulation by 2020. (The global human population was estimated at around seven billion, at the last count.) In other words the sensornet is here, it’s just not densely (or evenly) distributed yet. Google already owns Nest, a maker of connected devices for the home, such as a smoke alarm and learning thermostat. Google-Nest also owns Dropcam, a Wi-Fi security camera maker. Mountain View is clearly making a bid to be the nexus of the ‘connected home’ — which, along with the ‘connected car’ (of course Google is also building driverless, Internet-tethered cars), is the early locus for the sensornet. See also: wearables (‘connected people’), and the fact smartphones are gaining additional embedded sensors, turning our pervasive pocket computers into increasingly sensory mobile data nodes. One of Davos’ more outlandish (perhaps) predictions for our increasingly connected future came from a group of Harvard professors who apparently sketched a scenario where mosquito sized-robots buzz around stealing samples of our DNA, as reported by Mail Online. “Privacy as we knew it in the past is no longer feasible,” computer science professor Margo Seltzer is quoted as saying. “How we conventionally think of privacy is dead.” What Seltzer was actually arguing is that it needs no sneaky, DNA-sealing robo-mosquitos for connected technologies to violate our privacy. The point is, she later told TechCrunch, we are already at a privacy-eroding tipping point — even with current gen digital technologies. Let alone anything so futuristic as robotic mosquitos. “The high order message is that we don’t need pervasive sensor net technologies for this to be true. We merely have to use technologies that exist today: credit cards, debit card, the web, roads, highway transceivers, email, social networks, etc. We leave an enormous digital trail,” she added. Seltzer was also not in fact arguing for giving up on privacy — even if the Mail’s article reads that way. But rather for the importance of regulating data and data usage, rather than trying to outlaw particular technologies. “Technology is neither good nor bad, it is a tool,” she said. “However, hammers are tools too. They are wonderful for pounding in nails. That doesn’t mean that someone can’t pick up a hammer and use it to commit murder. We have laws that say you shouldn’t murder; we don’t specialize the laws to call out hammers. Similarly, the laws surrounding privacy need to be laws about data and usage, not about the technology.” With your permission What especially stands out to me from Schmidt’s comments at Davos is his afterthought caveat — that this invisible, reactive, all-pervasive future sensornet will be pulling its invisible strings with your permission. Perhaps he was paying lip-service to the warning of the FTC’s Chairwoman, Edith Ramirez, at CES earlier this month that building connected objects — the long discussed ‘Internet of Things’ — demands a new responsibility from businesses and startups to bake security and privacy protections into their products right from the get go. “[The Internet of Things] has the potential to provide enormous benefits for consumers, but it also has significant privacy and security implications,” she warned. “Connected devices that provide increased convenience and improve health services are also collecting, transmitting, storing, and often sharing vast amounts of consumer data, some of it highly personal, thereby creating a number of privacy risks.” Ramirez said that without businesses adopting security by design; engaging in data minimization rather than logging everything they can; and being transparent about the data they are collecting — and who else they want to share it with — by providing notifications and opt outs to users; then the risks to users’ privacy and security are enormous. The problem with those well-meaning words from a consumer watchdog organization is that we are already struggling to achieve such rigorous privacy standards on the current Internet — let alone on a distributed sensornet where there’s no single, controllable entry point into our lives. The Internet and the mobile Internet can still be switched off, in extremis, by the user turning off their router and/or powering their phone down (and putting it in the fridge if you’re really paranoid, post-Snowden). But once a distributed sensornet has achieved a certain penetration tipping point, into the objects with which we humans are surrounded, well then the sheer number of devices involved is going to take away our ability to trivially pull the plug. Unless some kind of regulatory layer is also erected to provide a framework for usage that works in the interests of privacy and consumer control. Without such consumer-oriented controls embedded into this embedded Internet, the user effectively loses the ability to take themselves offline, given that the most basic level of computing control — the on/off switch — is being subducted beneath the grand, over-arching utility of an all-seeing, always on sensornet. (Battery life constraints, in this context, might be viewed as a privacy safeguard, although low power connectivity technologies, such as Bluetooth Low Energy, work to circumvent that limit.) In parallel, a well-distributed Internet of Things likely demands greater levels of device automation and autonomy, given the inexorable gains in complexity generated by a dense network of networked objects. And because of the sheer number of connected devices. And more automation again risks reducing user control. Connected objects will be gathering environmental intelligence, talking to each other and talking to the cloud. Such a complex, interwoven web of real-time communications might well generate unique utility — as Schmidt evidently believes. But it also pulls in increased privacy concerns, given how many more data points are being connected and how all those puzzle pieces might slot together to form an ever more comprehensive, real-time representation of the actions and intentions of the people moving through this web. Earlier generation digital technologies like email were not engineered with far-sighted privacy protections in mind. Which is why they have been open to abuse — to being co-opted as part of a military industrial surveillance complex, as the Snowden revelations have shown, offering a honeypot of metadata for government intelligence agencies to suck up. Imagine what kind of surveillance opportunities are opened up by an ‘invisible’ Internet — which is both everywhere but also perceptually nowhere, encouraging users to submit to its data-mining embrace without objection. After all how can you resist what you can’t really see or properly control? All the debate about security services getting access to encrypted apps is a smokescreen. #IoT means they'll be able monitor you anyway — Dean Bubley (@disruptivedean) January 14, 2015 That is exactly the Internet that Schmidt wants to build, from his position atop Google’s ad sales empire. The more intelligence on web users Google can harvest, the more data it can package up and sell to companies who want to sell you stuff. Which, for all Google’s primary-colored, doodle-festooned branding, is the steely core of its business. Mountain View has long talked about wanting search to become predictive. Why? Because marketing becomes a perfect money-pipe if corporates can channel and influence your real-time intentions. That’s the Google end game. Learning about human intention from the stuff people type into search engines is laughably crude compared to how much can be inferred from a sensornet that joins up myriad, real-time data-dots and applies machine learning data-mining algorithms dynamically. More dots are already being joined by Google, across multiple web products and its mobile platform Android — which brings it a rich location layer. Doing even more and deeper data mining is a natural evolution of its business model. (Related: Google acquired AI firm Deep Mind last year — a maker of “general-purpose learning algorithms”.) The core reality of the Internet of Things is that a distributed network of connected objects could be deliberately engineered to catch us in its web — triangulating our comings and goings as we brush past its myriad nodes. The more connected objects surround us, the more data points wink into existence to be leveraged by the Googles of the digital world to improve the accuracy and texture of their understanding of our intentions, whether we like it or not. So while the future Internet may appear to fade into the background, as Schmidt suggests, that might just signify a correspondingly vast depth of activity going on in the background. All the processing power required to knit together so many connections and weave a concealed map of who we are and what we do. The risk here, clearly, is that our privacy is unpicked entirely. That an embedded ‘everywhere Internet’ becomes a highly efficient, hugely invasive machine analyzing us at every turn in order to package up every aspect of our existence as a marketing opportunity. That’s one possible future for the sensornet. But it seems to me that that defeatist argument is also part of the spinning which vested interests like Google, whose business models stand to benefit massively, engage in when they discuss the digital future that they are trying to shape. Technology is a tool. Diverse applications are possible. And just because technology makes something possible does not also mean it is inevitable. As Seltzer says, we need to be thinking about how we want the data to flow or not flow, rather than throwing our hands up in horror or defeat. What is also clearly necessary — indeed, I would argue, is imperative — is joined up thinking from regulators to comprehend the scope of the privacy risks posed by increasingly dense networks of networked objects, and how the accumulation of data-points can collectively erode consumer privacy. A clear-sighted strategy for ensuring end users can comprehend and control the processing of their personal data is paramount. Without that, the risk for startup businesses playing in this space is that the rise of more and more connected devices will be mirrored by a parallel rise in human mistrust of increasingly invasive products and services. In the hyper personal realm of the Internet of Things, user trust is paramount. So building a framework to regulate the data flows of connected devices now, while the sensornet is still in its infancy, is imperative for everyone involved. In the offline world we have cars and roads. We also have speed limits — for a reason. The key imperative for regulators now, as we are propelled towards a more densely-packed universe of connected devices, is coming up with the sensornet’s speed limits. And fast.

---

## Article published: 01/2015
### Source URL: https://www.nytimes.com/2015/01/11/magazine/death-by-robot.html?referrer=
### Query Relevance Score: 0.10990512371063232
### Highlights: None

The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you’d like. nytimes.com/subscription

---

## Article published: 01/2015
### Source URL: https://www.nytimes.com/2015/01/18/books/review/among-the-disrupted.html
### Query Relevance Score: 0.10286112129688263
### Highlights: None

The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you’d like. nytimes.com/subscription

---

## Article published: 01/2015
### Source URL: https://www.nytimes.com/2015/01/29/technology/personaltech/uber-a-rising-business-model.html?ref=business
### Query Relevance Score: 0.10057410597801208
### Highlights: None

State of the ArtCredit...Stuart GoldenbergJan. 28, 2015As Uber has grown to become one of the world’s most valuable start-ups, its ambitions often seem limitless.But of all the ways that Uber could change the world, the most far-reaching may be found closest at hand: your office. Uber, and more broadly the app-driven labor market it represents, is at the center of what could be a sea change in work, and in how people think about their jobs. You may not be contemplating becoming an Uber driver any time soon, but the Uberization of work may soon be coming to your chosen profession.Just as Uber is doing for taxis, new technologies have the potential to chop up a broad array of traditional jobs into discrete tasks that can be assigned to people just when they’re needed, with wages set by a dynamic measurement of supply and demand, and every worker’s performance constantly tracked, reviewed and subject to the sometimes harsh light of customer satisfaction. Uber and its ride-sharing competitors, including Lyft and Sidecar, are the boldest examples of this breed, which many in the tech industry see as a new kind of start-up — one whose primary mission is to efficiently allocate human beings and their possessions, rather than information.Various companies are now trying to emulate Uber’s business model in other fields, from daily chores like grocery shopping and laundry to more upmarket products like legal services and even medicine.“I do think we are defining a new category of work that isn’t full-time employment but is not running your own business either,” said Arun Sundararajan, a professor at New York University’s business school who has studied the rise of the so-called on-demand economy, and who is mainly optimistic about its prospects.Uberization will have its benefits: Technology could make your work life more flexible, allowing you to fit your job, or perhaps multiple jobs, around your schedule, rather than vice versa. Even during a time of renewed job growth, Americans’ wages are stubbornly stagnant, and the on-demand economy may provide novel streams of income.ImageCredit...Todd Heisler/The New York Times“We may end up with a future in which a fraction of the work force would do a portfolio of things to generate an income — you could be an Uber driver, an Instacart shopper, an Airbnb host and a Taskrabbit,” Dr. Sundararajan said.But the rise of such work could also make your income less predictable and your long-term employment less secure. And it may relegate the idea of establishing a lifelong career to a distant memory.“I think it’s nonsense, utter

---

## Article published: 01/2015
### Source URL: https://www.nytimes.com/2015/01/16/opinion/the-cruel-waste-of-americas-tech-talent.html
### Query Relevance Score: 0.09760820865631104
### Highlights: None

The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you’d like. nytimes.com/subscription

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/video/4002759760001
### Query Relevance Score: 0.03778235241770744
### Highlights: None

Home Watch Live Shows Topics Join the conversation Log in to comment on videos and join in on the fun. Watch Live TV Watch the live stream of Fox News and full episodes. Reduce eye strain and focus on the content that matters. This video is playing in picture-in-picture. Live Now All times eastern NOW - 3:30 AM 3:30 AM 4:00 AM 4:30 AM 5:00 AM 5:30 AM Fox News Channel The Ingraham Angle 3:00 AM - 4:00 AM Fox & Friends First 4:00 AM - 5:00 AM Fox & Friends First 5:00 AM - 6:00 AM Fox Business Channel Paid Programming 3:00 AM - 3:30 AM Paid Programming 3:30 AM - 4:00 AM American Dream Home 4:00 AM - 4:30 AM American Dream Home 4:30 AM - 5:00 AM Fox News Radio FOX News Radio Live Stream

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/science/new-apple-technology-stops-iphones-from-filming-live-events.print
### Query Relevance Score: 0.027467504143714905
### Highlights: None

By , Published January 08, 2015 Apple unveils the newest design for its iPhone at the Worldwide Developer Conference. (Engadget) CUPERTINO, Calif. -- Fans at concerts and sports games may soon be stopped from using their iPhones to film the action -- as a result of new technology being considered by Apple, The Times of London reported Thursday. The California company has plans to build a system that will sense when a person is trying to film a live event using a cell phone and automatically switch off their camera. A patent application filed by Apple, and obtained by the Times, reveals how the software would work. If a person were to hold up their iPhone, the device would trigger the attention of infra-red sensors installed at the venue. These sensors would then instruct the iPhone to disable its camera. Other features, such as the phone’s ability to make a call or send text messages, would not be affected. The software is seen as an attempt to protect the interests of event organizers and television broadcasters who have exclusive rights to film an event. These companies often sell their own recordings but are frustrated when cell phone videos appear online via websites such as YouTube, allowing people to watch the concert free. The concept may also allow Apple to reach more favorable terms with record labels when negotiating deals to sell content though its iTunes online store. Apple filed its patent application with U.S. authorities 18 months ago, but details only became available this month. It is not clear if Apple intends to develop the concept into a working system, and if so, whether it will be implemented within the company’s devices. The latest version of iPhone is expected in September. Apple declined to comment. URL https://www.foxnews.com/science/new-apple-technology-stops-iphones-from-filming-live-events

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/science/amazing-photo-captures-robot-cargo-ships-space-station-arrival.print
### Query Relevance Score: 0.022411145269870758
### Highlights: None

By , Published January 13, 2015 Astronauts aboard the International Space Station captured an extraordinary photo of an unmanned European cargo ship as it docked to the orbiting outpost last week. The European Space Agency's third Automated Transfer Vehicle (ATV-3) launched into orbit on March 23, and arrived at the space station five days later, on March 28. The robotic cargo ship delivered about 7 tons of supplies, including water, oxygen, food, clothing, experiments and propellant. The robotic ATV vehicles are designed to automatically dock to the space station. In this photo, the ATV-3 is approaching its parking spot at the Zvezda service module on the Russian segment of the orbiting complex. The photo shows the ATV-3's distinctive x-wing solar arrays bathed in light from the spacecraft's sophisticated laser guidance system. The starry night sky and the glow of lights from Earth below make up the remarkable backdrop, as the two vehicles fly 240 miles (386 kilometers) above the planet. American astronaut Don Pettit shared a version of the docking photo on Twitter the following day. "ATV docks, breathing fire and bringing good stuff," Pettit wrote under the name @astro_Pettit. The cylindrical spacecraft is 35 feet (10.7 meters) long and 14.7 feet (4.5 meters) wide. The 13-ton cargo freighter is disposable and will remain attached to the space station for up to six months before it is loaded with garbage and sent to deliberately burn up as it re-enters Earth's atmosphere. While NASA reported a power issue on the ATV-3 last weekend and a communications glitch on Thursday (April 5), both issues have since been resolved, and all systems are functioning normally, agency officials confirmed. The European ATV vehicles are typically named after prominent historical figures in astronomy or space exploration. The ATV-3 is named "Edoardo Amaldi," after the famed Italian physicist who helped create the European Space Agency. URL https://www.foxnews.com/science/amazing-photo-captures-robot-cargo-ships-space-station-arrival

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/video/4017758806001
### Query Relevance Score: 0.022259073331952095
### Highlights: None

Home Watch Live Shows Topics Join the conversation Log in to comment on videos and join in on the fun. Watch Live TV Watch the live stream of Fox News and full episodes. Reduce eye strain and focus on the content that matters. This video is playing in picture-in-picture. Live Now All times eastern NOW - 2:00 PM 2:00 PM 2:30 PM 3:00 PM 3:30 PM 4:00 PM Fox News Channel America Reports 1:00 PM - 2:00 PM America Reports 2:00 PM - 3:00 PM The Story 3:00 PM - 4:00 PM Your World with Neil Cavuto 4:00 PM - 4:30 PM Fox Business Channel The Big Money Show 1:00 PM - 2:00 PM Making Money with Charles Payne 2:00 PM - 3:00 PM The Claman Countdown 3:00 PM - 4:00 PM Fox News Radio FOX News Radio Live Stream

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/us/citing-costs-ibm-to-move-retirees-off-health-plan.print
### Query Relevance Score: 0.02224314771592617
### Highlights: None

By , Published January 13, 2015 International Business Machines Corp. plans to move about 110,000 retirees off its company-sponsored health plan and instead give them a payment to buy coverage on a health-insurance exchange, in a sign that even big, well-capitalized employers aren't likely to keep providing the once-common benefits as medical costs continue to rise. The move, which will affect all IBM retirees once they become eligible for Medicare, will relieve the technology company of the responsibility of managing retirement health-care benefits. IBM said the growing cost of care makes its current plan unsustainable without big premium increases. IBM's shift is an indication that health-insurance marketplaces, similar to the public exchanges proposed under President Barack Obama's health-care overhaul, will play a bigger role as companies move coverage down the path taken by many pensions, paying employees and retirees a fixed sum to manage their own care. In notices signed by Chief Health Director Kyu Rhee, IBM has told retirees in recent weeks that to keep receiving coverage, they will need to pick a plan offered through Extend Health, a large private Medicare exchange run by New York-based Towers Watson & Co. Medicare is the federally administered system of health insurance for people age 65 and over, and the disabled. Some people buy Medicare Advantage plans, administered by private insurers, and others buy policies to cover gaps in Medicare coverage. IBM told retirees that its current retiree coverage will end for Medicare-eligible retirees after Dec. 31, 2013, according to documents reviewed by The Wall Street Journal and confirmed by IBM. "Cost increases under our current retirement group health care plan are no longer sustainable for you," IBM said in the notices. "Health care costs under IBM's current plan options for Medicare eligible retirees will nearly triple by 2020, significantly impacting your premium and out of pocket costs," the notice said. Exchanges such as Extend Health generally present policies from a range of insurers and let participants choose what best meets their needs and budgets. The aim is to create competition that keeps costs down. Instead of subsidizing retiree health premiums directly, IBM will give retirees an annual contribution via a health retirement account that they can use to buy Medicare Advantage plans and supplemental Medicare policies on the exchange, as well as pay for other medical expenses. Retirees who don't enroll in a plan through Extend Health won't receive the subsidy. Some companies began experimenting with exchanges around eight years ago after accounting changes forced public corporations to disclose future health-care obligations. Click for more from WSJ.com URL https://www.foxnews.com/us/citing-costs-ibm-to-move-retirees-off-health-plan

---

## Article published: 01/2015
### Source URL: https://www.wired.com/2015/01/ai-arrived-really-worries-worlds-brightest-minds/
### Query Relevance Score: 0.09365683794021606
### Highlights: None

On the first Sunday afternoon of 2015, Elon Musk took to the stage at a closed-door conference at a Puerto Rican resort to discuss an intelligence explosion. This slightly scary theoretical term refers to an uncontrolled hyper-leap in the cognitive ability of AI that Musk and physicist Stephen Hawking worry could one day spell doom for the human race. That someone of Musk's considerable public stature was addressing an AI ethics conference—long the domain of obscure academics—was remarkable. But the conference, with the optimistic title "The Future of AI: Opportunities and Challenges," was an unprecedented meeting of the minds that brought academics like Oxford AI ethicist Nick Bostrom together with industry bigwigs like Skype founder Jaan Tallinn and Google AI expert Shane Legg. Musk and Hawking fret over an AI apocalypse, but there are more immediate threats. In the past five years, advances in artificial intelligence—in particular, within a branch of AI algorithms called deep neural networks—are putting AI-driven products front-and-center in our lives. Google, Facebook, Microsoft and Baidu, to name a few, are hiring artificial intelligence researchers at an unprecedented rate, and putting hundreds of millions of dollars into the race for better algorithms and smarter computers. AI problems that seemed nearly unassailable just a few years ago are now being solved. Deep learning has boosted Android's speech recognition, and given Skype Star Trek-like instant translation capabilities. Google is building self-driving cars, and computer systems that can teach themselves to identify cat videos. Robot dogs can now walk very much like their living counterparts. "Things like computer vision are starting to work; speech recognition is starting to work There's quite a bit of acceleration in the development of AI systems," says Bart Selman, a Cornell professor and AI ethicist who was at the event with Musk. "And that's making it more urgent to look at this issue." Given this rapid clip, Musk and others are calling on those building these products to carefully consider the ethical implications. At the Puerto Rico conference, delegates signed an open letter pledging to conduct AI research for good, while "avoiding potential pitfalls." Musk signed the letter too. "Here are all these leading AI researchers saying that AI safety is important," Musk said yesterday. "I agree with them." Google Gets on Board Nine researchers from DeepMind, the AI company that Google acquired last year, have also signed the letter. The story of how that came about goes back to 2011, however. That's when Jaan Tallinn introduced himself to Demis Hassabis after hearing him give a presentation at an artificial intelligence conference. Hassabis had recently founded the hot AI startup DeepMind, and Tallinn was on a mission. Since founding Skype, he'd become an AI safety evangelist, and he was looking for a convert. The two men started talking about AI and Tallinn soon invested in DeepMind, and last year, Google paid $400 million for the 50-person company. In one stroke, Google owned the largest available talent pool of deep learning experts in the world. Google has kept its DeepMind ambitions under wraps—the company wouldn't make Hassabis available for an interview—but DeepMind is doing the kind of research that could allow a robot or a self-driving car to make better sense of its surroundings. That worries Tallinn, somewhat. In a presentation he gave at the Puerto Rico conference, Tallinn recalled a lunchtime meeting where Hassabis showed how he'd built a machine learning system that could play the classic '80s arcade game Breakout. Not only had the machine mastered the game, it played it a ruthless efficiency that shocked Tallinn. While "the technologist in me marveled at the achievement, the other thought I had was that I was witnessing a toy model of how an AI disaster would begin, a sudden demonstration of an unexpected intellectual capability," Tallinn remembered. Deciding the dos and don'ts of scientific research is the kind of baseline ethical work that molecular biologists did during the 1975 Asilomar Conference on Recombinant DNA, where they agreed on safety standards designed to prevent manmade genetically modified organisms from posing a threat to the public. The Asilomar conference had a much more concrete result than the Puerto Rico AI confab. At the Puerto Rico conference, attendees signed a letter outlining the research priorities for AI—study of AI's economic and legal effects, for example, and the security of AI systems. And yesterday, Elon Musk kicked in $10 million to help pay for this research. These are significant first steps toward keeping robots from ruining the economy or generally running amok. But some companies are already going further. Last year, Canadian roboticists Clearpath Robotics promised not to build autonomous robots for military use. "To the people against killer robots: we support you," Clearpath Robotics CTO Ryan Gariepy wrote on the company's website.

---

## Article published: 01/2015
### Source URL: https://www.wired.com/2015/01/elon-musk-ai-safety/
### Query Relevance Score: 0.07879868149757385
### Highlights: None

Elon Musk is worried that artificial intelligence research could go wrong---very wrong. This may seem unexpected, coming as it does from the architect of the conceptual high-speed transportation system Hyperloop, and the CEO of such moonshot-seeking companies as SpaceX and Tesla Motors. But Musk is so committed to this point of view that on Thursday, he announced a donation of $10 million to the Future of Life Institute (FLI), which will run a global research program aimed at keeping AI “beneficial to humanity.” In other words, Musk wants to keep AI from running loose and growing into something that's a real danger to humans, a fear he's expressed before. Last week, an open letter from the Future of Life Institute circulated, containing the signatures of AI scientists who called for research that ensures AI systems aren't being used for evil. With this donation, Elon Musk voiced his support for the movement. "Here are all these leading AI researchers saying that AI safety is important,” Musk said of the effort. "I agree with them, so I'm today committing $10M to support research aimed at keeping AI beneficial for humanity." The program---and the $10 million donation---will fund research around the world supporting this mission. On Monday, the Future of Life Institute will open up a portal allowing researchers to apply for grants to the program. Beyond hardcore AI science, money will be awarded to researchers in other fields as well, including economics, law, ethics and policy. The FLI says the program will be open to individuals who work in academia, industry, or even independently. For so long, the artificially intelligent future has been fodder for Hollywood and science fiction, or discussed abstractly in philosophy. But as behemoths like Google incorporate AI into the very core of its current and future technologies, and as a wave of smaller startups build businesses on top of the science, we can no longer deny its very real rise. AI is here to stay; but the debate over AI ethics is just beginning.

---

## Article published: 01/2015
### Source URL: https://www.wired.com/2015/01/karpathy/
### Query Relevance Score: 0.06959018111228943
### Highlights: None

Andrej Karpathy knows what it's like to compete with artificial intelligence. He first went head-to-head with an artificial intelligence algorithm in 2011. A team of Stanford University researchers had just built the world's most effective image-recognition software, and he wanted to see how well his very real brain stacked up against their digital creation on what was, at the time, a standard image recognition test. The Stanford software analyzed a pool of about 50,000 images, slotting each into one of 10 categories, such as "dogs," "horses," and "trucks." It was right about 80 percent of the time. Karpathy took the same test and completely smoked the AI code, scoring 94 percent. Karpathy, himself a graduate student at Stanford, thought humans would beat machines on this type of test for a long time. "[I]t will be hard to go above 80 percent," he wrote in a blog post, referring to AI algorithms, "but I suspect improvements might be possible up to range of about 85-90 percent." Boy, was he wrong. Last year, a system built by researchers at Google aced another, more complex, image recognition test, called ImageNet, scoring 93.4 percent accuracy (you can see how Google's software performed on the test here). Again, Karpathy, with some colleagues at Stanford, went head-to-head with the system. But this time, they bombed what was a much more complex test, initially scoring about an 85 percent accuracy rate. Comparing the ImageNet test to the 2011 test software isn't exactly an apples-to-apples comparison, but here's the point: Humans were easily beating AI software in 2011; now that's not the case. Not by a long shot. The story goes a long way towards describing the excitement surrounding current artificial intelligence, which spans companies from Google to Facebook to IBM to Baidu. All of these giants are pouring big money into a burgeoning field called deep learning. Loosely modeled on the way the brain itself is able to accumulate knowledge, deep learning algorithms have been winning the ImageNet competition since 2013, and they've yielded remarkable results in the area of speech recognition, video recognition, and even financial analysis lately. This is causing a shake up in the field of AI, as problems that have been considered unsolvable for a very long time are suddenly being solved, says Stuart Russell, a University of California, Berkeley, professor and artificial intelligence expert. That said, computers still have a lot to learn. AI Boot Camp One reason Karpathy and his colleagues bombed against the Google systems was the way that ImageNet handles things like dogs. When he took that 2011 test, it had just one category for dogs. But in 2014, the test expected you to discern an artificial-mind-blowing 200 breeds. That meant Karpathy had to know the difference between, say, Rhodesian ridgebacks and Hungarian pointers. "When I saw all these dogs come up, I was like: "Oh no. [The machine is] going to get this image, and I'm just struggling and sweating to label this precise breed of dog." So Karpathy entered his own kind of AI boot camp, teaching himself the categories of images that the ImageNet test expected him to know, and becoming a minor authority on dog breeds in the process. Two weeks later, and after about 50 hours of training and testing himself by clicking on random pictures, he bested the machines. He was right 94.9 percent of the time, a 1.7 percent margin over Google's work. Chalk one up for humanity, but it wasn't easy. "It was a bit draining, but I felt that it was very important to get the human accuracy," he says. Abstract Thought At the same time, Karpathy and his colleagues want AI to improve. They're working to determine how the flaws in digital systems can be eliminated. "We're trying to see if the computers perform at a human level, but we're also trying to analyze their mistakes," he says. On the test, Karpathy could typically beat machines when he was presented with the image of something abstract. He could instantly spot, for example, a drawing of a bow. He could read the words "salt shaker" on a cone and understand what we was looking at. "Computers are not very good at abstract things," he says. They're also not good at figuring out images in three-dimensional reality. A computer might be able to spot a Jack Russell terrier. But reckoning its size, or figuring out how it is positioned relative to some other object in the same room? That's another matter. It's also one that the Googles of the world are, no doubt, trying to solve as they dream of computers that can interpret images with the depth and subtlety of humans. "Image recognition is important," Karpathy says, "but it's just a small piece of the puzzle."

---

## Article published: 01/2015
### Source URL: https://www.wired.com/2015/01/virtual-email-assistant/?mbid=social_twitter
### Query Relevance Score: 0.06297726929187775
### Highlights: None

Dennis Mortensen remembers the exact moment he decided he needed a better way to schedule meetings over email. He had just finished counting how many appointments he had in 2012. The number was 1,019. And of those 1,019 meetings, he had rescheduled 672 of them. “I looked at those two numbers, and I cried a little thinking about how much time I invested in emailing people,” Mortensen recalls. Mortensen shared his grievances with a good friend, Alex Poon, and the two tried a little experiment: They agreed to handle each other’s calendar appointments for the next couple of months. To their surprise the system worked out pretty well, even without a deep knowledge of each other’s preferences, such as knowing who the other person was willing to shift their schedule around for in order to meet. “We also started to see how quickly some of the responses clustered,” Mortensen says. “There’s a way people deal with meetings---how they set them up, postpone them, show gratitude, add new participants---and the whole thing started to crystallize in that moment.” So Mortensen and Poon, both analytics geeks, launched a startup called X.ai to work on an intelligent virtual assistant for scheduling meetings over email. They named their bot Amy, who has become another addition in the growing arsenal of intelligent, “invisible” software designed to automate the tedious minutiae of everyday life. Projects in this growing space range from a small decision such as deciding what to wear in the morning to big, complicated ones, like the algorithms that steer Google’s self-driving car. X.ai The X.ai system is still in closed beta, but it’s currently testing the virtual assistant internally with a few users. You don’t have to download an app or visit a website to use Amy (or Andrew, her male counterpart, if you prefer); she simply has her own email address. When someone emails you with a request for an appointment, you cc Amy and she takes it from there, handling the back and forth in a separate email chain. Since she can peek into your schedule, she can make nuanced suggestions, like meeting at your own workplace if you have a number of other appointments lined up in the same location that day. She can even handle typos. To pull this off, Amy uses an artificial intelligence technology known as natural language processing, drawing information from the email responses, breaking the sentences down, and attempting to classify each chunk properly. If the interaction is simple enough---the task is simply getting an appointment on the calendar---Mortensen claims Amy is accurate 98 percent of the time. She does a bit worse, he says, with more complicated scenarios. Say, for instance, two people had a meeting all set up for the next day, and late one night, one of them suddenly realized the time he'd agreed to was double-booked with another appointment. He might send a quick email to Amy at 1 a.m. trying to reschedule "tomorrow's meeting." Since 1 a.m. is technically past midnight, Mortensen explains, “Amy would literally interpret that as the next day, even if that wasn’t the sender’s intention.” But Mortensen says they're working on it. The Real World Problem Richard Socher, a one-time natural language processing researcher at Stanford University and current CTO of artificial intelligence startup MetaMind, says a portion of a system like the kind X.ai is trying to create can indeed be automated, but only up to a point. Things like personal preference matter, of course. There’s also time differences, locations of meetings, and other potential sources of real-world confusion. Socher points out that the problem could be more easily tackled by a player like Google, which has enough of a range of products that it could figure out where you live, where you work, and the current traffic. “There are a couple of situations that will require too much world knowledge, situational knowledge, and personal knowledge. That knowledge is going to be hard to collect,” he says. Even Mortensen acknowledges that X.ai’s final product may never be perfect. But he still thinks there’s potential to release a useful tool. “It needs a bit more data to work with, and we’ll work on it until we can get it as accurate as possible,” he says. “But I do have a fallback: going back to the user and asking them what they mean.”

---

## Article published: 01/2015
### Source URL: https://www.wired.com/insights/2015/01/google-and-elon-musk-good-for-humanity/
### Query Relevance Score: 0.06294625252485275
### Highlights: None

Skip Article Header. Skip to: Start of Article. esenkartal/Getty The recently published Future of Life Institute (FLI) letter “Research Priorities for Robust and Beneficial Artificial Intelligence,” signed by hundreds of AI researchers in addition to Elon Musk and Stephen Hawking, many representing government regulators and some sitting on committees with names like “Presidential Panel on Long Term AI future,” offers a program professing to protect the mankind from the threat of “super-intelligent AIs.” In a contrarian view, I believe that should they succeed, rather than upcoming salvation we will see a 21st century version of 17th century Salem Witch trials instead, where technologies competing with AI will be tried and burned at stake with much fanfare and applause from mainstream press. Before I proceed to my concerns, here’s some background on AI. For the last 50 years AI researchers have promised to deliver intelligent computers, which always seem to be five years in the future. For example, Dharmendra Modha, in charge of IBM’s Synapse “neuromorphic” chips, claimed two or three years ago that IBM “will deliver computer equivalent of human brain” by 2018. I have heard this echoed of in statements of virtually all recently funded AI and Deep Learning companies. The press accepts these claims with the same gullibility it displayed during Apple Siri’s launch, and hails arrival of the “brain like” computing as a fait accompli. This is very far from the truth. The investments on the other hand are real, with old AI technologies dressed up in new clothes of “Deep Learning.” In addition to acquiring Deep Mind, Google hired Geoffrey Hinton’s University of Toronto team as well as Ray Kurzweil, whose primary motivation for joining Google Brain seems to be the opportunity to upload his brain into vast Google supercomputer. Baidu invested $300M in Stanford University’s Andrew Ng Deep Learning lab, Facebook and Zuckerberg personally invested $55M in Vicarious and hired Yann LeCun, the “other” deep learning guru. Samsung and Intel invested in Expect labs and Reactor, and Qualcomm made a sizable investment in BrainCorp. While some progress in speech processing and image recognition will be made, it will not be sufficient to justify lofty valuations of recent funding events. While my background is in fact in AI, I worked for last few years closely with the preeminent neural scientist Walter Freeman at UC Berkeley on a new kind of wearable personal assistant, one based not on AI but on neural science. During this time, I came to the conclusion that symbol-based computing technologies, including point-to-point “deep” neural networks (not neural science) can not possibly deliver on claims made by many of these well funded AI labs and startups. Here are just three of the reasons: Every single innovation in evolution of vertebrate brains was due to advances in organism locomotion, and none of the new formations indicate the emergence of symbol processing in cortex. Human intelligence is a product of resonating, coupled electric fields produced by massive population of neurons, synapses and ion channels of cortex resulting in dynamic, AM modulated waves in gamma and beta range, not static point-to-point neural networks. Human memories are formed in hippocampus via “phase precession” of theta waves which transform time events into spatial domain without use of symbols like time stamps. Each of the above three empirical findings invalidates AI’s symbolic, computation approach. I could provide more but it is hard to fight prevalent cultural myths perpetuated by mass media. Movies are a good example. At the beginning of the movie Transcendence, Johnny Depp’s character, an AI researcher (from Berkeley) makes the bold claim that “just one AI will be smarter than the entire population of humans that ever lived on earth.” By my calculation this estimate is incorrect today by almost 20 orders of magnitude — it will take more than a few years to bridge this gap. Which brings me back to the FLI letter. While individual investors have every right to lose their assets, the problem gets much more complicated when government regulators are involved. Here are the the main claims of the letter I have a problem with (quotes from the letter in italics): Statements like: “There is a broad consensus that AI research is progressing steadily,” even “progressing dramatically” (Google Brain signatories on FLI web site), are just not true. In the last 50 years there has been very little AI progress (more stasis like than “steady”) and not a single major AI based breakthrough commercial product, unless you count iPhone’s infamous Siri. In short, despite the overwhelming media push, AI simply does not work. “AI systems must do what we want them to do” begs the question of who is “we?” There are 92 references included in this letter, all of them from CS, AI and political scientists, there are many references to approaching, civilization threatening “singularity,” several references to possibilities for “mind uploading,” but not a single reference from a biologist or a neural scientist. To call such an approach to study of intellect “interdisciplinary” is just not credible. “Identify research directions that can maximize societal benefits” is outright chilling. Again, who decides whether research is “socially desirable?” “AI super-intelligence will not act with human wishes and will threaten the humanity” is just a cover for justification of the attempted power grab of AI group over the competing approaches to study of intellect. Why should government regulators support technology that has failed to deliver on its promises repeatedly for 50 years? Newly emerging branches of neural science, which made major breakthroughs in recent years, are of much greater promise, in many cases exposing glaring weaknesses of AI approach. So it is precisely these groups which will suffer if AI is allowed to “regulate” the direction of future research of intellect, whether human or “artificial.” Neural scientists study actual brains with imaging techniques such as fMRI, EEG, ECOG, etc and then postulate predictions about their structure and function from the empirical data they gathered. The more neural research progresses, the clearer it becomes that brain is vastly more complex than we thought just a few decades ago. AI researchers, on the other hand, start with the a priori assumption that the brain is quite simple, really just a carbon version of a Von Neumann CPU. As Google Brain AI researcher and FLI letter signatory, Illya Sutskever, recently told me, “[The] brain absolutely is just a CPU and further study of brain would be a waste of my time.” This is almost word for word repetition of famous statement of Noam Chomsky made decades ago “predicting” the existence of a language “generator” in the brain. FLI letter signatories say: Do not to worry, “we” will allow “good” AI and “identify research directions” in order to maximize societal benefits and eradicate diseases and poverty. I believe that it would be precisely the newly emerging neural science groups which would suffer if AI is allowed to regulate research direction in this field. Why should “evidence” like this allow AI scientists to control what biologists and neural scientists can and cannot do? It is quite possible that signatories’ motives are pure. But at the moment the AI lobby has a near monopoly on forming public opinion and attracting government dollars through the influence of compliant media. Indeed government regulators in this space are all AI researchers, often funding AI startups with taxpayer dollars, and later taking up jobs with the very same companies they funded and were supposed to regulate. And often, when government regulators lead, private VC funds follow in a “Don’t fight the Fed,” sheep-like movement. There is yet another dimension to this story: In addition to threat of upcoming “singularity” FLI letter reference section has many references to “mind uploading.” After life-time of immersion in Von Neumann architectures, from Ray Kurzweill and Peter Thiel on, many silicon valley prodigies are obsesses with the idea of becoming immortal via mind upload into silicon. Threat of death is a powerful emotion indeed, but it belongs in realm of religious thinking rather than “dispassionate and objective science” they profess to advocate. Let me conclude with another movie quote: At the end of movie Beautiful Mind, mathematician John Nash played by Russell Crowe, recovering from mental illness, lectures a group of students in a cafeteria: “Trust mathematics, trust your teachers,” then pauses and adds with a wink: “Just stay away from biologists, do not trust those guys.” Indeed, today’s AI researchers are all children of Rene Descartes, trusting in absolute power of logic and mathematics as they push their religion of Cartesian Dualism on the rest of us. Inadvertently, they tell us all to drink AI Kool-Aid in their “SkyNet is Coming” sermon. I believe that neural science and biology utilizing wearable sensors is already much more fruitful than AI in delivering personal assistants guiding us through daily life, keeping us healthier and stress free, based on better understanding of brain, rather than logic of CPU programing and algorithms of AI focused on weapons and robotics. I hope US press will arise to a defense of scientists rights to continue to perform such free research rather then limiting their research to work on “desirable” results. As famous US journalist once said: “Those who sacrifice liberty for security deserve neither.” Roman Ormandy is the founder of Embody Corp. Go Back to Top. Skip To: Start of Article.

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/politics/2015/01/05/deep-learning-gaze-into-the-web-abyss-and-it-gazes-also-into-you/
### Query Relevance Score: 0.1004919484257698
### Highlights: None

Americans are accustomed to the dominance of Google, Microsoft, and Yahoo as search engines, but on the global stage, a Chinese service called Baidu is now second only to Google in popularity. Baidu hasn’t announced any firm plans to move into the U.S. market yet, but, in addition to the huge market in China, they’ve expanded services to countries such as Egypt, Thailand, and most recently Brazil. They’ve also recently hired away one of Google’s top researchers, Andrew Ng, a specialist in artificial intelligence who has taught courses at Stanford University. As Ng explained in an interview with VentureBeat, Baidu’s claim to fame is their search engine’s reliance on “deep learning” algorithms. In other words, their products learn what users want by analyzing their requests and what they do with the information. To some extent, every search engine and social media platform is trying to learn from its users, producing a certain degree of discomfort among those who feel their Internet tools are spying on them. The deep learning movement wants to take this idea of “living” software further, creating systems that digest enormous amounts of data very quickly, without much human intervention, producing highly customized experiences for each individual user. It is hoped that the benefits from this level of swift, automatic customization, and the fact that it’s all being done by faithful, computerized servants instead of intrusive, human programmers, will overcome consumer unease. One might also suppose that much of the early work is being carried out with customers who are, for better or worse, less nervous about having their online activities monitored than Americans and Europeans. If companies like Baidu can realize the ambitious plans of visionary designers like Ng and bring polished, incredibly useful living software perfected overseas to American audiences, consumer resistance could be minimal. As the old saying goes, nothing succeeds like success. The big American tech companies are also interested in developing this kind of technology, and while power players like Google are certainly swimming in resources, it doesn’t take much reading between the lines to get the idea that Ng left Google because they’ve become too hidebound and bureaucratic. Ng talks about receiving needed resources much faster than his unit at Google provided, getting things done without sitting through tedious committee meetings. He can frolick through Chinese markets with much faster birth-death cycles for online products than is typical in the West, and he has a field day hiring away other top artificial intelligence researchers to join his team. To get an idea of what this deep learning software would be like, the VentureBeat article on Ng references the recent movie “Her,” a near-future science fiction film in which Scarlett Johansson voiced an artificially intelligent digital assistant whose tireless efforts to relate with her user, and enhance the quality of his life, blossomed into an actual romance. (Not to spoil anything for those who have not seen the film, but hopefully deep learning researchers are pondering the ultimate resolution of that romance at great length.) The key feature of these next generation systems over existing smart search engines and voice-activated smartphones is the causal grace of their relationship with users. Computer systems have grown steadily easier to use with each passing decade, long ago reaching the point of widespread consumer acceptance by growing friendly enough for average, non-techie folks to use reflexively on a daily basis. One indicator of how user-friendly consumer systems have become is that no one really talks about “user friendliness” any more. It was the sizzling-hot buzz phrase of the computer industry not very long ago, but it is now well-understood that if an application isn’t simple enough for most people to figure out with a casual glance and a few curious mouse clicks, it’s not going to break through to a mass audience. Cryptic text-based systems have given way to graphical user interfaces that didn’t really work—they tended to cripple the machines they were running on. Eventually graphical interfaces were perfected, and now they’re ubiquitous, running on handheld devices with incredibly convenient touch screens. Voice command is the next step, but even the most gee-whiz voice apps today, like Apple’s famed Siri, are just a shadow of what they could be. Greater accuracy is the key to making smart search engines and voice applications work, and that’s what Baidu hired Andrew Ng to work on. At the moment, the ability of online systems to adjust themselves automatically to suit user preferences is fairly crude. Much of the learning is based on what users request of a particular system… but people don’t always know what they really want. A deep learning system would study what they actually do with the data they request, build a network of preferences from many different data sources, consider the preferences of similar users, and learn to interpret the personal subtleties of spoken language. The personal assistant envisioned in “Her” displays these qualities—she can anticipate what her user wants based on very vague requests, she adjusts her behavior to meet his demonstrated preferences, she understands the nuances of spoken conversation (there are some cleverly written early interactions where she asks questions of her user to figure out when he’s being sarcastic, what he really means when he uses certain figures of speech, and so forth.) The ability to accurately pull meaningful data from the random noise of human behavior is crucial to making such features work. Living human beings don’t actually handle these tasks with a very high degree of accuracy, unless they know each other very well. The speed and power of artificially intelligent computer systems, combined with the endless patience of machine intelligence, could make them much better at becoming everyone’s close personal friend. With these capabilities perfected, computer systems would cross the final user-friendliness bridge and begin doing more than half of the work to maintain a relationship with humans. As it stands, even the most easy-to-use interfaces require us to learn how the system thinks—we have to figure out where the menus are, perhaps learn a few shortcut keys, learn the peculiarities of each system’s tools for such routine tasks as file uploads, and learn how we can configure the interface to suit our personal tastes. One reason we tend to think of modern systems as more user-friendly is that the typical user is far more comfortable with learning his way around an interface than his father was; an online generation is coming of age, and it’s more adventuresome and patient than the eighties and nineties executives who howled in frustration at the inscrutable but indispensable machines on their desks. When computer systems are able to shoulder most of the interaction burden, and we can use them as casually as we would request help from a trusted human assistant, the next computer revolution will be at hand. Artificial intelligence is still an argument—there are those who believe it will never be anything but an illusion, that talk of “neural networks” is just slick marketing for really fast computers. We currently have search engines that work extremely well; both user experiences and uninvited advertising have been tailored to individual preferences. Such tasks as piloting an automobile, routinely accomplished by millions of humans without extensive training or individual genius, remain beyond the capability of the smartest computer system; the ability of an incredibly complicated machine to briefly fool a panel of human judges into thinking they are holding a conversation with a human child is celebrated as a landmark A.I. triumph. The practical application of such technology remains elusive, and yesterday’s promised miracles remain science fiction. But then again, many electronic conveniences we currently take for granted were science fiction just twenty years ago. It’s interesting to watch where high-rolling tech companies are placing their bets, and they all seem convinced that deep learning systems are worth investing sizable sums of money in. The problem for American investors is that it doesn’t seem like our domestic giants are nimble and adventurous enough to keep up with the work being done by the big Chinese companies. At least, that’s how Andrew Ng placed his bets.

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/local/2015/01/23/apple-friends-hammered-for-employment-fraud/
### Query Relevance Score: 0.03825127333402634
### Highlights: None

Apple, Google, Adobe and Intel, whose politics are often to the far left, have agreed to pay $415 million to resolve a class-action lawsuit alleging they conspired as an illegal cartel to suppress tech workers’ wages through secret “non-poaching” agreements involving 64,000 employees. The new settlement offer follows the stunning August 8, 2014 rejection by federal Judge Lucy Koh of a $324.5 million settlement by the companies as “inadequate” after the Court “admitted” into evidence the plaintiff’s expert testimony that the direct wage losses by workers were the result of the companies’ actions. The evidence gathered during the discovery phase of the class-action lawsuit exposed Apple and Google emails that cast their top executives as sleazy and corrupt. Apple’s late CEO Steve Jobs was depicted as the conniving ringleader of a scheme designed to minimize the chances that the top computer programmers and other talented employees would defect to other technology companies. The lawsuit contended the secret “no-poaching” agreements orchestrated by Jobs suppressed the employees’ wages, though many of whom were already making more than $100,000 annually. Former Google CEO Eric Schmidt, who was on Apple’s Board at the time that the alleged collusion began, sometimes took drastic actions to make sure his company didn’t cross Jobs. In 2007, Schmidt, in a bid to keep Jobs happy, fired a Google recruiter for contacting an Apple engineer, according to internal emails. The suit against Apple and Google followed the capitulation of the defendants in the “High-Tech Employee Antitrust Litigation” filed in 2006 that has served as the roadmap for numerous other lawsuits regarding allegedly “injured tech workers.” Lucas Films, Pixar, and Intuit were alleged to have defrauded 147 tech workers as an “impacted class” in an “overarching conspiracy.” Because the court ruled the entertainment companies’ fraud was built on a series of six written agreements not to solicit each other’s employees, the case seemed to qualify for treble damages. The settlement last year provided that each “injured” employee receive $136,054, and their lawyers’ fees were paid by the defendants. As reported by Breitbart News on August 11, the evidentiary rulings in High-Tech Employee Antitrust Litigation meant the class-action lawsuits would mushroom into a “broad conspiracy that could ensnare hundreds of large and small tech companies across America in a blatant fraud led by Apple to cheat tech workers out of many billions in wages.” Apple, Google, Adobe and Intel claim their alleged collusion was stopped after the U.S. Justice Department opened an investigation that culminated with an antitrust complaint being filed against Apple, Intel, Google and Adobe in 2010. That Justice Department case was settled without the companies admitting any guilt or paying any fines. The 64,000 member plaintiff class each sought $15,000 in actual monetary loss and $30,000 in anti-trust punitive damages for suppressing wages under the Sherman Antitrust Act and Clayton Antitrust Act. But they settled for about $5,200 apiece. Because as many as 200,000 tech workers are alleged to have been defrauded by “no poaching” agreements, there are a number of similar tech worker lawsuits moving toward trial. If discovery in any of those lawsuits reveal that Apple, Intel, Google or Adobe were trying to suppress wages after 2010, Judge Koh will haul the these industry leaders back into court and hammer them even harder.

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/national-security/2015/01/04/atheists-still-dreaming-that-science-will-do-away-with-god/
### Query Relevance Score: 0.03775789588689804
### Highlights: None

A recent piece by liberal pundit Paul Rosenberg says that “God is on the Ropes,” thanks to a “brilliant new science” that has the “Christian right terrified.” Over the centuries God has shown himself remarkably resilient to every attempt to kill him off, and the odds are that He will survive this latest effort, but that doesn’t keep Rosenberg from getting teary-eyed over the thought. The God-slaying discovery touted by Rosenberg refers to a theory, based in thermodynamics, which proposes that the emergence of life was not accidental, but necessary. The theory has been advanced by a young MIT professor named Jeremy England. According to England, “under certain conditions, matter inexorably acquires the key physical attribute associated with life.” In other words, life itself would be a product of evolution from simpler, non-living systems. Whereas near unanimity reigns in the scientific community regarding the origins of the universe, science has had much less to offer concerning the origins of life. Natural selection, of course, presupposes life and seeks only to explain the survival and gradual change of reproducing species. It does nothing to explain the genesis of life, or how it came about given its statistical improbability. Francis S. Collins, the director of the National Institutes of Health (NIH) and former leader of the monumental Human Genome Project, wrote that “no current hypothesis comes close to explaining how in the space of a mere 150 million years, the prebiotic environment that existed on earth gave rise to life.” Sir Fred Hoyle, the celebrated English physicist and cosmologist, thought that the appearance of life on earth was all but impossible, from a statistical standpoint. In his 1981 book Evolution from Space (with Chandra Wickramasinghe), Hoyle calculated that the chance of obtaining the required set of enzymes for even the simplest living cell was one in 1040,000 (one followed by 40,000 zeroes). He came up with the fanciful image that the probability of life originating on earth is no greater than the chance that a hurricane, sweeping through a scrapyard, would have the luck to assemble a Boeing 747. So if true, England’s theory would represent an incredibly important breakthrough for the scientific community. The fact that it has received only modest attention suggests that it may yet be far from verifiable, but sounds interesting nonetheless. But the real question becomes, why would this be a problem for believers, let alone for God? Why should Christians be afraid of the verifiable findings of science? Many on the Left still languish under the illusion that science and faith are irreconcilable adversaries, while most believers have no problem whatsoever with science and welcome its advances as testimony to the power of the human intellect and the intelligibility of creation. As students of history know, the natural sciences grew out of Christian culture. As the sociologist Rodney Stark has so convincingly shown, science was “still-born” in the great civilizations of the ancient world, except in Christian civilization. Why is it, Stark asks, that empirical science and the scientific method did not develop in China (with its sophisticated society), in India (with its philosophical schools), in Arabia (with its advanced mathematics), in Japan (with its dedicated craftsmen and technologies), or even in ancient Greece or Rome? Science flourished in societies where a Christian mindset understood nature to be ordered and intelligible, the work of an intelligent Creator. Far from being an obstacle to science, Christian soil was the necessary humus where science took root. Liberal humbugs like Paul Rosenberg will continue to try to pit science against faith, hoping against hope that they will be able to put “God on the ropes.” Based on the historical record, I wouldn’t hold my breath. Follow Thomas D. Williams on Twitter @tdwilliamsrome

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/europe/2015/01/21/civil-war-brewing-for-the-cultural-left/
### Query Relevance Score: 0.03755449876189232
### Highlights: None

It has often been remarked that the right won the economic arguments of the twentieth century, while the left won the culture war. Although Thatcher and Reagan succeeded in their quest to overturn the postwar economic consensus and undermine the USSR, the left consistently triumphed over social conservatives in political debates on society and culture. Throughout the 1980s and the 1990s, the left successfully positioned itself as the guardian of liberty and reason against a dogmatic and authoritarian “moral majority”. Moderates and liberals could not understand why the right wanted to deny gay people the right to marry, or women the right to an abortion. Nor could they understand the conservative quest to pull the theory of evolution from primary schools, or the regular campaigns by conservative moral crusaders against filth, blasphemy and even Satanism (1, 2) in popular culture. Against such opponents, it was relatively easy for the left to position itself as the defenders of academic inquiry, artistic expression and personal freedom. But the sands are beginning to shift. The coalition of moderate liberals, sceptical intellectuals, and radical progressives that once stood together against the conservative “moral majority” is beginning to fracture. In the absence of a compelling external opponent, the internal tensions of this coalition are becoming more visible. While it is too soon to say if the revolution is about to consume itself, a number of serious divisions have emerged on the cultural left. And they are becoming increasingly bitter. Religion Richard Dawkins, Sam Harris, Daniel Dennett and Christopher Hitchens were once known as the “Four Horsemen” of New Atheism. For a long while, there was nothing more amusing to a young liberal than watching one of them debate against a creationist, or someone who objected to abortion or gay marriage on religious grounds. Dawkins, for a while, was the darling of the British media. Then things started to sour. Christopher Hitchens, in his full-throated defences of the second Iraq war, was the first to lose left-wing support. Notoriously, Feminist Frequency producer Jonathan McIntosh celebrated Hitchens’ death, saying he was a “despicable, warmongering, hateful human being. Good riddance.” (To put that in perspective, McIntosh had just a few months earlier refused to celebrate the death of Osama Bin Laden.) Dawkins, who recently discovered the joys of deliberately offending people on Twitter, has become an even greater figure of hate for progressives. This is probably due to his indiscriminate rationalism: he is just as willing to poke holes in theories of post-modern feminism as he is to attack religion. And when he does attack religion, he insists that Islam is probably the worst one out there. He has become persona non grata in progressive circles as a result. 2014 saw atheists and progressives embroiled in what looked like an all-out war. Ayaan Hirsi Ali, a female genital mutilation survivor and one of the fiercest critics of Islam in the atheist movement, was disinvited from a planned speaking engagement at Brandeis University for her criticism of Islam, and was stripped of her honorary degree. Salon.com immediately applauded the decision. Students at UC Berkeley attempted to do the same to Bill Maher over his alleged islamophobia, but were stopped by the college administration. Sam Harris, another of the “four horsemen”, felt compelled to engage in a three-hour debate with progressive commentator Cenk Uygur after enduring a wave of hatchet-jobs from media progressives for his own comments on Islam. Progressives may be overwhelmingly atheist, but there is only so much heresy they can stand. One of their core beliefs is that you do not “punch down”–that is, attack vulnerable or marginalised communities. Islam, despite being the dominant religion of dozens of nation-states, is said by progressives to fall into this category. But many atheists don’t buy it. And so they continue, with creationists to the right and progressives to the left, blaspheming against the beliefs of both. As Christianity declines and Islam grows, it is progressives, constantly impeding criticism of the latter, who may prove to be a bigger thorn in the side of atheism than conservatives ever were. Due process During the Bush administration, liberals eagerly positioned themselves as champions of the rights of the accused: specifically, those accused of plotting terrorism. For the left, Guantanamo Bay became a byword for a new authoritarian lawlessness, in which jury trials were a thing of the past and punishment was meted out on suspicion alone. These days, however, defenders of due process are more likely to be at loggerheads with radical progressives than Bush-era neocons. Nowadays, it is progressives, not conservatives, who championed the use of campus tribunals to deal with sexual assault on US campuses. These tribunals, conducted by untrained faculty members, with no requirement for defendants to have access to legal representation, have attracted a growing tide of criticism, as well as a number of lawsuits. In the UK, progressives have also lent their support to prosecutors’ efforts to interfere in the jury system, most recently by calling on judges to tackle the “unconscious biases” of juries. While progressives on Twitter and the blogosphere rejoice in the assault on due process, others on the left are not so enthusiastic. Last summer, 28 Harvard lawyers, most of them liberal, came out against the university’s attempts to hand control of sexual assault cases to a single “Title IX compliance officer.” In the aftermath of Rolling Stone’s disastrous story on a hoax rape claim at the University of Virginia, even more liberals–including Slate’s Emily Yoffe and former Salon writer Richar d Bradley–began to speak out against the new atmosphere of vigilantism and automatic credulity sometimes referred to as “listen and believe.” Meanwhile, progressives continue to call for “affirmative consent” laws that would redefine sexual assault to include any form of sexual contact that is not explicitly, verbally consented to. Ezra Klein, the arch-progressive editor of Vox, accepted that such laws are “terrible” and would convict people for “genuinely ambiguous situations” – but, incredibly, also said that this was OK. In order to prevent sexual assault, wrote Klein, it was necessary for the law to “create a world in which men are afraid.” Bad laws that could be enforced arbitrarily were, in his view, a great way of accomplishing that. Other liberals understandably recoiled in horror at this line of reasoning. As doubts about the data behind the “campus rape epidemic” grow, battle lines are being drawn between progressives like Klein and liberals like Yoffe, Bradley, and the Harvard lawyers. The crusade to change culture – the very heart of the progressive mission – is on a collision course with due process, and perhaps liberalism itself. Censorship The blood was scarcely cold on the corpses of the murdered Charlie Hebdo cartoonists before progressives began to call them racist. Among them was Jonathan McIntosh, our Hitchens-hating friend from the previous section, leading some bloggers to dub him “Jihad Jonathan.” But he wasn’t alone. Around the world, it soon became apparent that a number of news organisations, including Sky, CNN, the New York Times and the Daily Telegraph were refusing to run the cartoons. on the grounds of religious offence. It is understandable for these outlets to be afraid of people with guns. What is less understandable to many liberals is being afraid of people of people with petitions. The left-libertarian journal Spiked, which has become a focal point for people dissatisfied with the mainstream left, recently wrote a biting piece of satire outlining how a mix of student boycotts and Change.org petitions might have ended Charlie Hebdo altogether, had it been published in the UK. The article was shared over four thousand times. But it’s not just Brendan O’Neill who’s noticed the rise of the “Stepford Students”. His longtime opponent, the liberal columnist Nick Cohen has been warning about the very same thing. Chris Rock, also a liberal, recently revealed that he no longer performs comedy for student audiences, arguing that they were too “conservative” in the way they handled offensive content. His use of the word “conservative” is telling. For decades, it was social conservatives who put pressure on private institutions to censor material that offended them. Cinemas that screened Monty Python movies were boycotted, panics were stoked about the “satanism” of Dungeons & Dragons, and born-again Christians led campaigns against violent videogames. Today, however, it is progressives who are not just standing up for the right of private censorship, but also actively demand it. It is progressives, not Christian conservatives, who now lead campaigns against sex and violence in the media. And it was progressive students, not middle-aged moral crusaders, who banned a pop song on over 20 university campuses. The #GamerGate uprising was in part a reaction to this culture of censorship, and they have already helped protect two video games – the controversial shooter Hatred, and the farming simulator Seedscape – from attempted boycotts. But despite rabid opposition from the left, #GamerGate – like Nick Cohen, like Bill Maher, and like Richard Dawkins – continues to identify with liberalism. The pro-censorship left and the anti-censorship left know they will never convince each other. Both sides have begun to dig trenches. Academia I have left the most surprising arena of left-vs-left conflict for last. Academia has long been assumed by those on the right to be an impenetrable fortress of progressive dogma. But times are changing. Most intriguing of all is that it is in the social sciences where the most change is taking place. I remember my own surprise, as a young undergraduate, to find an entire module on evolutionary psychology in the social science reading list. I was also surprised to see professors excitedly recommending Daniel Kahneman’s Thinking, Fast and Slow to students. Most surprising of all was when my sociology tutor – a leading egalitarian theorist – informed our class that he thought social construction theory was “mostly wish-wash.” Those who are unfamiliar with the nature-vs-nurture debate will probably not understand why such a statement is controversial. Cognitive and genetic scientists, and evolutionary theorists, have long been viewed with suspicion by sociologists. After all, one of the chief projects of cognitive and evolutionary scientists in the past two decades has been the dismantling of the standard social science model, the theoretical framework that looks to external influences (nurture) to explain human behaviour, as opposed to genes or other innate factors (nature). If my university is anything to go by, however, even social scientists themselves are beginning to see flaws in the old model. This is bad news for progressives. The idea that human minds are infinitely malleable, and that the human behaviour can be altered simply by changing the social environment, underpins almost every progressive campaign – from No More Page Three to non-selective schooling. This is no accident: anyone who wishes to radically change the world must, on some level, believe that human nature can be altered. Frightened by the growing weakness of their flagship theories, progressives on campus have begun to lash out. One of the biggest controversies was in 2005, when then-President of Harvard University, Larry Summers, was faced with a motion of no confidence after suggesting that innate differences between the genders should be a line of inquiry when analysing the gender pay gap. The motion passed, and it left serious scars in the academic community. “Good grief, shouldn’t everything be within the pale of legitimate academic discourse?” asked Steven Pinker shortly after the controversy. “That’s the difference between a university and a madrasa.” There have been no comparable controversies since then, but a stream of outrage continues to follow the work of Pinker, Simon Baron-Cohen, Robert Plomin, Nicholas Wade and anyone else who investigates the idea of innate differences between persons and groups. It doesn’t matter how much they stress their commitment to liberalism and egalitarianism (which they do, frequently), nothing can calm their opponents. Academics aren’t usually minded to make snap political judgments on a whim. But there is a growing perception that some on the progressive left are just as committed to their dogmas as creationists are to theirs. With the standard social science model increasingly looking like a relic of the 1960s, the fierceness with which some progressives continue to cling to it will determine the fierceness of future divisions in academia, and, by extension, the left. The future of the culture wars As 21st-century progressives begin to embrace many of the tactics, arguments, and moral panics employed by 20th-century conservatives, the old distinctions in the culture wars begin to lose their relevance. In a number of arenas, the cultural left is ignoring conservatives and has begun to fight itself. The question for the right is: what to do? There is no doubt room for common ground with liberal atheists who want to criticize Islam, with liberal lawyers who want to protect due process and with content producers fighting the new, petition-led censorship. Opposition to political correctness and a respect for individual rights have always been strong traditions on the right. On the other hand, appealing to liberal, atheist, Grand Theft Auto V fans without losing the support of social conservatives could prove difficult. Nevertheless, it increasingly appears that cultural politics, once the great strength of the left-wing movement, is rapidly turning into its Achilles heel. Once a source of unity, it has turned into perhaps the primary source of division. With moderate liberals and radical progressives sharpening their weapons on a number of fronts, a battle for the soul of the left is about to begin.

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/politics/2015/01/16/facebook-board-member-marc-andreessen-illegal-immigration-best-thing-for-america/
### Query Relevance Score: 0.03702375665307045
### Highlights: None

Marc Andreessen, the Facebook board member who condemned Sen. Jeff Sessions (R-AL) for standing up for American workers, believes that illegal immigration is actually the “best thing” that is happening to America. In an interview with the Financial Times, Andreessen said that “birth rates are going to drop” in wealthier societies and claimed that America’s economic growth would be slower if not for “illegal immigration.” “It’s the best thing that can possibly be happening to us, and I find it ironic; nobody wants to talk about that,” he reportedly said of illegal immigration. Last year, Andreessen took offense when Sessions went to the Senate floor to denounce the so-called “Masters of the Universe” who view borders and sovereignty as nuisances. Andreessen actually called Sessions an “odious hack” who was “clinically insane” and accused him of “outright slander” for suggesting that Facebook co-founder Mark Zuckerberg hire more American workers instead of relentlessly pushing Congress for more H-1B guest-worker visas at a time when America does not have a demonstrated shortage of American high-tech workers. The tech industry and its financiers like Andreessen want more guest workers because they would lower wages in the industry.

---

## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/15/elon-musk-donates-10m-to-make-sure-ai-doesnt-go-the-way-of-skynet/
### Query Relevance Score: 0.10107767581939697
### Highlights: None

Tesla and SpaceX chief executive Elon Musk has gone on record before proclaiming the potential risks of artificial intelligence, and now he’s putting his money where his mouth is. The intrepid inventor and entrepreneur announced a donation of $10 million to help fund research to “keep AI beneficial” to humanity today. The funds go to the Future of Life Institute (FLI), an organization run by volunteers dedicated to research aimed at “mitigate[ing] existential risks facing humanity,” and specifically those related to our ongoing progress towards AI that can approach human capabilities. Musk offered a statement along with the announcement, noting that he is in agreement with a long list of prominent AI researchers who have signed an open letter calling for exactly the type of work the FLI is doing. “Here are all these leading AI researchers saying that AI safety is important”, said Elon Musk in the statement, referring to this letter originally put forward by FLI founder and MIT professor Max Tegmark. “I agree with them, so I’m today committing $10M to support research aimed at keeping AI beneficial for humanity.” The FLI will distribute the funds donated by Musk to relevant research products, and some of it will make its way to participants in an open grants competition opening for applications on Monday January 19 to help select projects that look at AI-specialist researchers in particular, and then at research in ethics, law, economics and other areas which feature AI-related components. While the threat of an evil AI has seemed like a concern primarily for science fiction authors for most of modern history, it’s undeniable that questions about how ethics and law apply to computers with intelligence on par with human counterparts is becoming something we likely do indeed start to begin preparing for. Musk’s interest in talking about, and funding thinking around this side of the equation, at the same time as he continues to espouse technical progress, is refreshing, and it’ll be interesting to see what kind of results come from research in this area.

---

## Article published: 01/2015
### Source URL: https://www.nytimes.com/2015/01/11/magazine/death-by-robot.html?referrer=
### Query Relevance Score: 0.11494594812393188
### Highlights: None

The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you’d like. nytimes.com/subscription

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/science/want-to-call-aliens-keep-it-short-and-simple-scientists-say.print
### Query Relevance Score: 0.036552585661411285
### Highlights: None

By , Published January 13, 2015 If extraterrestrials ever phoned us, they would be more likely to send narrowly directed bursts than constantly blaring signals — more text-messaging than novel-writing, some scientists now suggest. Such a strategy also would be more practical for anyone on Earth to reach out to aliens — if one had at least a billion dollars to spend. How cheap is talk? For 50 years, people have watched the skies with radio telescopes, hoping to detect signals of intelligent alien life. So far, however, the efforts of SETI (the search for extraterrestrial intelligence ) have proved fruitless. Now some scientists suggest the kind of signals SETI has hoped to find for decades might not be what aliens would broadcast. In a way, the form of communication might come down to cost. "Our grandfather used to say, 'Talk is cheap, but whiskey costs money,'" said researcher Gregory Benford, an astrophysicist at the University of California, Irvine, and an award-winning science fiction novelist. "Whatever the life form, evolution selects for economy of resources. Broadcasting is expensive, and transmitting signals across light-years would require considerable resources." Assuming that aliens would strive to optimize costs, limit waste and make their signaling technology more efficient, Benford and his twin, James — a fellow physicist who specializes in high-powered microwave technology — suggest the signals would not be steadily blasted out in all directions. Extraterrestrials would be more likely to send narrow "searchlight" beams delivered in pulses. "This approach is more like Twitter and less like 'War and Peace,'" said James Benford, founder and president of Microwave Sciences Inc., in Lafayette, Calif. The Benford twins, along with James' son Dominic, a NASA scientist, detailed their findings in two studies appearing in the June issue of the journal Astrobiology. The Benfords suggest a continuous signal blared at thousands of stars would simply cost too much energy. They say aliens might use short bursts — say, anywhere from a second to an hour long — and point these signals in narrow beams at one star and then another in a cycle involving up to thousands of stars that repeats over days or years. For civilizations that constantly watch the skies, the bursts would convey enough data to be recognized as undeniably artificial. As observant civilizations concentrated on this simple beacon, other beacons could broadcast more complex data at lower power (assuming the aliens were still pursuing a frugal strategy). The Benfords suggested looking at a broad range of radio signals in the 1- to 10-gigahertz range, where the travel of light is relatively unimpeded by interstellar matter. Currently SETI is focused on just the 1-to-2 gigahertz region , where components of water such as hydrogen and hydroxyl (a compound of hydrogen and oxygen) emit radio signals. "The idea is actually based mostly on how all the electronics of the 1960s, when SETI was first starting out, only operated in the range of a few gigahertz," Gregory Benford explained. "Now they operate in a range of up to 100 or 200 gigahertz, so it's a reason to revisit our assumptions. Looking to 10 gigahertz makes sense, since it's cheaper by a factor of 10 to build a transmitter at 10 gigahertz than at 1 gigahertz." Looking inward The Benfords also said that instead of gazing at stars within 500 or so light-years, as most SETI efforts have done for decades, observers should point more toward our galaxy's center to distances up to 1,000 light-years from Earth, where 90 percent of the galaxy's stars are clustered. "The stars there are a billion years older than our sun, which suggests a greater possibility of contact with an advanced civilization than does pointing SETI receivers outward to the newer and less crowded edge of our galaxy," Gregory Benford said. Although the galactic center is home to many bursting stars whose explosions are expected to sterilize the space around them, "the vast bulk of stars in the 28,000 light-years between Earth and the galactic center are not in sterilizing environments," Benford told SPACE.com. "For an analogy, you wouldn't want to hang out in Times Square all the time, but if you live in New Jersey it's obvious that Manhattan is the place to look for the action." "Will searching for distant messages work? Is there intelligent life out there? The SETI effort is worth continuing, but our common-sense beacons approach seems more likely to answer those questions," Benford said. Have we seen a beacon? One possibility of an extraterrestrial beacon is a puzzling transient radio source some 26,000 light-years from Earth that was discovered in 2002 in the direction of the galactic center. It sends out radio waves in bursts lasting up to 10 minutes in a 77-minute cycle. Scientists have suggested the source, labeled GCRT J17445-3009, is a flare star, an extrasolar planet, a pulsar or a brown dwarf, but none of these explanations fits well, the Benfords said. Based on the burst length and the short cycle, the Benfords doubt GCRT J17445-3009 is a beacon aimed at possible civilizations among a large crowd of stars. Still, if GCRT J17445-3009 is artificial in nature, it could be a signal that aliens pointed just at us, having detected signs of life from our planet. If that is the case, the Benfords said, we may want to pay closer attention to its signals to look for hidden details. On the other hand, GCRT J17445-3009 could be one link in an interstellar communications network. If so, it would make sense to look in the opposite direction, to see if another beam was communicating at it. "We studied GCRT not because we really think it's a beacon, but because it's an interesting way to look at similar bursting sources," Gregory Benford said. "There's the famous 'Wow' signal from 1977, for instance, that involved an enormous amount of power, and that there's still no good explanation for." Calling aliens Instead of just searching for extraterrestrial intelligence, the Benfords also considered messaging to extraterrestrial intelligences , or METI. They calculated that a galactic-scale beacon, with an antenna roughly a half-mile (0.9 km) wide with a range of a little more than 1,000 light-years, could be built for $1.3 billion. It would cost $200 million annually to operate. To work economically, it would use only narrow, high-power microwave beams and 35-second bursts aimed at each target star. "Of course, if you want to send a message, first you have to find a billionaire for this," Gregory Benford told SPACE.com. He noted he has spoken with a number of billionaires, including former Microsoft chief technology officer Paul Allen and Amazon.com founder Jeff Bezos, "and everyone has the same remark — that they would rather spend a billion dollars a different way." Copyright © 2010 Space.com. All Rights Reserved. This material may not be published, broadcast, rewritten or redistributed. URL https://www.foxnews.com/science/want-to-call-aliens-keep-it-short-and-simple-scientists-say

---

## Article published: 01/2015
### Source URL: https://www.wired.com/2015/01/ai-arrived-really-worries-worlds-brightest-minds/
### Query Relevance Score: 0.13268576562404633
### Highlights: None

On the first Sunday afternoon of 2015, Elon Musk took to the stage at a closed-door conference at a Puerto Rican resort to discuss an intelligence explosion. This slightly scary theoretical term refers to an uncontrolled hyper-leap in the cognitive ability of AI that Musk and physicist Stephen Hawking worry could one day spell doom for the human race. That someone of Musk's considerable public stature was addressing an AI ethics conference—long the domain of obscure academics—was remarkable. But the conference, with the optimistic title "The Future of AI: Opportunities and Challenges," was an unprecedented meeting of the minds that brought academics like Oxford AI ethicist Nick Bostrom together with industry bigwigs like Skype founder Jaan Tallinn and Google AI expert Shane Legg. Musk and Hawking fret over an AI apocalypse, but there are more immediate threats. In the past five years, advances in artificial intelligence—in particular, within a branch of AI algorithms called deep neural networks—are putting AI-driven products front-and-center in our lives. Google, Facebook, Microsoft and Baidu, to name a few, are hiring artificial intelligence researchers at an unprecedented rate, and putting hundreds of millions of dollars into the race for better algorithms and smarter computers. AI problems that seemed nearly unassailable just a few years ago are now being solved. Deep learning has boosted Android's speech recognition, and given Skype Star Trek-like instant translation capabilities. Google is building self-driving cars, and computer systems that can teach themselves to identify cat videos. Robot dogs can now walk very much like their living counterparts. "Things like computer vision are starting to work; speech recognition is starting to work There's quite a bit of acceleration in the development of AI systems," says Bart Selman, a Cornell professor and AI ethicist who was at the event with Musk. "And that's making it more urgent to look at this issue." Given this rapid clip, Musk and others are calling on those building these products to carefully consider the ethical implications. At the Puerto Rico conference, delegates signed an open letter pledging to conduct AI research for good, while "avoiding potential pitfalls." Musk signed the letter too. "Here are all these leading AI researchers saying that AI safety is important," Musk said yesterday. "I agree with them." Google Gets on Board Nine researchers from DeepMind, the AI company that Google acquired last year, have also signed the letter. The story of how that came about goes back to 2011, however. That's when Jaan Tallinn introduced himself to Demis Hassabis after hearing him give a presentation at an artificial intelligence conference. Hassabis had recently founded the hot AI startup DeepMind, and Tallinn was on a mission. Since founding Skype, he'd become an AI safety evangelist, and he was looking for a convert. The two men started talking about AI and Tallinn soon invested in DeepMind, and last year, Google paid $400 million for the 50-person company. In one stroke, Google owned the largest available talent pool of deep learning experts in the world. Google has kept its DeepMind ambitions under wraps—the company wouldn't make Hassabis available for an interview—but DeepMind is doing the kind of research that could allow a robot or a self-driving car to make better sense of its surroundings. That worries Tallinn, somewhat. In a presentation he gave at the Puerto Rico conference, Tallinn recalled a lunchtime meeting where Hassabis showed how he'd built a machine learning system that could play the classic '80s arcade game Breakout. Not only had the machine mastered the game, it played it a ruthless efficiency that shocked Tallinn. While "the technologist in me marveled at the achievement, the other thought I had was that I was witnessing a toy model of how an AI disaster would begin, a sudden demonstration of an unexpected intellectual capability," Tallinn remembered. Deciding the dos and don'ts of scientific research is the kind of baseline ethical work that molecular biologists did during the 1975 Asilomar Conference on Recombinant DNA, where they agreed on safety standards designed to prevent manmade genetically modified organisms from posing a threat to the public. The Asilomar conference had a much more concrete result than the Puerto Rico AI confab. At the Puerto Rico conference, attendees signed a letter outlining the research priorities for AI—study of AI's economic and legal effects, for example, and the security of AI systems. And yesterday, Elon Musk kicked in $10 million to help pay for this research. These are significant first steps toward keeping robots from ruining the economy or generally running amok. But some companies are already going further. Last year, Canadian roboticists Clearpath Robotics promised not to build autonomous robots for military use. "To the people against killer robots: we support you," Clearpath Robotics CTO Ryan Gariepy wrote on the company's website.

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/politics/2015/01/05/deep-learning-gaze-into-the-web-abyss-and-it-gazes-also-into-you/
### Query Relevance Score: 0.10148071497678757
### Highlights: None

Americans are accustomed to the dominance of Google, Microsoft, and Yahoo as search engines, but on the global stage, a Chinese service called Baidu is now second only to Google in popularity. Baidu hasn’t announced any firm plans to move into the U.S. market yet, but, in addition to the huge market in China, they’ve expanded services to countries such as Egypt, Thailand, and most recently Brazil. They’ve also recently hired away one of Google’s top researchers, Andrew Ng, a specialist in artificial intelligence who has taught courses at Stanford University. As Ng explained in an interview with VentureBeat, Baidu’s claim to fame is their search engine’s reliance on “deep learning” algorithms. In other words, their products learn what users want by analyzing their requests and what they do with the information. To some extent, every search engine and social media platform is trying to learn from its users, producing a certain degree of discomfort among those who feel their Internet tools are spying on them. The deep learning movement wants to take this idea of “living” software further, creating systems that digest enormous amounts of data very quickly, without much human intervention, producing highly customized experiences for each individual user. It is hoped that the benefits from this level of swift, automatic customization, and the fact that it’s all being done by faithful, computerized servants instead of intrusive, human programmers, will overcome consumer unease. One might also suppose that much of the early work is being carried out with customers who are, for better or worse, less nervous about having their online activities monitored than Americans and Europeans. If companies like Baidu can realize the ambitious plans of visionary designers like Ng and bring polished, incredibly useful living software perfected overseas to American audiences, consumer resistance could be minimal. As the old saying goes, nothing succeeds like success. The big American tech companies are also interested in developing this kind of technology, and while power players like Google are certainly swimming in resources, it doesn’t take much reading between the lines to get the idea that Ng left Google because they’ve become too hidebound and bureaucratic. Ng talks about receiving needed resources much faster than his unit at Google provided, getting things done without sitting through tedious committee meetings. He can frolick through Chinese markets with much faster birth-death cycles for online products than is typical in the West, and he has a field day hiring away other top artificial intelligence researchers to join his team. To get an idea of what this deep learning software would be like, the VentureBeat article on Ng references the recent movie “Her,” a near-future science fiction film in which Scarlett Johansson voiced an artificially intelligent digital assistant whose tireless efforts to relate with her user, and enhance the quality of his life, blossomed into an actual romance. (Not to spoil anything for those who have not seen the film, but hopefully deep learning researchers are pondering the ultimate resolution of that romance at great length.) The key feature of these next generation systems over existing smart search engines and voice-activated smartphones is the causal grace of their relationship with users. Computer systems have grown steadily easier to use with each passing decade, long ago reaching the point of widespread consumer acceptance by growing friendly enough for average, non-techie folks to use reflexively on a daily basis. One indicator of how user-friendly consumer systems have become is that no one really talks about “user friendliness” any more. It was the sizzling-hot buzz phrase of the computer industry not very long ago, but it is now well-understood that if an application isn’t simple enough for most people to figure out with a casual glance and a few curious mouse clicks, it’s not going to break through to a mass audience. Cryptic text-based systems have given way to graphical user interfaces that didn’t really work—they tended to cripple the machines they were running on. Eventually graphical interfaces were perfected, and now they’re ubiquitous, running on handheld devices with incredibly convenient touch screens. Voice command is the next step, but even the most gee-whiz voice apps today, like Apple’s famed Siri, are just a shadow of what they could be. Greater accuracy is the key to making smart search engines and voice applications work, and that’s what Baidu hired Andrew Ng to work on. At the moment, the ability of online systems to adjust themselves automatically to suit user preferences is fairly crude. Much of the learning is based on what users request of a particular system… but people don’t always know what they really want. A deep learning system would study what they actually do with the data they request, build a network of preferences from many different data sources, consider the preferences of similar users, and learn to interpret the personal subtleties of spoken language. The personal assistant envisioned in “Her” displays these qualities—she can anticipate what her user wants based on very vague requests, she adjusts her behavior to meet his demonstrated preferences, she understands the nuances of spoken conversation (there are some cleverly written early interactions where she asks questions of her user to figure out when he’s being sarcastic, what he really means when he uses certain figures of speech, and so forth.) The ability to accurately pull meaningful data from the random noise of human behavior is crucial to making such features work. Living human beings don’t actually handle these tasks with a very high degree of accuracy, unless they know each other very well. The speed and power of artificially intelligent computer systems, combined with the endless patience of machine intelligence, could make them much better at becoming everyone’s close personal friend. With these capabilities perfected, computer systems would cross the final user-friendliness bridge and begin doing more than half of the work to maintain a relationship with humans. As it stands, even the most easy-to-use interfaces require us to learn how the system thinks—we have to figure out where the menus are, perhaps learn a few shortcut keys, learn the peculiarities of each system’s tools for such routine tasks as file uploads, and learn how we can configure the interface to suit our personal tastes. One reason we tend to think of modern systems as more user-friendly is that the typical user is far more comfortable with learning his way around an interface than his father was; an online generation is coming of age, and it’s more adventuresome and patient than the eighties and nineties executives who howled in frustration at the inscrutable but indispensable machines on their desks. When computer systems are able to shoulder most of the interaction burden, and we can use them as casually as we would request help from a trusted human assistant, the next computer revolution will be at hand. Artificial intelligence is still an argument—there are those who believe it will never be anything but an illusion, that talk of “neural networks” is just slick marketing for really fast computers. We currently have search engines that work extremely well; both user experiences and uninvited advertising have been tailored to individual preferences. Such tasks as piloting an automobile, routinely accomplished by millions of humans without extensive training or individual genius, remain beyond the capability of the smartest computer system; the ability of an incredibly complicated machine to briefly fool a panel of human judges into thinking they are holding a conversation with a human child is celebrated as a landmark A.I. triumph. The practical application of such technology remains elusive, and yesterday’s promised miracles remain science fiction. But then again, many electronic conveniences we currently take for granted were science fiction just twenty years ago. It’s interesting to watch where high-rolling tech companies are placing their bets, and they all seem convinced that deep learning systems are worth investing sizable sums of money in. The problem for American investors is that it doesn’t seem like our domestic giants are nimble and adventurous enough to keep up with the work being done by the big Chinese companies. At least, that’s how Andrew Ng placed his bets.

---

## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/31/silicon-valley-reinvents-the-invisible-hand/
### Query Relevance Score: 0.10686248540878296
### Highlights: None

Jobs are the lifeblood of the economy. Whether self-employed or company-employed, workers rely on steady incomes to consume goods and pay for all of the necessities of life. When jobs are plentiful, friction decreases between job applicants and employers, ensuring that income is earned earlier and more frequently. Adam Smith’s invisible hand is the guiding force of the market in a capitalist system. By allowing everyone to make independent decisions in whatever way they define their self-interest, the market is presumed to automatically clear, moving goods and services from those who can offer them to those who most desire them. It’s ultimately a statement about market efficiency. That same invisible hand also rules the labor markets. Salaries are commensurate with skills, and employers compete for the workers they desire. If one company is able to create more profit from a talented individual than another company, then they theoretically will offer greater rewards (since they can) and poach that worker away. That’s the theory anyway. In reality, labor markets are filled with a viscous friction that continues to bedevil hiring managers and workers alike. Even in the best of times, it can take weeks from the moment someone starts seeking a job until they have secured a new income (and of course, it can take another several weeks for the first paycheck to actually arrive). In worse times, those weeks can quickly drag out to months and even years, creating a permanent class of unemployed workers with little hope of securing a job. Labor markets are filled with friction, which means that losing a job can easily destroy the financial security of an entire family. To get a sense of this friction, just note how easy it is to buy a product versus hiring an employee, and more importantly, how much easier the former has become over the years compared to the latter. Buying a product from Amazon today can be as simple as a single click, and it arrives on time with great customer service in case something goes wrong. Labor markets seem like they are stuck in Adam Smith’s era. We still apply for jobs the way we did decades ago, with resumes and cover sheets. We still conduct interviews, even though there is increasing proof not to mention plentiful anecdata that such practices are not a good judge of a worker’s abilities. Frankly, the biggest improvement to the process has been the use of machine learning to read resumes, but that has only led to keyword gaming and other useless activities which fail to actually make hiring more effective. Put simply, the invisible hand has been broken in the labor markets for years, even if we haven’t fully realized it. The reason is that labor markets are thin, with few available workers and even fewer employers. That’s why algorithmic marketplaces are so important. Startups are completely transforming the nature of labor markets, changing our notions of what employment can be and how income security functions. In the process, they are not just making our economy more efficient, but improving the lives of workers. Job Security and Thick Labor Markets To understand the context for these changes to our labor markets, we have to peer back into history to understand their development. For centuries, work revolved around the land. Nearly everyone in Europe following the fall of the Roman Empire was related to the land, whether working it directly, or in the case of feudal lords, sitting and waiting for the land to be tilled. What if looking at the past is precisely the wrong way to solve the problem though? What would happen if we actually had thick labor marketplaces without the friction that exists today? Our modern notion of income security is completely alien in this context. Income was the harvest, and variations in weather patterns could ensure a bountiful crop or a disastrous winter. There were some means of absorbing these shocks, but ultimately the market could force even the most careful families to succumb to its will. The growth in cities started to change this dynamic. For the first time, a worker was able to move between jobs, freed from a direct connection to the soil. These markets were rudimentary, and they remained so for centuries until the rapid growth of the industrial age in the early 1800s transformed them. From then on, we observe steady progress toward our modern notion of labor. What are some of those qualities? A safe work environment is one key component of the modern workplace, as well as various mechanisms for rest like weekends and vacation. But probably the most important element of the modern development of labor markets is the protections afforded workers from being fired. Whether tenure laws, anti-discrimination laws, human resources rules, lawsuits, or other institutions, safety from job termination is arguably one of the most fought after rights of workers over the past two centuries of development. I’ve already alluded to the reason for this focus. Labor markets are filled with friction, which means that losing a job can easily destroy the financial security of an entire family. Since it can take weeks or even months to find new employment, workers desire to stay in their current jobs as long as possible. As employment protections have decreased in the United States over the past few decades, families have compensated by relying on dual-incomes to ensure steady finances. In short, job security in our current labor markets is about avoiding job switches. Fundamentally, the challenge is one of the “thickness” of the market. Market thickness can be defined as the simultaneous number of buyers and sellers in a market. When there are only a handful of consumers and producers, it is hard for prices to be set and thus, for transactions to be completed. As both sides increase in volume, markets have more information, and thus are able to operate more efficiently and more rapidly. This is one of the reasons why it is easier to buy clothes than a house – the high price of houses and thus limited number of transactions makes it difficult for buyers and sellers to understand the market and make a deal. This is even more acute in the labor market. Outside of a handful of tech hubs where employee turnover is rapid, hiring is not at all robust. That makes labor markets quite thin, with limited numbers of employees moving from company to company to provide information on current market conditions to other participants. In these locales across the United States and really the whole world, the entire goal is to hold on to employment as tightly as possible and avoid the labor market as much as possible. For all of our progress, so little has changed. What Happens If We Have Thick Labor Markets? Actually, there has been some progress, most of it terrible. If you want to see what a somewhat thick labor market looks like, take a look at the service jobs in areas like food preparation and building maintenance. Due to innovations like franchising and increased usage of independent contractor terms for employment, these workers have none of the job protections of their predecessors and all the downside of friction-filled labor marketplaces. Employers have the upper hand in these transactions, and that has led to a disintegration of living wages and stable work arrangements. The schedules for service workers are anything but stable from week-to-week, creating vast problems for scheduling multiple jobs and child care for families. That has led to even more focus on holding onto a job no matter what the costs are, since switching jobs when you are living on the razor’s edge is impossible without financial disaster. We can see these trends reflected in all of the statistics about inequality that have been discussed in the media and academia the past few years. Middle-class jobs that used to provide job security and decent pay are increasingly being replaced by low-wage contract labor without any job protections at all. Wealth is being accumulated by a small percentage of the population, while the sons and daughters of middle-class families discover that the hollowing out of the American economy is going to force them to slide down the economic ladder. That has led to movements like Occupy Wall Street to demand changes to the ways that companies handle workers. Looking back at the 1960s and 70s, there is an increasing desire to return to the way things were, with secure employment in companies with greater job protections (of course, with women actually in the workplace this time). The cry from all of these movements is the same and it is clear: they want greater protection from the vagaries of the market, and they want everyone to be placed on an equal footing in the economy. That is one approach, but what if looking at the past is precisely the wrong way to solve the problem though? What would happen if we actually had thick labor marketplaces without the friction that exists today? Take, for instance, the service worker who can’t risk changing their job because they need their wages to arrive absolutely on time otherwise everything will fall apart. Since their shift schedule is changing so often, this individual also can’t switch jobs because they can’t find the time to interview. With a thick labor market, suddenly this person is able to seek out employment and instantly know who is hiring. Wages would be clearly available since information in the market is plentiful, and shift schedules would have to be equally spelled out. If the market was thick enough, it would even be possible to switch jobs within a single day. The ability of a software engineer in Silicon Valley to be working at Google in the morning and then switch to Facebook by the afternoon would be available to everyone in the market. It is our current system that is dehumanizing, whether it is applying for hundreds of jobs and never receiving a response or doing stupid exercises during interviews that are completely unrelated to the job that we applied for in the first place. For the first time, those service workers who have been powerless to resist their employers’ caprices would suddenly find that they have new leverage to use: they can actually leave since the market can provide them immediate employment. Their employer would be forced to comprehend their loss of power, or otherwise they may soon find that they have no workers at all. Suddenly, job security isn’t about avoiding the market, but rather fully embracing it. Knowing that you always have another job available can provide a psychological relief that no rule or law will ever fully offer. That also ensures that those who leave the market, say for maternity leave, can easily find their way back into the market again. It isn’t just job security that benefits with this development, but also job flexibility. Thick labor markets can also facilitate greater flexibility with shift schedules and work hours. A worker who wants to take a Wednesday off can now just choose to do so, knowing that the market will work its invisible hand to find another worker to take the shift. Similarly, workers with an extra hour or two may be able to find something to do productively in that time, providing them an extra bit of income. All of this sounds great, but there is an obvious concern. Markets only work when the price mechanism is allowed to be fully determined by market dynamics, and that means that wages will fluctuate until an equilibrium is established. Doesn’t that mean wages will plunge as a great mass of workers seek out these newly accessible service jobs that were previously difficult to get due to market friction? It is an obvious criticism, and an important one, but it makes one fatal flaw: it takes its data from thin labor markets, and not thick ones. With thick labor markets, employers could no longer miss wage payments because they want to make an extra buck – they would be forced out of the market due to their terrible reputation. Employers for the first time would actually have to compete for workers, since workers would have the leverage to leave at will at the prevailing wage. Perhaps most powerfully, consumers would know the general pay of individual establishments, because again, the market is able to provide that information to all participants. All of these forces would be strong, and all would push wages higher. We shouldn’t assume that the dynamics of our current labor market would carry over to a thick labor market. It’s entirely different, and in terms of job security, is vastly superior to our current model. The question of wages is vitally important, but we shouldn’t immediately dismiss thick labor markets as wage destroyers. There are other forces at work they may actually make them higher. Rebuilding The Invisible Hand The idea of thick labor markets is not unknown to Silicon Valley – Uber is the obvious example that comes up here. According to their own research, the company has created 160,000 flexible jobs for workers, and many thousands more are on the way. But Uber is just one small piece of the overall labor market, and thus its effect is still not enough to thicken the overall market. The idea of thick labor markets may be a bit of a thought experiment. Certainly their effects are a bit of a mystery – markets work in complex ways, and emergent properties of these markets may be difficult if not impossible to predict without building them in the first place. However, it is not hard to believe that technology and startups are going to increasingly take the friction out of the marketplace. Better algorithms can align the right workers with the right employers almost instantaneously, and reputation systems ensures that workers are more consistent in their work (and employers act responsibly as well). There are concerns that workers are dehumanized in such algorithmic assignment of work, a criticism that I strongly disagree with. It is our current system that is dehumanizing, whether it is applying for hundreds of jobs and never receiving a response or doing stupid exercises during interviews that are completely unrelated to the job that we applied for in the first place. Thicker labor markets would be an incredible improvement over our existing arrangement. I want to make finding work as easy as buying a pair of socks on Amazon. I want to hit a button, and be at my job in 20 minutes ready to go. When technology has allowed that to happen, we will have finally have given the power to workers to shape their lives how they want to. What a revolution indeed.

---

## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/31/narrow-ai-cant-do-that-or-can-it/
### Query Relevance Score: 0.1023608148097992
### Highlights: None

More posts by this contributor Editor’s note: David Senior is the CEO and co-founder of Lowdownapp. With industry pundits, including Stephen Hawking, Elon Musk and others, hotly debating the dangers of artificial intelligence and Hollywood priming the public for the release of a slew of new movies — including Terminator 5 — that warn what can happen when software and hardware evolve to the point that they are capable of human feats of intelligence, it’s little wonder that the popular view of AI has become confused and convoluted. The assumption that all AI is about systems designed to autonomously learn new tasks, adapt to changing environments and perhaps, like HAL, outwit their creators in the end, skims over the many important differences between classic AI (the one movies are made of) and its over one dozen subdisciplines. These AI subsets, ranging from the speech recognition and natural language understanding we know from personal assistants like Siri and Cortana, to the machine learning and deep learning capabilities core to business analytics and systems designed to make sense of big data, are — and will remain — where the action and opportunity is. VCs need no convincing. The last few months have seen a flurry of activity and a wave of investment as startups in Silicon Valley and beyond raise substantial funding for AI approaches and innovations that emphasize the business benefits of AI and narrow AI, a technology subset focused on solving specific, reasonably well-defined problems. It’s a stampede as many of Silicon Valley’s leading venture capital firms, including Khosla Ventures and Greylock Partners, as well as financial institutions, such as Goldman Sachs, flock to the space and invest big dollars in companies using narrow AI technologies to tackle tough business tasks like taming big data. A prime example is Seattle-based Context Relevant, a provider of automated predictive analytics software for big data 2.0 applications. It raised $13.5 million in series B-1 funding with participation by Goldman Sachs, Bank of America Merrill Lynch, Formation 8, New York Life and Bloomberg Beta in September 2014. The round of funding came just months after it closed $21 million in a Series B round, bringing total funding for Context Relevant to $42 million. With an estimated 170 startups in the starting gate ready to recast themselves as AI companies, or simply jump on the bandwagon, you can bet this year will see AI (in all its flavors and forms) lead the list of mega-trends. But before you dismiss this as hype, consider that the rise of AI, and specifically weak or narrow AI, is also inextricably linked with the growth of big data. Simply put, it’s the explosive rate of information growth that creates the requirement for narrow AI. Add to that the recent avalanche of user-generated content — the nearly 300,000 tweets, 220,000 Instagram photos, 72 hours of YouTube video content and the 2.5 million pieces of content shared by Facebook users every single minute that businesses must monitor and acknowledge — and it’s clear that no organization (or human) can cope without the aid of narrow AI. But the business and personal benefits of narrow AI go far beyond the ability to trawl through massive amounts of information and automate routine knowledge work. Some narrow AI approaches sift through the data to pull together and expose what is relevant and valuable to the individual user and their “need” state. An early and rather primitive example of this is Apple’s Siri. I give it credit for bringing narrow AI to the mainstream. But I also side with Robert Scoble, who has repeatedly remarked that the fatal flaw in this and other personal assistant services is a lack of context. Successful narrow AI services, to deliver value and benefit, must be aware of the user’s environment and factor this into the equation before delivering answers or advice or simply taking action. A great sandbox for ideas and innovation is the smartphone calendar, the fiercely personal device most people regard as a digital extension of their physical “self.” The calendar and contacts is not only where people live and record their lives; it’s a living microcosm of their social graph device that grows and evolves as people and their networks do. And so it makes sense that this space — where the calendar, contacts and context come together — is where startups are staging a new battle. The stakes are high, which is why competition is also high, as companies conduct a digital battle as fierce as Arnie and as determined as HAL.

---

## Article published: 01/2015
### Source URL: https://techcrunch.com/2015/01/25/the-human-impact-of-the-industrial-internet-of-things/?ncid=rss
### Query Relevance Score: 0.09944818913936615
### Highlights: None

Bruno Berthon Contributor Editor’s note: Bruno Berthon is the managing director of digital strategy at Accenture. Will digital technology be positive for workers and jobs? Amid today’s public debate about the consequences of artificial intelligence and advanced robotics, along comes the industrial Internet of things (IIoT). Little understood but potentially very significant for multiple industry sectors, this next wave of technology will create more jobs than it will destroy, according to the majority of business leaders Accenture has surveyed. The industrial Internet of things is a fast-growing network of intelligent connected devices, machines and objects. It will certainly automate and drive efficiencies, but the optimism of employers reflects their recognition that, more importantly, it will enable the creation of entirely new products and services and markets. Indeed, Accenture Strategy estimates the IIoT could boost the gross domestic product (GDP) of 20 of the world’s largest economies by an additional US$14 trillion by 2030. Where some technology advances have simply improved the quality and price competitiveness of products—mainly through automation—the IIoT breaks new ground in helping use vast volumes of data from those products and other physical objects to offer tailored outcomes to customers. Take the example of the agrochemical sector. By integrating climatic, geological and other data, companies can go beyond selling products to earning revenues from guaranteed yields for specific crops in certain locations. Similarly, engine manufacturers could be rewarded for delivering reduced air travel delays by pre-empting maintenance issues through the real-time monitoring of engine performance in flight. In short, the outcome economy has arrived, where partnerships between companies and their respective workers inspire more bespoke and varied solutions for customers. Small wonder, then, that 86 percent of the 1,400 business leaders we polled think the industrial Internet of things will be a net creator of jobs. There is already evidence that workforce transformation is happening. A Maryland steel company used automation and robotics coupled with analytics, which led to more knowledge-intensive work. The result? A safer, more engaging work experience alongside higher productivity and quality. By making digital investments, the steel company was able to significantly increase hourly pay and experienced growing demand that led to an increase in hires. It is not just about new jobs but the content of those new roles. Many businesses will demand new skills and reward workers with more interesting work. Accenture and Royal Philips’ proof-of-concept demonstration uses a Google Glass head-mounted display to research ways to improve the effectiveness and efficiency of performing surgical procedures. Theoretically, hands-free access to critical clinical information could also be applied in the utilities or communications industries, helping field engineers repair complex equipment in more difficult stations than they can today. These digital enhancements augment skills as employees blend their skills with those of digital labor. The IIoT can also empower workers. By sharing data about how customers are interacting with products, employees can use 3D printing and other technologies to experiment in virtual teams, produce prototypes more quickly and tweak product design. Innovation is not only more spontaneous and synchronized; it is autonomous, liberating employees from traditional research and development structures. At the core of this change is the way workers will be freed from volume activity to address individual exceptions revealed by data. In this way, they can resolve challenges faced by particular customers and design more tailored solutions for them. This shift in focus from delivering mass products to delivering outcomes for customers places greater emphasis on talent. The benefits are not guaranteed, however. Seventy two percent of the CEOs and business leaders Accenture surveyed say their company has yet to make concrete plans for the industrial Internet of things. Only seven percent have developed a comprehensive IIoT strategy with investments to match. What do leaders need to do to ensure digital technology brings advantages to workers? Leaders will have to take risks by collapsing hierarchies and permitting new levels of autonomy so that workers can use data and intelligent connected devices to collaborate more with counterparts in other companies. Some companies will need to get ahead of organizational change that will otherwise be forced on them, as digital technology reverses recent trends by centralizing manufacturing while decentralizing services’ delivery. The IIoT depends on significant investments in developing ad hoc skills and breeding new talent. New jobs, from digital robot design and healthcare analytics to transport network engineering and software development, can only be created if businesses and governments and the education sector work together to redesign education curricula. Talent and skills are the most important determinant of whether countries and companies use this new digital era to secure growth and boost their competitiveness. And workers and employability could also be the greatest beneficiaries. That tantalizing prospect depends on business and governments doing more to recognize the generational transformation in the workforce that could result from embracing the industrial Internet of things.

---

## Article published: 01/2015
### Source URL: https://www.nytimes.com/2015/01/29/technology/personaltech/uber-a-rising-business-model.html?ref=business
### Query Relevance Score: 0.12235559523105621
### Highlights: None

State of the ArtCredit...Stuart GoldenbergJan. 28, 2015As Uber has grown to become one of the world’s most valuable start-ups, its ambitions often seem limitless.But of all the ways that Uber could change the world, the most far-reaching may be found closest at hand: your office. Uber, and more broadly the app-driven labor market it represents, is at the center of what could be a sea change in work, and in how people think about their jobs. You may not be contemplating becoming an Uber driver any time soon, but the Uberization of work may soon be coming to your chosen profession.Just as Uber is doing for taxis, new technologies have the potential to chop up a broad array of traditional jobs into discrete tasks that can be assigned to people just when they’re needed, with wages set by a dynamic measurement of supply and demand, and every worker’s performance constantly tracked, reviewed and subject to the sometimes harsh light of customer satisfaction. Uber and its ride-sharing competitors, including Lyft and Sidecar, are the boldest examples of this breed, which many in the tech industry see as a new kind of start-up — one whose primary mission is to efficiently allocate human beings and their possessions, rather than information.Various companies are now trying to emulate Uber’s business model in other fields, from daily chores like grocery shopping and laundry to more upmarket products like legal services and even medicine.“I do think we are defining a new category of work that isn’t full-time employment but is not running your own business either,” said Arun Sundararajan, a professor at New York University’s business school who has studied the rise of the so-called on-demand economy, and who is mainly optimistic about its prospects.Uberization will have its benefits: Technology could make your work life more flexible, allowing you to fit your job, or perhaps multiple jobs, around your schedule, rather than vice versa. Even during a time of renewed job growth, Americans’ wages are stubbornly stagnant, and the on-demand economy may provide novel streams of income.ImageCredit...Todd Heisler/The New York Times“We may end up with a future in which a fraction of the work force would do a portfolio of things to generate an income — you could be an Uber driver, an Instacart shopper, an Airbnb host and a Taskrabbit,” Dr. Sundararajan said.But the rise of such work could also make your income less predictable and your long-term employment less secure. And it may relegate the idea of establishing a lifelong career to a distant memory.“I think it’s nonsense, utter

---

## Article published: 01/2015
### Source URL: https://www.nytimes.com/2015/01/18/books/review/among-the-disrupted.html?referrer=&_r=0
### Query Relevance Score: 0.11083629727363586
### Highlights: None

https://www.nytimes.com/2015/01/18/books/review/among-the-disrupted.html?referrer=&_r=0 Among the Disrupted 2015-01-18 Leon Wieseltier AdvertisementSKIP ADVERTISEMENTEssayJan. 7, 2015Amid the bacchanal of disruption, let us pause to honor the disrupted. The streets of American cities are haunted by the ghosts of bookstores and record stores, which have been destroyed by the greatest thugs in the history of the culture industry. Writers hover between a decent poverty and an indecent one; they are expected to render the fruits of their labors for little and even for nothing, and all the miracles of electronic dissemination somehow do not suffice for compensation, either of the fiscal or the spiritual kind. Everybody talks frantically about media, a second-order subject if ever there was one, as content disappears into “content.” What does the understanding of media contribute to the understanding of life? Journalistic institutions slowly transform themselves into silent sweatshops in which words cannot wait for thoughts, and first responses are promoted into best responses, and patience is a professional liability. As the frequency of expression grows, the force of expression diminishes: Digital expectations of alacrity and terseness confer the highest prestige upon the twittering cacophony of one-liners and promotional announcements. It was always the case that all things must pass, but this is ridiculous.Meanwhile the discussion of culture is being steadily absorbed into the discussion of business. There are “metrics” for phenomena that cannot be metrically measured. Numerical values are assigned to things that cannot be captured by numbers. Economic concepts go rampaging through noneconomic realms: Economists are our experts on happiness! Where wisdom once was, quantification will now be. Quantification is the most overwhelming influence upon the contemporary American understanding of, well, everything. It is enabled by the idolatry of data, which has itself been enabled by the almost unimaginable data-generating capabilities of the new technology. The distinction between knowledge and information is a thing of the past, and there is no greater disgrace than to be a thing of the past. Beyond its impact upon culture, the new technology penetrates even deeper levels of identity and experience, to cognition and to consciousness. Such transformations embolden certain high priests in the church of tech to espouse the doctrine of “transhumanism” and to suggest, without any recollection of the bankruptcy of utopia, without any consideration of the cost to human dignity, that our computational ability will carry us magnificently beyond our humanity and “allow us to transcend these limitations of our biological bodies and brains. . . . There will be no distinction, post-Singularity, between human and machine.” (The author of that updated mechanistic nonsense is a director of engineering at Google.)And even as technologism, which is not the same as technology, asserts itself over more and more precincts of human life, so too does scientism, which is not the same as science. The notion that the nonmaterial dimensions of life must be explained in terms of the material dimensions, and that nonscientific understandings must be translated into scientific understandings if they are to qualify as knowledge, is increasingly popular inside and outside the university, where the humanities are disparaged as soft and impractical and insufficiently new. The contrary insistence that the glories of art and thought are not evolutionary adaptations, or that the mind is not the brain, or that love is not just biology’s bait for sex, now amounts to a kind of heresy. So, too, does the view that the strongest defense of the humanities lies not in the appeal to their utility — that literature majors may find good jobs, that theaters may economically revitalize neighborhoods — but rather in the appeal to their defiantly nonutilitarian character, so that individuals can know more than how things work, and develop their powers of discernment and judgment, their competence in matters of truth and goodness and beauty, to equip themselves adequately for the choices and the crucibles of private and public life.◆This gloomy inventory of certain tendencies in contemporary American culture — it is not the whole story, but it is an alarmingly large part of the story — is offered for the purpose of proposing an accurate name for our moment. We are not becoming transhumanists, obviously. We are too singular for the Singularity. But are we becoming posthumanists?No culture is philosophically monolithic, or promotes a single conception of the human. A culture is an internecine contest between alternative conceptions of the human. Which culture is free of contradictions between first principles? This is no less true of religious cultures than of secular ones, of closed societies than of open ones. Popular culture may be as soaked in ideas as high culture: A worldview can be found in a song. Wherever mortal beings are thoughtful about their mortality, and finite beings ponder their finitude, at whatever level of intellectual articulation, there is philosophy. Philosophy is ubiquitous and inalienable; even the discourse about the end of philosophy is philosophy. A culture may be regarded as the sum of all the philosophies, all the reflective approaches to living, that are manifestly or latently expressed in a society. It is a gorgeous anarchy, even if it contains illusions and errors. There are worse things than being wrong.Within a culture, however, some views may come to prevail over others, for intellectual or social reasons. The war between the worldviews has winners and losers, though none of the worldviews are ever erased and there is honor also in loss. In American culture right now, as I say, the worldview that is ascendant may be described as posthumanism. We have been here before, and not too long ago, but for different reasons. The posthumanism of the 1970s and 1980s was more insular, an academic affair of “theory,” an insurgency of professors; our posthumanism is a way of life, a social fate. An important book, a brilliant book, an exasperating book has just been written about the origins of that previous posthumanist moment. In “The Age of the Crisis of Man: Thought and Fiction in America, 1933-1973,” the gifted essayist Mark Greif, who reveals himself to be also a skillful historian of ideas, charts the history of the 20th-century reckonings with the definition of “man.” Strangely, he seems to regret the entire enterprise. Here is his conclusion: “Anytime your inquiries lead you to say, ‘At this moment we must ask and decide who we fundamentally are, our solution and salvation must lie in a new picture of ourselves and humanity, this is our profound responsibility and a new opportunity’ — just stop.” Greif seems not to realize that his own book is a lasting monument to precisely such inquiry, and to its grandeur. “Answer, rather, the practical matters,” he counsels, in accordance with the current pragmatist orthodoxy. “Find the immediate actions necessary to achieve an aim.” But before an aim is achieved, should it not be justified? And the activity of justification may require a “picture of ourselves.” Don’t just stop. Think harder. Get it right. (Why are liberals so afraid of their own philosophy?)Greif’s book is a prehistory of our predicament, of our own “crisis of man.” (The “man” is archaic, the “crisis” is not.) It recognizes that the intellectual history of modernity may be written in part as the epic tale of a series of rebellions against humanism. Humanism has been savaged by theists and atheists, conservatives and progressives, fascists and socialists, scientists and philosophers, though it has also been propounded by the same diversity of thinkers. Who has not felt superior to humanism? It is the cheapest target of all: Humanism is sentimental, flabby, bourgeois, hypocritical, complacent, middlebrow, liberal, sanctimonious, constricting and often an alibi for power. The abusers of humanism, of course, are guilty of none of those sins. From Heidegger to Althusser, they come as emancipators. I think we should emancipate ourselves from their emancipations.But what is humanism? For a start, humanism is not the antithesis of religion, as Pope Francis is exquisitely demonstrating. The most common understanding of humanism is that it denotes a pedagogy and a worldview. The pedagogy consists in the traditional Western curriculum of literary and philosophical classics, beginning in Greek and Roman antiquity and — after an unfortunate banishment of medieval culture from any pertinence to our own — erupting in the rediscovery of that antiquity in Europe in the early modern centuries, and in the ideals of personal cultivation by means of textual study and aesthetic experience that it bequeathed, or that were developed under its inspiration, in the “enlightened” 18th and 19th centuries, and eventually culminated in programs of education in the humanities in modern universities. The worldview takes many forms: a philosophical claim about the centrality of humankind to the universe, and about the irreducibility of the human difference to any aspect of our animality; a methodological claim about the most illuminating way to explain history and human affairs, and about the essential inability of the natural sciences to offer a satisfactory explanation; a moral claim about the priority, and the universal nature, of certain values, not least tolerance and compassion. It is all a little inchoate — human, humane, humanities, humanism, humanitarianism; but there is nothing shameful or demeaning about any of it.And posthumanism? It elects to understand the world in terms of impersonal forces and structures, and to deny the importance, and even the legitimacy, of human agency. It certainly does not mean, as Greif correctly notes about antihumanism, a “hatred of the human.” There have been humane posthumanists and there have been inhumane humanists. But the inhumanity of humanists may be refuted on the basis of their own worldview, whereas the condemnation of cruelty toward “man the machine,” to borrow the old but enduring notion of an 18th-century French materialist, requires the importation of another framework of judgment. The same is true about universalism, which every critic of humanism has arraigned for its failure to live up to the promise of a perfect inclusiveness. It is a melancholy fact of history that there has never been a universalism that did not exclude. Yet the same is plainly the case about every particularism, which is nothing but a doctrine of exclusion; and the correction of particularism, the extension of its concept and its care, cannot be accomplished in its own name. It requires an idea from outside, an idea external to itself, a universalistic idea, a humanistic idea. Asking universalism to keep faith with its own principles is a perennial activity of moral life. Asking particularism to keep faith with its own principles is asking for trouble.◆Aside from issues of life and death, there is no more urgent task for American intellectuals and writers than to think critically about the salience, even the tyranny, of technology in individual and collective life. All revolutions exaggerate, and the digital revolution is no different. We are still in the middle of the great transformation, but it is not too early to begin to expose the exaggerations, and to sort out the continuities from the discontinuities. The burden of proof falls on the revolutionaries, and their success in the marketplace is not sufficient proof. Presumptions of obsolescence, which are often nothing more than the marketing techniques of corporate behemoths, need to be scrupulously examined. By now we are familiar enough with the magnitude of the changes in all the spheres of our existence to move beyond the futuristic rhapsodies that characterize much of the literature on the subject. We can no longer roll over and celebrate and shop. Every phone in every pocket contains a “picture of ourselves,” and we must ascertain what that picture is and whether we should wish to resist it. Here is a humanist proposition for the age of Google: The processing of information is not the highest aim to which the human spirit can aspire, and neither is competitiveness in a global economy. The character of our society cannot be determined by engineers.“Our very mastery seems to escape our mastery,” Michel Serres has anxiously remarked. “How can we dominate our domination; how can we master our own mastery?” Every technology is used before it is completely understood. There is always a lag between an innovation and the apprehension of its consequences. We are living in that lag, and it is a right time to keep our heads and reflect. We have much to gain and much to lose. In the media, for example, the general inebriation about the multiplicity of platforms has distracted many people from the scruple that questions of quality on the new platforms should be no different from questions of quality on the old platforms. Otherwise a quantitative expansion will result in a qualitative contraction. The new devices do not in themselves authorize a revision of the standards of evidence and argument and style that we championed in the old devices. (What a voluptuous device paper is!) Such revisions may be made on other grounds — out of commercial ambition, for example; but there is nothing innovative about pandering for the sake of a profit. The decision to prefer the requirements of commerce to the requirements of culture cannot be exonerated by the thrills of the digital revolution.And therein lies a consoling irony of our situation. The machines may be more neutral about their uses than the propagandists and the advertisers want us to believe. We can leave aside the ideology of digitality and its aggressions, and regard the devices as simply new means for old ends. Tradition “travels” in many ways. It has already flourished in many technologies — but only when its flourishing has been the objective. I will give an example from the humanities. The day is approaching when the dream of the democratization of knowledge — Borges’s fantasy of “the total library” — will be realized. Soon all the collections in all the libraries and all the archives in the world will be available to everyone with a screen. Who would not welcome such a vast enfranchisement? But universal accessibility is not the end of the story, it is the beginning. The humanistic methods that were practiced before digitalization will be even more urgent after digitalization, because we will need help in navigating the unprecedented welter. Searches for keywords will not provide contexts for keywords. Patterns that are revealed by searches will not identify their own causes and reasons. The new order will not relieve us of the old burdens, and the old pleasures, of erudition and interpretation.Is all this — is humanism — sentimental? But sentimentality is not always a counterfeit emotion. Sometimes sentiment is warranted by reality. The persistence of humanism through the centuries, in the face of formidable intellectual and social obstacles, has been owed to the truth of its representations of our complexly beating hearts, and to the guidance that it has offered, in its variegated and conflicting versions, for a soulful and sensitive existence. There is nothing soft about the quest for a significant life. And a complacent humanist is a humanist who has not read his books closely, since they teach disquiet and difficulty. In a society rife with theories and practices that flatten and shrink and chill the human subject, the humanist is the dissenter. Never mind the platforms. Our solemn responsibility is for the substance.LEON WIESELTIER is a contributing editor at The Atlantic and the author of “Kaddish.”A version of this article appears in print on , Page 1 of the Sunday Book Review with the headline: Among the DisruptedAdvertisementSKIP ADVERTISEMENT||||I|||| Skip to contentSkip to site index Book Review Today’s Paper What to Read * January Releases * Your Next Read * Critics’ Reviews * Editors’ Choice * 100 Notable Books * U.S. * World * Business * Arts * Lifestyle * Opinion * Audio * Games * Cooking * Wirecutter * The Athletic What to Read * January Releases * Your Next Read * Critics’ Reviews * Editors’ Choice * 100 Notable Books Advertisement SKIP ADVERTISEMENT What to Read * January Releases * Your Next Read * Critics’ Reviews * Editors’ Choice * 100 Notable Books Supported by SKIP ADVERTISEMENT Essay Among the Disrupted * Share full article * * Credit... Joon Mo Kang By Leon Wieseltier * Jan. 7, 2015 Amid the bacchanal of disruption, let us pause to honor the disrupted. The streets of American cities are haunted by the ghosts of bookstores and record stores, which have been destroyed by the greatest thugs in the history of the culture industry. Writers hover between a decent poverty and an indecent one; they are expected to render the fruits of their labors for little and even for nothing, and all the miracles of electronic dissemination somehow do not suffice for compensation, either of the fiscal or the spiritual kind. Everybody talks frantically about media, a second-order subject if ever there was one, as content disappears into “content.” What does the understanding of media contribute to the understanding of life? Journalistic institutions slowly transform themselves into silent sweatshops in which words cannot wait for thoughts, and first responses are promoted into best responses, and patience is a professional liability. As the frequency of expression grows, the force of expression diminishes: Digital expectations of alacrity and terseness confer the highest prestige upon the twittering cacophony of one-liners and promotional announcements. It was always the case that all things must pass, but this is ridiculous. Meanwhile the discussion of culture is being steadily absorbed into the discussion of business. There are “metrics” for phenomena that cannot be metrically measured. Numerical values are assigned to things that cannot be captured by numbers. Economic concepts go rampaging through noneconomic realms: Economists are our experts on happiness! Where wisdom once was, quantification will now be. Quantification is the most overwhelming influence upon the contemporary American understanding of, well, everything. It is enabled by the idolatry of data, which has itself been enabled by the almost unimaginable data-generating capabilities of the new technology. The distinction between knowledge and information is a thing of the past, and there is no greater disgrace than to be a thing of the past. Beyond its impact upon culture, the new technology penetrates even deeper levels of identity and experience, to cognition and to consciousness. Such transformations embolden certain high priests in the church of tech to espouse the doctrine of “transhumanism” and to suggest, without any recollection of the bankruptcy of utopia, without any consideration of the cost to human dignity, that our computational ability will carry us magnificently beyond our humanity and “allow us to transcend these limitations of our biological bodies and brains. . . . There will be no distinction, post-Singularity, between human and machine.” (The author of that updated mechanistic nonsense is a director of engineering at Google.) And even as technologism, which is not the same as technology, asserts itself over more and more precincts of human life, so too does scientism, which is not the same as science. The notion that the nonmaterial dimensions of life must be explained in terms of the material dimensions, and that nonscientific understandings must be translated into scientific understandings if they are to qualify as knowledge, is increasingly popular inside and outside the university, where the humanities are disparaged as soft and impractical and insufficiently new. The contrary insistence that the glories of art and thought are not evolutionary adaptations, or that the mind is not the brain, or that love is not just biology’s bait for sex, now amounts to a kind of heresy. So, too, does the view that the strongest defense of the humanities lies not in the appeal to their utility — that literature majors may find good jobs, that theaters may economically revitalize neighborhoods — but rather in the appeal to their defiantly nonutilitarian character, so that individuals can know more than how things work, and develop their powers of discernment and judgment, their competence in matters of truth and goodness and beauty, to equip themselves adequately for the choices and the crucibles of private and public life. ◆ This gloomy inventory of certain tendencies in contemporary American culture — it is not the whole story, but it is an alarmingly large part of the story — is offered for the purpose of proposing an accurate name for our moment. We are not becoming transhumanists, obviously. We are too singular for the Singularity. But are we becoming posthumanists? No culture is philosophically monolithic, or promotes a single conception of the human. A culture is an internecine contest between alternative conceptions of the human. Which culture is free of contradictions between first principles? This is no less true of religious cultures than of secular ones, of closed societies than of open ones. Popular culture may be as soaked in ideas as high culture: A worldview can be found in a song. Wherever mortal beings are thoughtful about their mortality, and finite beings ponder their finitude, at whatever level of intellectual articulation, there is philosophy. Philosophy is ubiquitous and inalienable; even the discourse about the end of philosophy is philosophy. A culture may be regarded as the sum of all the philosophies, all the reflective approaches to living, that are manifestly or latently expressed in a society. It is a gorgeous anarchy, even if it contains illusions and errors. There are worse things than being wrong. Within a culture, however, some views may come to prevail over others, for intellectual or social reasons. The war between the worldviews has winners and losers, though none of the worldviews are ever erased and there is honor also in loss. In American culture right now, as I say, the worldview that is ascendant may be described as posthumanism. We have been here before, and not too long ago, but for different reasons. The posthumanism of the 1970s and 1980s was more insular, an academic affair of “theory,” an insurgency of professors; our posthumanism is a way of life, a social fate. An important book, a brilliant book, an exasperating book has just been written about the origins of that previous posthumanist moment. In “The Age of the Crisis of Man: Thought and Fiction in America, 1933-1973,” the gifted essayist Mark Greif, who reveals himself to be also a skillful historian of ideas, charts the history of the 20th-century reckonings with the definition of “man.” Strangely, he seems to regret the entire enterprise. Here is his conclusion: “Anytime your inquiries lead you to say, ‘At this moment we must ask and decide who we fundamentally are, our solution and salvation must lie in a new picture of ourselves and humanity, this is our profound responsibility and a new opportunity’ — just stop.” Greif seems not to realize that his own book is a lasting monument to precisely such inquiry, and to its grandeur. “Answer, rather, the practical matters,” he counsels, in accordance with the current pragmatist orthodoxy. “Find the immediate actions necessary to achieve an aim.” But before an aim is achieved, should it not be justified? And the activity of justification may require a “picture of ourselves.” Don’t just stop. Think harder. Get it right. (Why are liberals so afraid of their own philosophy?) Image Credit... Joon Mo Kang Greif’s book is a prehistory of our predicament, of our own “crisis of man.” (The “man” is archaic, the “crisis” is not.) It recognizes that the intellectual history of modernity may be written in part as the epic tale of a series of rebellions against humanism. Humanism has been savaged by theists and atheists, conservatives and progressives, fascists and socialists, scientists and philosophers, though it has also been propounded by the same diversity of thinkers. Who has not felt superior to humanism? It is the cheapest target of all: Humanism is sentimental, flabby, bourgeois, hypocritical, complacent, middlebrow, liberal, sanctimonious, constricting and often an alibi for power. The abusers of humanism, of course, are guilty of none of those sins. From Heidegger to Althusser, they come as emancipators. I think we should emancipate ourselves from their emancipations. But what is humanism? For a start, humanism is not the antithesis of religion, as Pope Francis is exquisitely demonstrating. The most common understanding of humanism is that it denotes a pedagogy and a worldview. The pedagogy consists in the traditional Western curriculum of literary and philosophical classics, beginning in Greek and Roman antiquity and — after an unfortunate banishment of medieval culture from any pertinence to our own — erupting in the rediscovery of that antiquity in Europe in the early modern centuries, and in the ideals of personal cultivation by means of textual study and aesthetic experience that it bequeathed, or that were developed under its inspiration, in the “enlightened” 18th and 19th centuries, and eventually culminated in programs of education in the humanities in modern universities. The worldview takes many forms: a philosophical claim about the centrality of humankind to the universe, and about the irreducibility of the human difference to any aspect of our animality; a methodological claim about the most illuminating way to explain history and human affairs, and about the essential inability of the natural sciences to offer a satisfactory explanation; a moral claim about the priority, and the universal nature, of certain values, not least tolerance and compassion. It is all a little inchoate — ­human, humane, humanities, humanism, humanitarianism; but there is nothing shameful or demeaning about any of it. And posthumanism? It elects to understand the world in terms of impersonal forces and structures, and to deny the importance, and even the legitimacy, of human agency. It certainly does not mean, as Greif correctly notes about antihumanism, a “hatred of the human.” There have been humane posthumanists and there have been inhumane humanists. But the inhumanity of humanists may be refuted on the basis of their own worldview, whereas the condemnation of cruelty toward “man the machine,” to borrow the old but enduring notion of an 18th-century French materialist, requires the importation of another framework of judgment. The same is true about universalism, which every critic of humanism has arraigned for its failure to live up to the promise of a perfect inclusiveness. It is a melancholy fact of history that there has never been a universalism that did not exclude. Yet the same is plainly the case about every particularism, which is nothing but a doctrine of exclusion; and the correction of particularism, the extension of its concept and its care, cannot be accomplished in its own name. It requires an idea from outside, an idea external to itself, a universalistic idea, a humanistic idea. Asking universalism to keep faith with its own principles is a perennial activity of moral life. Asking particularism to keep faith with its own principles is asking for trouble. ◆ Aside from issues of life and death, there is no more urgent task for American intellectuals and writers than to think critically about the salience, even the tyranny, of technology in individual and collective life. All revolutions exaggerate, and the digital revolution is no different. We are still in the middle of the great transformation, but it is not too early to begin to expose the exaggerations, and to sort out the continuities from the discontinuities. The burden of proof falls on the revolutionaries, and their success in the marketplace is not sufficient proof. Presumptions of obsolescence, which are often nothing more than the marketing techniques of corporate behemoths, need to be scrupulously examined. By now we are familiar enough with the magnitude of the changes in all the spheres of our existence to move beyond the futuristic rhapsodies that characterize much of the literature on the subject. We can no longer roll over and celebrate and shop. Every phone in every pocket contains a “picture of ourselves,” and we must ascertain what that picture is and whether we should wish to resist it. Here is a humanist proposition for the age of Google: The processing of information is not the highest aim to which the human spirit can aspire, and neither is competitiveness in a global economy. The character of our society cannot be determined by engineers. “Our very mastery seems to escape our mastery,” Michel Serres has anxiously remarked. “How can we dominate our domination; how can we master our own mastery?” Every technology is used before it is completely understood. There is always a lag between an innovation and the apprehension of its consequences. We are living in that lag, and it is a right time to keep our heads and reflect. We have much to gain and much to lose. In the media, for example, the general inebriation about the multiplicity of platforms has distracted many people from the scruple that questions of quality on the new platforms should be no different from questions of quality on the old platforms. Otherwise a quantitative expansion will result in a qualitative contraction. The new devices do not in themselves authorize a revision of the standards of evidence and argument and style that we championed in the old devices. (What a voluptuous device paper is!) Such revisions may be made on other grounds — out of commercial ambition, for example; but there is nothing innovative about pandering for the sake of a profit. The decision to prefer the requirements of commerce to the requirements of culture cannot be exonerated by the thrills of the digital revolution. And therein lies a consoling irony of our situation. The machines may be more neutral about their uses than the propagandists and the advertisers want us to believe. We can leave aside the ideology of digitality and its aggressions, and regard the devices as simply new means for old ends. Tradition “travels” in many ways. It has already flourished in many technologies — but only when its flourishing has been the objective. I will give an example from the humanities. The day is approaching when the dream of the democratization of knowledge — Borges’s fantasy of “the total library” — will be realized. Soon all the collections in all the libraries and all the archives in the world will be available to everyone with a screen. Who would not welcome such a vast enfranchisement? But universal accessibility is not the end of the story, it is the beginning. The humanistic methods that were practiced before digitalization will be even more urgent after digitalization, because we will need help in navigating the unprecedented welter. Searches for keywords will not provide contexts for keywords. Patterns that are revealed by searches will not identify their own causes and reasons. The new order will not relieve us of the old burdens, and the old pleasures, of erudition and interpretation. Is all this — is humanism — sentimental? But sentimentality is not always a counterfeit emotion. Sometimes sentiment is warranted by reality. The persistence of humanism through the centuries, in the face of formidable intellectual and social obstacles, has been owed to the truth of its representations of our complexly beating hearts, and to the guidance that it has offered, in its variegated and conflicting versions, for a soulful and sensitive existence. There is nothing soft about the quest for a significant life. And a complacent humanist is a humanist who has not read his books closely, since they teach disquiet and difficulty. In a society rife with theories and practices that flatten and shrink and chill the human subject, the humanist is the dissenter. Never mind the platforms. Our solemn responsibility is for the substance. LEON WIESELTIER is a contributing editor at The Atlantic and the author of “Kaddish.” A version of this article appears in print on , Page 1 of the Sunday Book Review with the headline: Among the Disrupted . Order Reprints | Today’s Paper | Subscribe * Share full article * * Advertisement SKIP ADVERTISEMENT Site Index Site Information Navigation * © 2024 The New York Times Company * NYTCo * Contact Us * Accessibility * Work with us * Advertise * T Brand Studio * Your Ad Choices * Privacy Policy * Terms of Service * Terms of Sale * Site Map * Canada * International * Help * Subscriptions * Manage Privacy Preferences

---

## Article published: 01/2015
### Source URL: https://www.nytimes.com/2015/01/16/opinion/the-cruel-waste-of-americas-tech-talent.html
### Query Relevance Score: 0.10828440636396408
### Highlights: None

The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you’d like. nytimes.com/subscription

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/science/want-to-call-aliens-keep-it-short-and-simple-scientists-say.print
### Query Relevance Score: 0.050767723470926285
### Highlights: None

By , Published January 13, 2015 If extraterrestrials ever phoned us, they would be more likely to send narrowly directed bursts than constantly blaring signals — more text-messaging than novel-writing, some scientists now suggest. Such a strategy also would be more practical for anyone on Earth to reach out to aliens — if one had at least a billion dollars to spend. How cheap is talk? For 50 years, people have watched the skies with radio telescopes, hoping to detect signals of intelligent alien life. So far, however, the efforts of SETI (the search for extraterrestrial intelligence ) have proved fruitless. Now some scientists suggest the kind of signals SETI has hoped to find for decades might not be what aliens would broadcast. In a way, the form of communication might come down to cost. "Our grandfather used to say, 'Talk is cheap, but whiskey costs money,'" said researcher Gregory Benford, an astrophysicist at the University of California, Irvine, and an award-winning science fiction novelist. "Whatever the life form, evolution selects for economy of resources. Broadcasting is expensive, and transmitting signals across light-years would require considerable resources." Assuming that aliens would strive to optimize costs, limit waste and make their signaling technology more efficient, Benford and his twin, James — a fellow physicist who specializes in high-powered microwave technology — suggest the signals would not be steadily blasted out in all directions. Extraterrestrials would be more likely to send narrow "searchlight" beams delivered in pulses. "This approach is more like Twitter and less like 'War and Peace,'" said James Benford, founder and president of Microwave Sciences Inc., in Lafayette, Calif. The Benford twins, along with James' son Dominic, a NASA scientist, detailed their findings in two studies appearing in the June issue of the journal Astrobiology. The Benfords suggest a continuous signal blared at thousands of stars would simply cost too much energy. They say aliens might use short bursts — say, anywhere from a second to an hour long — and point these signals in narrow beams at one star and then another in a cycle involving up to thousands of stars that repeats over days or years. For civilizations that constantly watch the skies, the bursts would convey enough data to be recognized as undeniably artificial. As observant civilizations concentrated on this simple beacon, other beacons could broadcast more complex data at lower power (assuming the aliens were still pursuing a frugal strategy). The Benfords suggested looking at a broad range of radio signals in the 1- to 10-gigahertz range, where the travel of light is relatively unimpeded by interstellar matter. Currently SETI is focused on just the 1-to-2 gigahertz region , where components of water such as hydrogen and hydroxyl (a compound of hydrogen and oxygen) emit radio signals. "The idea is actually based mostly on how all the electronics of the 1960s, when SETI was first starting out, only operated in the range of a few gigahertz," Gregory Benford explained. "Now they operate in a range of up to 100 or 200 gigahertz, so it's a reason to revisit our assumptions. Looking to 10 gigahertz makes sense, since it's cheaper by a factor of 10 to build a transmitter at 10 gigahertz than at 1 gigahertz." Looking inward The Benfords also said that instead of gazing at stars within 500 or so light-years, as most SETI efforts have done for decades, observers should point more toward our galaxy's center to distances up to 1,000 light-years from Earth, where 90 percent of the galaxy's stars are clustered. "The stars there are a billion years older than our sun, which suggests a greater possibility of contact with an advanced civilization than does pointing SETI receivers outward to the newer and less crowded edge of our galaxy," Gregory Benford said. Although the galactic center is home to many bursting stars whose explosions are expected to sterilize the space around them, "the vast bulk of stars in the 28,000 light-years between Earth and the galactic center are not in sterilizing environments," Benford told SPACE.com. "For an analogy, you wouldn't want to hang out in Times Square all the time, but if you live in New Jersey it's obvious that Manhattan is the place to look for the action." "Will searching for distant messages work? Is there intelligent life out there? The SETI effort is worth continuing, but our common-sense beacons approach seems more likely to answer those questions," Benford said. Have we seen a beacon? One possibility of an extraterrestrial beacon is a puzzling transient radio source some 26,000 light-years from Earth that was discovered in 2002 in the direction of the galactic center. It sends out radio waves in bursts lasting up to 10 minutes in a 77-minute cycle. Scientists have suggested the source, labeled GCRT J17445-3009, is a flare star, an extrasolar planet, a pulsar or a brown dwarf, but none of these explanations fits well, the Benfords said. Based on the burst length and the short cycle, the Benfords doubt GCRT J17445-3009 is a beacon aimed at possible civilizations among a large crowd of stars. Still, if GCRT J17445-3009 is artificial in nature, it could be a signal that aliens pointed just at us, having detected signs of life from our planet. If that is the case, the Benfords said, we may want to pay closer attention to its signals to look for hidden details. On the other hand, GCRT J17445-3009 could be one link in an interstellar communications network. If so, it would make sense to look in the opposite direction, to see if another beam was communicating at it. "We studied GCRT not because we really think it's a beacon, but because it's an interesting way to look at similar bursting sources," Gregory Benford said. "There's the famous 'Wow' signal from 1977, for instance, that involved an enormous amount of power, and that there's still no good explanation for." Calling aliens Instead of just searching for extraterrestrial intelligence, the Benfords also considered messaging to extraterrestrial intelligences , or METI. They calculated that a galactic-scale beacon, with an antenna roughly a half-mile (0.9 km) wide with a range of a little more than 1,000 light-years, could be built for $1.3 billion. It would cost $200 million annually to operate. To work economically, it would use only narrow, high-power microwave beams and 35-second bursts aimed at each target star. "Of course, if you want to send a message, first you have to find a billionaire for this," Gregory Benford told SPACE.com. He noted he has spoken with a number of billionaires, including former Microsoft chief technology officer Paul Allen and Amazon.com founder Jeff Bezos, "and everyone has the same remark — that they would rather spend a billion dollars a different way." Copyright © 2010 Space.com. All Rights Reserved. This material may not be published, broadcast, rewritten or redistributed. URL https://www.foxnews.com/science/want-to-call-aliens-keep-it-short-and-simple-scientists-say

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/video/4002759760001
### Query Relevance Score: 0.045530401170253754
### Highlights: None

Home Watch Live Shows Topics Join the conversation Log in to comment on videos and join in on the fun. Watch Live TV Watch the live stream of Fox News and full episodes. Reduce eye strain and focus on the content that matters. This video is playing in picture-in-picture. Live Now All times eastern NOW - 3:30 AM 3:30 AM 4:00 AM 4:30 AM 5:00 AM 5:30 AM Fox News Channel The Ingraham Angle 3:00 AM - 4:00 AM Fox & Friends First 4:00 AM - 5:00 AM Fox & Friends First 5:00 AM - 6:00 AM Fox Business Channel Paid Programming 3:00 AM - 3:30 AM Paid Programming 3:30 AM - 4:00 AM American Dream Home 4:00 AM - 4:30 AM American Dream Home 4:30 AM - 5:00 AM Fox News Radio FOX News Radio Live Stream

---

## Article published: 01/2015
### Source URL: https://www.foxnews.com/science/worlds-most-powerful-laser-fires-most-powerful-laser-blast-in-history.print
### Query Relevance Score: 0.042691707611083984
### Highlights: None

By , Published January 08, 2015 next Before each experiment, a positioner precisely centers the target inside the target chamber and serves as a reference to align the laser beams. (Lawrence Livermore National Laboratory) prev The final optics assemblies, shown here mounted on the lower hemisphere of the target chamber, contain special optics for beam conditioning, color conversion, and color separation. They also focus the beams from 40- by 40-centimeter squares of light to a spot on the target only 0.2 to 2 millimeters in diameter. (Lawrence Livermore National Laboratory) The largest laser in the world was turned on for a fraction of a second last week -- and it unleashed the most powerful laser blast in history. The National Ignition Facility (NIF) -- a laser test facility at Lawrence Livermore National Laboratory in Livermore, Calif. -- turned on its 192 laser beams for a brief instant on March 15, unleashing a record-setting 1.875-megajoule blast into a target chamber. The lasers were combined, gathered and focused through a series of lens into a 2.03-megajoule shot, said Ed Moses, NIF director -- a record for the facility. That pulse of energy lasted for just 23 billionths of a second, yet it generated 411 trillion watts of power, NIF said -- 1,000 times more than the entire United States consumes at any given instant. “It’s a remarkable demonstration of the laser from the standpoint of its energy, its precision, its power, and its availability,” Moses told Nature magazine. But it’s barely half the battle. NIF hopes to dramatically increase the power of the laser shots by the end of year, intending to ultimately use the facility to harness the energy reaction that occurs naturally within the sun: fusion. “This event marks a key milestone in the National Ignition Campaign’s drive toward fusion ignition,” Moses said. In fission, atoms are split and the massive energy released is captured. The NIF aims for fusion, the ongoing energy process in the sun and other stars where hydrogen and helium nuclei are continually fusing and releasing enormous amounts of energy. In the ignition facility, beams of light converge on pellets of hydrogen isotopes to create a similar, though controlled, micro-explosion. As the beams move through a series of amplifiers, their energy increases exponentially. From beginning to end, the beams' total energy grows from one-billionth of a joule to a potential high of four million joules, NIF said -- a factor of more than a quadrillion. And it all happens in about five millionths of a second. Because the laser is on for the merest fraction of a second, it costs little to operate -- between $5 and $20 per blast, said spokeswoman Lynda Seaver. But the potential is enormous. NIF’s managers hope by the end of the year to reach a break-even point, where the energy released is equal to if not greater than the energy that went into the blast. “We have all the capability to make it happen in fiscal year 2012,” Moses told Nature. Experts aren't so sure, citing challenges that NIF and other types of fusion have had in the past. Glen Wurden, a plasma physicist at Los Alamos National Laboratory in New Mexico, told Nature scientists should be wary of putting all their eggs in the laser basket. “It’s premature right now,” he told the magazine, citing the troubles that have plagued a competing approach to fusion and its flagship project in France. URL https://www.foxnews.com/science/worlds-most-powerful-laser-fires-most-powerful-laser-blast-in-history

---

## Article published: 01/2015
### Source URL: https://www.wired.com/2015/01/ai-arrived-really-worries-worlds-brightest-minds/
### Query Relevance Score: 0.13933342695236206
### Highlights: None

On the first Sunday afternoon of 2015, Elon Musk took to the stage at a closed-door conference at a Puerto Rican resort to discuss an intelligence explosion. This slightly scary theoretical term refers to an uncontrolled hyper-leap in the cognitive ability of AI that Musk and physicist Stephen Hawking worry could one day spell doom for the human race. That someone of Musk's considerable public stature was addressing an AI ethics conference—long the domain of obscure academics—was remarkable. But the conference, with the optimistic title "The Future of AI: Opportunities and Challenges," was an unprecedented meeting of the minds that brought academics like Oxford AI ethicist Nick Bostrom together with industry bigwigs like Skype founder Jaan Tallinn and Google AI expert Shane Legg. Musk and Hawking fret over an AI apocalypse, but there are more immediate threats. In the past five years, advances in artificial intelligence—in particular, within a branch of AI algorithms called deep neural networks—are putting AI-driven products front-and-center in our lives. Google, Facebook, Microsoft and Baidu, to name a few, are hiring artificial intelligence researchers at an unprecedented rate, and putting hundreds of millions of dollars into the race for better algorithms and smarter computers. AI problems that seemed nearly unassailable just a few years ago are now being solved. Deep learning has boosted Android's speech recognition, and given Skype Star Trek-like instant translation capabilities. Google is building self-driving cars, and computer systems that can teach themselves to identify cat videos. Robot dogs can now walk very much like their living counterparts. "Things like computer vision are starting to work; speech recognition is starting to work There's quite a bit of acceleration in the development of AI systems," says Bart Selman, a Cornell professor and AI ethicist who was at the event with Musk. "And that's making it more urgent to look at this issue." Given this rapid clip, Musk and others are calling on those building these products to carefully consider the ethical implications. At the Puerto Rico conference, delegates signed an open letter pledging to conduct AI research for good, while "avoiding potential pitfalls." Musk signed the letter too. "Here are all these leading AI researchers saying that AI safety is important," Musk said yesterday. "I agree with them." Google Gets on Board Nine researchers from DeepMind, the AI company that Google acquired last year, have also signed the letter. The story of how that came about goes back to 2011, however. That's when Jaan Tallinn introduced himself to Demis Hassabis after hearing him give a presentation at an artificial intelligence conference. Hassabis had recently founded the hot AI startup DeepMind, and Tallinn was on a mission. Since founding Skype, he'd become an AI safety evangelist, and he was looking for a convert. The two men started talking about AI and Tallinn soon invested in DeepMind, and last year, Google paid $400 million for the 50-person company. In one stroke, Google owned the largest available talent pool of deep learning experts in the world. Google has kept its DeepMind ambitions under wraps—the company wouldn't make Hassabis available for an interview—but DeepMind is doing the kind of research that could allow a robot or a self-driving car to make better sense of its surroundings. That worries Tallinn, somewhat. In a presentation he gave at the Puerto Rico conference, Tallinn recalled a lunchtime meeting where Hassabis showed how he'd built a machine learning system that could play the classic '80s arcade game Breakout. Not only had the machine mastered the game, it played it a ruthless efficiency that shocked Tallinn. While "the technologist in me marveled at the achievement, the other thought I had was that I was witnessing a toy model of how an AI disaster would begin, a sudden demonstration of an unexpected intellectual capability," Tallinn remembered. Deciding the dos and don'ts of scientific research is the kind of baseline ethical work that molecular biologists did during the 1975 Asilomar Conference on Recombinant DNA, where they agreed on safety standards designed to prevent manmade genetically modified organisms from posing a threat to the public. The Asilomar conference had a much more concrete result than the Puerto Rico AI confab. At the Puerto Rico conference, attendees signed a letter outlining the research priorities for AI—study of AI's economic and legal effects, for example, and the security of AI systems. And yesterday, Elon Musk kicked in $10 million to help pay for this research. These are significant first steps toward keeping robots from ruining the economy or generally running amok. But some companies are already going further. Last year, Canadian roboticists Clearpath Robotics promised not to build autonomous robots for military use. "To the people against killer robots: we support you," Clearpath Robotics CTO Ryan Gariepy wrote on the company's website.

---

## Article published: 01/2015
### Source URL: https://www.wired.com/insights/2015/01/google-and-elon-musk-good-for-humanity/
### Query Relevance Score: 0.1372062712907791
### Highlights: None

Skip Article Header. Skip to: Start of Article. esenkartal/Getty The recently published Future of Life Institute (FLI) letter “Research Priorities for Robust and Beneficial Artificial Intelligence,” signed by hundreds of AI researchers in addition to Elon Musk and Stephen Hawking, many representing government regulators and some sitting on committees with names like “Presidential Panel on Long Term AI future,” offers a program professing to protect the mankind from the threat of “super-intelligent AIs.” In a contrarian view, I believe that should they succeed, rather than upcoming salvation we will see a 21st century version of 17th century Salem Witch trials instead, where technologies competing with AI will be tried and burned at stake with much fanfare and applause from mainstream press. Before I proceed to my concerns, here’s some background on AI. For the last 50 years AI researchers have promised to deliver intelligent computers, which always seem to be five years in the future. For example, Dharmendra Modha, in charge of IBM’s Synapse “neuromorphic” chips, claimed two or three years ago that IBM “will deliver computer equivalent of human brain” by 2018. I have heard this echoed of in statements of virtually all recently funded AI and Deep Learning companies. The press accepts these claims with the same gullibility it displayed during Apple Siri’s launch, and hails arrival of the “brain like” computing as a fait accompli. This is very far from the truth. The investments on the other hand are real, with old AI technologies dressed up in new clothes of “Deep Learning.” In addition to acquiring Deep Mind, Google hired Geoffrey Hinton’s University of Toronto team as well as Ray Kurzweil, whose primary motivation for joining Google Brain seems to be the opportunity to upload his brain into vast Google supercomputer. Baidu invested $300M in Stanford University’s Andrew Ng Deep Learning lab, Facebook and Zuckerberg personally invested $55M in Vicarious and hired Yann LeCun, the “other” deep learning guru. Samsung and Intel invested in Expect labs and Reactor, and Qualcomm made a sizable investment in BrainCorp. While some progress in speech processing and image recognition will be made, it will not be sufficient to justify lofty valuations of recent funding events. While my background is in fact in AI, I worked for last few years closely with the preeminent neural scientist Walter Freeman at UC Berkeley on a new kind of wearable personal assistant, one based not on AI but on neural science. During this time, I came to the conclusion that symbol-based computing technologies, including point-to-point “deep” neural networks (not neural science) can not possibly deliver on claims made by many of these well funded AI labs and startups. Here are just three of the reasons: Every single innovation in evolution of vertebrate brains was due to advances in organism locomotion, and none of the new formations indicate the emergence of symbol processing in cortex. Human intelligence is a product of resonating, coupled electric fields produced by massive population of neurons, synapses and ion channels of cortex resulting in dynamic, AM modulated waves in gamma and beta range, not static point-to-point neural networks. Human memories are formed in hippocampus via “phase precession” of theta waves which transform time events into spatial domain without use of symbols like time stamps. Each of the above three empirical findings invalidates AI’s symbolic, computation approach. I could provide more but it is hard to fight prevalent cultural myths perpetuated by mass media. Movies are a good example. At the beginning of the movie Transcendence, Johnny Depp’s character, an AI researcher (from Berkeley) makes the bold claim that “just one AI will be smarter than the entire population of humans that ever lived on earth.” By my calculation this estimate is incorrect today by almost 20 orders of magnitude — it will take more than a few years to bridge this gap. Which brings me back to the FLI letter. While individual investors have every right to lose their assets, the problem gets much more complicated when government regulators are involved. Here are the the main claims of the letter I have a problem with (quotes from the letter in italics): Statements like: “There is a broad consensus that AI research is progressing steadily,” even “progressing dramatically” (Google Brain signatories on FLI web site), are just not true. In the last 50 years there has been very little AI progress (more stasis like than “steady”) and not a single major AI based breakthrough commercial product, unless you count iPhone’s infamous Siri. In short, despite the overwhelming media push, AI simply does not work. “AI systems must do what we want them to do” begs the question of who is “we?” There are 92 references included in this letter, all of them from CS, AI and political scientists, there are many references to approaching, civilization threatening “singularity,” several references to possibilities for “mind uploading,” but not a single reference from a biologist or a neural scientist. To call such an approach to study of intellect “interdisciplinary” is just not credible. “Identify research directions that can maximize societal benefits” is outright chilling. Again, who decides whether research is “socially desirable?” “AI super-intelligence will not act with human wishes and will threaten the humanity” is just a cover for justification of the attempted power grab of AI group over the competing approaches to study of intellect. Why should government regulators support technology that has failed to deliver on its promises repeatedly for 50 years? Newly emerging branches of neural science, which made major breakthroughs in recent years, are of much greater promise, in many cases exposing glaring weaknesses of AI approach. So it is precisely these groups which will suffer if AI is allowed to “regulate” the direction of future research of intellect, whether human or “artificial.” Neural scientists study actual brains with imaging techniques such as fMRI, EEG, ECOG, etc and then postulate predictions about their structure and function from the empirical data they gathered. The more neural research progresses, the clearer it becomes that brain is vastly more complex than we thought just a few decades ago. AI researchers, on the other hand, start with the a priori assumption that the brain is quite simple, really just a carbon version of a Von Neumann CPU. As Google Brain AI researcher and FLI letter signatory, Illya Sutskever, recently told me, “[The] brain absolutely is just a CPU and further study of brain would be a waste of my time.” This is almost word for word repetition of famous statement of Noam Chomsky made decades ago “predicting” the existence of a language “generator” in the brain. FLI letter signatories say: Do not to worry, “we” will allow “good” AI and “identify research directions” in order to maximize societal benefits and eradicate diseases and poverty. I believe that it would be precisely the newly emerging neural science groups which would suffer if AI is allowed to regulate research direction in this field. Why should “evidence” like this allow AI scientists to control what biologists and neural scientists can and cannot do? It is quite possible that signatories’ motives are pure. But at the moment the AI lobby has a near monopoly on forming public opinion and attracting government dollars through the influence of compliant media. Indeed government regulators in this space are all AI researchers, often funding AI startups with taxpayer dollars, and later taking up jobs with the very same companies they funded and were supposed to regulate. And often, when government regulators lead, private VC funds follow in a “Don’t fight the Fed,” sheep-like movement. There is yet another dimension to this story: In addition to threat of upcoming “singularity” FLI letter reference section has many references to “mind uploading.” After life-time of immersion in Von Neumann architectures, from Ray Kurzweill and Peter Thiel on, many silicon valley prodigies are obsesses with the idea of becoming immortal via mind upload into silicon. Threat of death is a powerful emotion indeed, but it belongs in realm of religious thinking rather than “dispassionate and objective science” they profess to advocate. Let me conclude with another movie quote: At the end of movie Beautiful Mind, mathematician John Nash played by Russell Crowe, recovering from mental illness, lectures a group of students in a cafeteria: “Trust mathematics, trust your teachers,” then pauses and adds with a wink: “Just stay away from biologists, do not trust those guys.” Indeed, today’s AI researchers are all children of Rene Descartes, trusting in absolute power of logic and mathematics as they push their religion of Cartesian Dualism on the rest of us. Inadvertently, they tell us all to drink AI Kool-Aid in their “SkyNet is Coming” sermon. I believe that neural science and biology utilizing wearable sensors is already much more fruitful than AI in delivering personal assistants guiding us through daily life, keeping us healthier and stress free, based on better understanding of brain, rather than logic of CPU programing and algorithms of AI focused on weapons and robotics. I hope US press will arise to a defense of scientists rights to continue to perform such free research rather then limiting their research to work on “desirable” results. As famous US journalist once said: “Those who sacrifice liberty for security deserve neither.” Roman Ormandy is the founder of Embody Corp. Go Back to Top. Skip To: Start of Article.

---

## Article published: 01/2015
### Source URL: https://www.wired.com/insights/2015/01/innovation-takes-the-exponential-express/
### Query Relevance Score: 0.1294884979724884
### Highlights: None

We’re all familiar with Moore’s Law: the observation that the number of transistors in an integrated circuit doubles every two years or so. We’re also familiar with the plethora of corollaries to the law, pertaining to everything from network speed to hard drive capacity to the number of pixels in our digital cameras. It seems that the natural behavior for technology advancement follows an exponential growth curve. However, not all innovation follows such a curve. Organizational and process improvements, in particular, seem to proceed at a glacial pace. Management fads come and go, and they don’t even seem to be getting much better, let alone better at a faster rate. In development shops, the Agile Manifesto is now fifteen years old or so, and yet organizations still struggle with it. Where’s Moore’s Law when you need it, eh? The problem today’s organizations face is that digital transformation relies both upon technology advancement as well as organizational innovation – and yet, it seems our inability to accelerate the latter may be putting the whole kit and caboodle in jeopardy. If only we could bottle up the secret to Moore’s Law and pass the bottle around. It’s true that Moore’s law is about fifty years old, more than a lifetime for most of the people who have been riding the exponential express for their entire careers. Yet, while such exponential behavior is now all around us, one question strangely appears to be as yet unanswered: why. Kurzweil’s Key Why? What is it about technology innovation that naturally proceeds along an exponential path? Note first of all that calling Moore’s Law a law is a misnomer; in reality it is simply an observation. An observation so ingrained now in the planning of technology innovators that perhaps it has become self-fulfilling – but such a sui generis explanation is certainly no explanation at all. I had the great fortune of asking Ray Kurzweil this very question at a recent conference. Kurzweil, of course, is a noted inventor, futurist, and deep thinker who has been writing about the exponential pattern of technology innovation for many years now, what he calls the law of accelerating returns . He has extended his argument well into the future, leading him to theorize about computers that get so smart they completely swamp our human intelligence, or perhaps to technology so advanced we’re able to augment our brains with it. Fascinating stuff, to be sure, especially to us lifelong science fiction fans. And while some of his prognostications sound outlandish, there’s no arguing with the exponential behavior of technology innovation – and once you get your head around the wheat on the chessboard lessons of such growth, one would be hard pressed not to allow Kurzweil a good measure of leeway in his predictions. Our focus here at Intellyx, however, is more short term. Planning for the next year or two is difficult enough without worrying about whether we’ll be able to back up our consciousnesses to the cloud before we die of old age. Let us therefore return to the question of the day: why? Kurzweil’s answer is essentially to point to the feedback loop intrinsic to all exponential innovation. Essentially, each generation leverages the one before. Intel 386 processors enabled the brilliant minds at Intel to create the 486, and then the Pentium, and so on. The same with memory capacity and Ethernet speeds and so on ad infinitum. Perhaps. Yes, feedback loops do seem to be part of the answer, but they don’t give me the warm fuzzies I would require in order to believe they are the whole story. What is it about human innovation that requires such steadfast adherence to each exponential curve? Furthermore, what happens when exponential curves give out? After all, transistors can only get so small. What happens when Moore’s Law drives right past the single-atom transistor and keeps on going? Moore’s Law Before Moore Kurzweil, as you might expect, has thought long and hard about this question. In fact, he extends Moore’s Law into the past, well before the 1971 invention of the integrated circuit, as shown in the following diagram. Moore’s Law Extended Back to 1900 (Source: Kurzweil AI) As electromechanical calculating machines gave way to relay-based computers and then vacuum tubes and eventually transistors, Kurzweil’s favored metric of calculations per second per dollar stuck closely to an exponential curve (represented as a straight line on this logarithmic scale) – even curving upward a bit. In other words, just as a particular computing approach (what Kurzweil calls a paradigm) runs out of steam, human innovation comes up with another just in the nick of time. Again my question: why? Feedback loops don’t help us answer this question now. The inventors of the transistor didn’t look at the fact that vacuum tube technology had reached its limit in order to schedule the timing of their invention. Human innovation doesn’t work that way. In other words, disruptive innovation and incremental innovation are at opposite ends of a spectrum – and yet, still adhere to the same curve. Such adherence can’t be coincidence. There must be some underlying principle of human innovation – or broadly speaking, human behavior – that drives both ends of this spectrum of innovation. I don’t think Kurzweil has an answer to this question. At least, several days of poking around his writing haven’t uncovered an answer. (Ray, drop me a line if I’ve missed something!) The good news: I believe I’ve cracked this nut. In fact, I approached this question from an entirely different angle in an earlier Cortex where I discussed the nature of disruptive innovation. The insight necessary to answer the question of why depends upon treating any large group of people as a complex adaptive system, where innovativeness is one of the emergent properties of that system. As I explained in that Cortex, shifting such a system from high connectivity to low connectivity increases evolutionary change. Furthermore, disruptions also lead to increasing rates of evolutionary change, in particular when the system exhibits low connectivity. Such change leads to adaptation to the disruptions in the environment, so combining low connectivity with disruptions leads to periods of rapid innovation and adaptation. Emergent behavior essentially appears as patterns out of patternlessness. Take a bunch of independent actors (scientists, business people, engineers, etc.), give them a number of external motivations (the drive for profit, the human need to create something new, and so on), as well as various resources – without the high connectivity, rigid reporting structures of traditional hierarchical organizations – and stir. The end results are specific, repeatable patterns we can take to the bank: in this case, the exponential growth of the law of accelerating returns. Extending Exponential Innovation to the Organization The law of accelerating returns applies to many evolutionary processes entirely separate from the behavior of members of Homo sapiens. In fact, Kurzweil points to the evolution of our species itself as an example, from single-celled life to the creation of DNA to the Cambrian Explosion, right on up to the evolution of sentience – a pattern that also follows an exponential curve. That being said, even for the most technical of innovations, the most important component systems of the complex system of systems we call an organization are people, not technology subsystems. Innovation, after all, is a human endeavor. Technology innovation is itself a set of organizational processes. What all of these processes have in common – a property, in fact, of all complex systems – is self-organization. Systems as large as galaxies and as small as the cells in your body self-organize. Sentience isn’t required – and paradoxically, often gets in the way, as I’ve discussed in a recent article on self-organizing teams. Sometimes, however, these well-dressed primates walking the planet get it right. People ask Google their secret to innovation. The answer: self-organization. The same for NetFlix. And if they can do it, so can you. Whether such self-organization will lead to exponential improvements, however, is an open question. It’s essential to the continuous delivery model that characterizes DevOps – but progress at even the best-run DevOps organizations can only go so fast. Clearly there’s more to applying the law of accelerating returns to organizational change – but that discussion will have to wait till a future Cortex newsletter. Stay tuned. Jason Bloomberg is President of Intellyx. Go Back to Top. Skip To: Start of Article.

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/politics/2015/01/05/deep-learning-gaze-into-the-web-abyss-and-it-gazes-also-into-you/
### Query Relevance Score: 0.1009039431810379
### Highlights: None

Americans are accustomed to the dominance of Google, Microsoft, and Yahoo as search engines, but on the global stage, a Chinese service called Baidu is now second only to Google in popularity. Baidu hasn’t announced any firm plans to move into the U.S. market yet, but, in addition to the huge market in China, they’ve expanded services to countries such as Egypt, Thailand, and most recently Brazil. They’ve also recently hired away one of Google’s top researchers, Andrew Ng, a specialist in artificial intelligence who has taught courses at Stanford University. As Ng explained in an interview with VentureBeat, Baidu’s claim to fame is their search engine’s reliance on “deep learning” algorithms. In other words, their products learn what users want by analyzing their requests and what they do with the information. To some extent, every search engine and social media platform is trying to learn from its users, producing a certain degree of discomfort among those who feel their Internet tools are spying on them. The deep learning movement wants to take this idea of “living” software further, creating systems that digest enormous amounts of data very quickly, without much human intervention, producing highly customized experiences for each individual user. It is hoped that the benefits from this level of swift, automatic customization, and the fact that it’s all being done by faithful, computerized servants instead of intrusive, human programmers, will overcome consumer unease. One might also suppose that much of the early work is being carried out with customers who are, for better or worse, less nervous about having their online activities monitored than Americans and Europeans. If companies like Baidu can realize the ambitious plans of visionary designers like Ng and bring polished, incredibly useful living software perfected overseas to American audiences, consumer resistance could be minimal. As the old saying goes, nothing succeeds like success. The big American tech companies are also interested in developing this kind of technology, and while power players like Google are certainly swimming in resources, it doesn’t take much reading between the lines to get the idea that Ng left Google because they’ve become too hidebound and bureaucratic. Ng talks about receiving needed resources much faster than his unit at Google provided, getting things done without sitting through tedious committee meetings. He can frolick through Chinese markets with much faster birth-death cycles for online products than is typical in the West, and he has a field day hiring away other top artificial intelligence researchers to join his team. To get an idea of what this deep learning software would be like, the VentureBeat article on Ng references the recent movie “Her,” a near-future science fiction film in which Scarlett Johansson voiced an artificially intelligent digital assistant whose tireless efforts to relate with her user, and enhance the quality of his life, blossomed into an actual romance. (Not to spoil anything for those who have not seen the film, but hopefully deep learning researchers are pondering the ultimate resolution of that romance at great length.) The key feature of these next generation systems over existing smart search engines and voice-activated smartphones is the causal grace of their relationship with users. Computer systems have grown steadily easier to use with each passing decade, long ago reaching the point of widespread consumer acceptance by growing friendly enough for average, non-techie folks to use reflexively on a daily basis. One indicator of how user-friendly consumer systems have become is that no one really talks about “user friendliness” any more. It was the sizzling-hot buzz phrase of the computer industry not very long ago, but it is now well-understood that if an application isn’t simple enough for most people to figure out with a casual glance and a few curious mouse clicks, it’s not going to break through to a mass audience. Cryptic text-based systems have given way to graphical user interfaces that didn’t really work—they tended to cripple the machines they were running on. Eventually graphical interfaces were perfected, and now they’re ubiquitous, running on handheld devices with incredibly convenient touch screens. Voice command is the next step, but even the most gee-whiz voice apps today, like Apple’s famed Siri, are just a shadow of what they could be. Greater accuracy is the key to making smart search engines and voice applications work, and that’s what Baidu hired Andrew Ng to work on. At the moment, the ability of online systems to adjust themselves automatically to suit user preferences is fairly crude. Much of the learning is based on what users request of a particular system… but people don’t always know what they really want. A deep learning system would study what they actually do with the data they request, build a network of preferences from many different data sources, consider the preferences of similar users, and learn to interpret the personal subtleties of spoken language. The personal assistant envisioned in “Her” displays these qualities—she can anticipate what her user wants based on very vague requests, she adjusts her behavior to meet his demonstrated preferences, she understands the nuances of spoken conversation (there are some cleverly written early interactions where she asks questions of her user to figure out when he’s being sarcastic, what he really means when he uses certain figures of speech, and so forth.) The ability to accurately pull meaningful data from the random noise of human behavior is crucial to making such features work. Living human beings don’t actually handle these tasks with a very high degree of accuracy, unless they know each other very well. The speed and power of artificially intelligent computer systems, combined with the endless patience of machine intelligence, could make them much better at becoming everyone’s close personal friend. With these capabilities perfected, computer systems would cross the final user-friendliness bridge and begin doing more than half of the work to maintain a relationship with humans. As it stands, even the most easy-to-use interfaces require us to learn how the system thinks—we have to figure out where the menus are, perhaps learn a few shortcut keys, learn the peculiarities of each system’s tools for such routine tasks as file uploads, and learn how we can configure the interface to suit our personal tastes. One reason we tend to think of modern systems as more user-friendly is that the typical user is far more comfortable with learning his way around an interface than his father was; an online generation is coming of age, and it’s more adventuresome and patient than the eighties and nineties executives who howled in frustration at the inscrutable but indispensable machines on their desks. When computer systems are able to shoulder most of the interaction burden, and we can use them as casually as we would request help from a trusted human assistant, the next computer revolution will be at hand. Artificial intelligence is still an argument—there are those who believe it will never be anything but an illusion, that talk of “neural networks” is just slick marketing for really fast computers. We currently have search engines that work extremely well; both user experiences and uninvited advertising have been tailored to individual preferences. Such tasks as piloting an automobile, routinely accomplished by millions of humans without extensive training or individual genius, remain beyond the capability of the smartest computer system; the ability of an incredibly complicated machine to briefly fool a panel of human judges into thinking they are holding a conversation with a human child is celebrated as a landmark A.I. triumph. The practical application of such technology remains elusive, and yesterday’s promised miracles remain science fiction. But then again, many electronic conveniences we currently take for granted were science fiction just twenty years ago. It’s interesting to watch where high-rolling tech companies are placing their bets, and they all seem convinced that deep learning systems are worth investing sizable sums of money in. The problem for American investors is that it doesn’t seem like our domestic giants are nimble and adventurous enough to keep up with the work being done by the big Chinese companies. At least, that’s how Andrew Ng placed his bets.

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/europe/2015/01/21/civil-war-brewing-for-the-cultural-left/
### Query Relevance Score: 0.05474191904067993
### Highlights: None

It has often been remarked that the right won the economic arguments of the twentieth century, while the left won the culture war. Although Thatcher and Reagan succeeded in their quest to overturn the postwar economic consensus and undermine the USSR, the left consistently triumphed over social conservatives in political debates on society and culture. Throughout the 1980s and the 1990s, the left successfully positioned itself as the guardian of liberty and reason against a dogmatic and authoritarian “moral majority”. Moderates and liberals could not understand why the right wanted to deny gay people the right to marry, or women the right to an abortion. Nor could they understand the conservative quest to pull the theory of evolution from primary schools, or the regular campaigns by conservative moral crusaders against filth, blasphemy and even Satanism (1, 2) in popular culture. Against such opponents, it was relatively easy for the left to position itself as the defenders of academic inquiry, artistic expression and personal freedom. But the sands are beginning to shift. The coalition of moderate liberals, sceptical intellectuals, and radical progressives that once stood together against the conservative “moral majority” is beginning to fracture. In the absence of a compelling external opponent, the internal tensions of this coalition are becoming more visible. While it is too soon to say if the revolution is about to consume itself, a number of serious divisions have emerged on the cultural left. And they are becoming increasingly bitter. Religion Richard Dawkins, Sam Harris, Daniel Dennett and Christopher Hitchens were once known as the “Four Horsemen” of New Atheism. For a long while, there was nothing more amusing to a young liberal than watching one of them debate against a creationist, or someone who objected to abortion or gay marriage on religious grounds. Dawkins, for a while, was the darling of the British media. Then things started to sour. Christopher Hitchens, in his full-throated defences of the second Iraq war, was the first to lose left-wing support. Notoriously, Feminist Frequency producer Jonathan McIntosh celebrated Hitchens’ death, saying he was a “despicable, warmongering, hateful human being. Good riddance.” (To put that in perspective, McIntosh had just a few months earlier refused to celebrate the death of Osama Bin Laden.) Dawkins, who recently discovered the joys of deliberately offending people on Twitter, has become an even greater figure of hate for progressives. This is probably due to his indiscriminate rationalism: he is just as willing to poke holes in theories of post-modern feminism as he is to attack religion. And when he does attack religion, he insists that Islam is probably the worst one out there. He has become persona non grata in progressive circles as a result. 2014 saw atheists and progressives embroiled in what looked like an all-out war. Ayaan Hirsi Ali, a female genital mutilation survivor and one of the fiercest critics of Islam in the atheist movement, was disinvited from a planned speaking engagement at Brandeis University for her criticism of Islam, and was stripped of her honorary degree. Salon.com immediately applauded the decision. Students at UC Berkeley attempted to do the same to Bill Maher over his alleged islamophobia, but were stopped by the college administration. Sam Harris, another of the “four horsemen”, felt compelled to engage in a three-hour debate with progressive commentator Cenk Uygur after enduring a wave of hatchet-jobs from media progressives for his own comments on Islam. Progressives may be overwhelmingly atheist, but there is only so much heresy they can stand. One of their core beliefs is that you do not “punch down”–that is, attack vulnerable or marginalised communities. Islam, despite being the dominant religion of dozens of nation-states, is said by progressives to fall into this category. But many atheists don’t buy it. And so they continue, with creationists to the right and progressives to the left, blaspheming against the beliefs of both. As Christianity declines and Islam grows, it is progressives, constantly impeding criticism of the latter, who may prove to be a bigger thorn in the side of atheism than conservatives ever were. Due process During the Bush administration, liberals eagerly positioned themselves as champions of the rights of the accused: specifically, those accused of plotting terrorism. For the left, Guantanamo Bay became a byword for a new authoritarian lawlessness, in which jury trials were a thing of the past and punishment was meted out on suspicion alone. These days, however, defenders of due process are more likely to be at loggerheads with radical progressives than Bush-era neocons. Nowadays, it is progressives, not conservatives, who championed the use of campus tribunals to deal with sexual assault on US campuses. These tribunals, conducted by untrained faculty members, with no requirement for defendants to have access to legal representation, have attracted a growing tide of criticism, as well as a number of lawsuits. In the UK, progressives have also lent their support to prosecutors’ efforts to interfere in the jury system, most recently by calling on judges to tackle the “unconscious biases” of juries. While progressives on Twitter and the blogosphere rejoice in the assault on due process, others on the left are not so enthusiastic. Last summer, 28 Harvard lawyers, most of them liberal, came out against the university’s attempts to hand control of sexual assault cases to a single “Title IX compliance officer.” In the aftermath of Rolling Stone’s disastrous story on a hoax rape claim at the University of Virginia, even more liberals–including Slate’s Emily Yoffe and former Salon writer Richar d Bradley–began to speak out against the new atmosphere of vigilantism and automatic credulity sometimes referred to as “listen and believe.” Meanwhile, progressives continue to call for “affirmative consent” laws that would redefine sexual assault to include any form of sexual contact that is not explicitly, verbally consented to. Ezra Klein, the arch-progressive editor of Vox, accepted that such laws are “terrible” and would convict people for “genuinely ambiguous situations” – but, incredibly, also said that this was OK. In order to prevent sexual assault, wrote Klein, it was necessary for the law to “create a world in which men are afraid.” Bad laws that could be enforced arbitrarily were, in his view, a great way of accomplishing that. Other liberals understandably recoiled in horror at this line of reasoning. As doubts about the data behind the “campus rape epidemic” grow, battle lines are being drawn between progressives like Klein and liberals like Yoffe, Bradley, and the Harvard lawyers. The crusade to change culture – the very heart of the progressive mission – is on a collision course with due process, and perhaps liberalism itself. Censorship The blood was scarcely cold on the corpses of the murdered Charlie Hebdo cartoonists before progressives began to call them racist. Among them was Jonathan McIntosh, our Hitchens-hating friend from the previous section, leading some bloggers to dub him “Jihad Jonathan.” But he wasn’t alone. Around the world, it soon became apparent that a number of news organisations, including Sky, CNN, the New York Times and the Daily Telegraph were refusing to run the cartoons. on the grounds of religious offence. It is understandable for these outlets to be afraid of people with guns. What is less understandable to many liberals is being afraid of people of people with petitions. The left-libertarian journal Spiked, which has become a focal point for people dissatisfied with the mainstream left, recently wrote a biting piece of satire outlining how a mix of student boycotts and Change.org petitions might have ended Charlie Hebdo altogether, had it been published in the UK. The article was shared over four thousand times. But it’s not just Brendan O’Neill who’s noticed the rise of the “Stepford Students”. His longtime opponent, the liberal columnist Nick Cohen has been warning about the very same thing. Chris Rock, also a liberal, recently revealed that he no longer performs comedy for student audiences, arguing that they were too “conservative” in the way they handled offensive content. His use of the word “conservative” is telling. For decades, it was social conservatives who put pressure on private institutions to censor material that offended them. Cinemas that screened Monty Python movies were boycotted, panics were stoked about the “satanism” of Dungeons & Dragons, and born-again Christians led campaigns against violent videogames. Today, however, it is progressives who are not just standing up for the right of private censorship, but also actively demand it. It is progressives, not Christian conservatives, who now lead campaigns against sex and violence in the media. And it was progressive students, not middle-aged moral crusaders, who banned a pop song on over 20 university campuses. The #GamerGate uprising was in part a reaction to this culture of censorship, and they have already helped protect two video games – the controversial shooter Hatred, and the farming simulator Seedscape – from attempted boycotts. But despite rabid opposition from the left, #GamerGate – like Nick Cohen, like Bill Maher, and like Richard Dawkins – continues to identify with liberalism. The pro-censorship left and the anti-censorship left know they will never convince each other. Both sides have begun to dig trenches. Academia I have left the most surprising arena of left-vs-left conflict for last. Academia has long been assumed by those on the right to be an impenetrable fortress of progressive dogma. But times are changing. Most intriguing of all is that it is in the social sciences where the most change is taking place. I remember my own surprise, as a young undergraduate, to find an entire module on evolutionary psychology in the social science reading list. I was also surprised to see professors excitedly recommending Daniel Kahneman’s Thinking, Fast and Slow to students. Most surprising of all was when my sociology tutor – a leading egalitarian theorist – informed our class that he thought social construction theory was “mostly wish-wash.” Those who are unfamiliar with the nature-vs-nurture debate will probably not understand why such a statement is controversial. Cognitive and genetic scientists, and evolutionary theorists, have long been viewed with suspicion by sociologists. After all, one of the chief projects of cognitive and evolutionary scientists in the past two decades has been the dismantling of the standard social science model, the theoretical framework that looks to external influences (nurture) to explain human behaviour, as opposed to genes or other innate factors (nature). If my university is anything to go by, however, even social scientists themselves are beginning to see flaws in the old model. This is bad news for progressives. The idea that human minds are infinitely malleable, and that the human behaviour can be altered simply by changing the social environment, underpins almost every progressive campaign – from No More Page Three to non-selective schooling. This is no accident: anyone who wishes to radically change the world must, on some level, believe that human nature can be altered. Frightened by the growing weakness of their flagship theories, progressives on campus have begun to lash out. One of the biggest controversies was in 2005, when then-President of Harvard University, Larry Summers, was faced with a motion of no confidence after suggesting that innate differences between the genders should be a line of inquiry when analysing the gender pay gap. The motion passed, and it left serious scars in the academic community. “Good grief, shouldn’t everything be within the pale of legitimate academic discourse?” asked Steven Pinker shortly after the controversy. “That’s the difference between a university and a madrasa.” There have been no comparable controversies since then, but a stream of outrage continues to follow the work of Pinker, Simon Baron-Cohen, Robert Plomin, Nicholas Wade and anyone else who investigates the idea of innate differences between persons and groups. It doesn’t matter how much they stress their commitment to liberalism and egalitarianism (which they do, frequently), nothing can calm their opponents. Academics aren’t usually minded to make snap political judgments on a whim. But there is a growing perception that some on the progressive left are just as committed to their dogmas as creationists are to theirs. With the standard social science model increasingly looking like a relic of the 1960s, the fierceness with which some progressives continue to cling to it will determine the fierceness of future divisions in academia, and, by extension, the left. The future of the culture wars As 21st-century progressives begin to embrace many of the tactics, arguments, and moral panics employed by 20th-century conservatives, the old distinctions in the culture wars begin to lose their relevance. In a number of arenas, the cultural left is ignoring conservatives and has begun to fight itself. The question for the right is: what to do? There is no doubt room for common ground with liberal atheists who want to criticize Islam, with liberal lawyers who want to protect due process and with content producers fighting the new, petition-led censorship. Opposition to political correctness and a respect for individual rights have always been strong traditions on the right. On the other hand, appealing to liberal, atheist, Grand Theft Auto V fans without losing the support of social conservatives could prove difficult. Nevertheless, it increasingly appears that cultural politics, once the great strength of the left-wing movement, is rapidly turning into its Achilles heel. Once a source of unity, it has turned into perhaps the primary source of division. With moderate liberals and radical progressives sharpening their weapons on a number of fronts, a battle for the soul of the left is about to begin.

---

## Article published: 01/2015
### Source URL: https://www.breitbart.com/europe/2015/01/02/how-feminist-propaganda-is-destroying-mens-lives/
### Query Relevance Score: 0.052497051656246185
### Highlights: None

Otherwise baffled as to why the STEM fields of science, technology, engineering and maths aren’t crammed full of women like psychology, social work and education, feminist activists have been leading a noisy campaign over the past few years to paint men in these fields, entirely inaccurately, as raging misogynists – even going to the media with the word “brogrammer,” to convey their annoyance. While the most gullible media outlets took the bait, even left-leaning Gizmodo hit back with a piece called, “There’s No Such Thing as a Brogrammer”. Nagging and haranguing about supposed sexism has only been getting louder, the pinnacle of which was reached when scientist Dr Matt Taylor was reduced to apologising in tears after wearing a mildly risqué shirt on camera to celebrate his achievement of landing a spacecraft on a comet. On Twitter, a woman proclaimed that the shirt had signalled that women “weren’t welcome in science,” which over 1,000 of the usual suspects retweeted with feverish enthusiasm. One computer scientist, Scott Aaronson, took issue with the overwhelming narrative being cooked up that men working in the STEM fields were misogynist pigs with no knowledge of the necessary and beautiful art of feminist discourse, and, writing a very personal account of his adolescence and youth, he described reading dozens of feminist books, “studies and task reports about the ‘privilege’ and ‘entitlement’ of the nerdy males that’s keeping women away from science” and feminist blogs. Comment #171 subsequently gained a lot of attention on Twitter, Facebook and Reddit. From when he was 12, up until his mid-twenties, far from the “entitled” attitude to women that feminists accuse men working in STEM of having, Aaronson, having read these books, having been to “sexual-assault prevention workshops” as an undergrad, was terrified of even talking to women so fearful was he his intentions could be misconstrued and have him deemed a demonic rape fiend. Desperate at this point to have been born a woman or a gay man so he didn’t have to bear the burden of being a heterosexual white male, seeping malevolent privilege and doomed to stalk the planet oppressing all those to cross his path, Aaronson “scoured the feminist literature for any statement to the effect that [his] fears were as silly as [he] hoped they were,” only to find the opposite. “I found reams of text about how even the most ordinary male/female interactions are filled with “microaggressions,” and how even the most “enlightened” males—especially the most “enlightened” males, in fact—are filled with hidden entitlement and privilege and a propensity to sexual violence that could burst forth at any moment.” Suicidal, such was Aaronson’s trauma, he even tried to acquire a prescription for chemical castration, so that he could focus on devoting his entire life to maths and have no reason to be seen as a predator. Aaronson ends his piece with the by poignantly noting that while, no, the trauma of sexual assault and the trauma he went through in his 15 years of “life-destroying anxiety” weren’t comparable but that with his, “there are no academics studying it, no task forces devoted to it, no campus rallies in support of the sufferers, no therapists or activists to tell you that you’re not alone or it isn’t your fault. There are only therapists and activists to deliver the opposite message: that you are alone and it is your privileged, entitled, male fault.” While obviously tragic for him on a personal level, Aaronson’s blog comment has wider, more serious and indeed urgent implications. Universities are rolling out these sexual consent workshops, which will likely become compulsory. Aaronson said he left each of those workshops “with enough fresh paranoia and self-hatred to last through another year,” and there is a petition by the Everyday Sexism and End Violence Against Women Coalition to have them made compulsory in all schools. Terrifyingly, the petition sets out a curriculum of “sexual consent, healthy and respectful relationships, gender stereotypes and online pornography”. Given that for feminists, there is only one possible viewpoint to take on all these things, the petition basically ensures that third-wave feminism is beamed straight into children’s heads. The scariest thing about this petition is that apart from David Cameron and Nigel Farage, all the other party leaders have wholeheartedly backed it. Aaronson isn’t alone in being negatively affected by feminist propaganda. He received many grateful emails from guys in the same position, for giving them hope they too could one day make a success of themselves, and when I read Scott’s piece, I was immediately put in mind of a man in his 20s I knew through the games platform Steam, in 2011. Having lurked on the internet a lot and got sucked into reading feminist blogs and websites for days and weeks on end, this man since his late teens had suffered from such crippling social anxiety he was living on disability benefits and unable to leave the house. If children throughout the country are to be conditioned in schools with this sort of poisonous thinking, this kind of tragedy will become far more commonplace. Yet instead of thinking perhaps feminists would be able to learn from cases such as Aaronson’s, the response he got from them was brutal. Amanda Marcotte claimed his post was a “yalp of entitlement” from someone who saw women only as sex toys, and her piece was met with rapturous applause. Laurie Penny, whose response was deemed by many to be too kind, rather than actually showing any empathy instead managed to talk about how hard it was to be a “horny”, “socially awkward”, nerdy girl growing up, and that as a female, when she wanted to enter a “life of the mind” she found the patriarchy standing in her way. I find this curious, since women are a third more likely to go to university, get well over half of all Master’s degrees and just over half of all PhDs. But anyway, the upshot is, that Aaronson’s problems with social anxiety were caused by, according to Penny… the patriarchy! Aaronson’s trauma was completely dismissed and replaced with a diatribe about her own experiences growing up, and then she completely ignored what had actually caused his anxiety and twisted it round to pretend the culprit was the ever imaginary “patriarchy”. Reading Penny’s piece and, especially, Marcotte’s gives an unsettling window into the minds of modern feminists. Even when a man reveals his vulnerabilities and writes with heartbreaking honesty about a cripplingly awful period of his life, they seem pathologically unable to see why he was suffering, and, in the case of Marcotte, that he was suffering at all. It’s as though unless men and boys adhere to their exact way of thinking, they’re inhuman and unworthy of empathy. Rather than filling more young people’s heads with the acrid fantasies of third wave feminists that foster distrust and disquiet between the sexes, venomous ideas like “privilege theory” and “rape culture” should go the way of the woolly mammoth. Only then can we be closer to equality.

---
