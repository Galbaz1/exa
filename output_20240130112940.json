{
    "question": "https://www.wired.com/story/artificial-intelligence-animals-anthropomorphism/",
    "full_extract": "https://www.scientificamerican.com/article/artificial-intelligence-could-finally-let-us-talk-with-animals/ Artificial Intelligence Could Finally Let Us Talk with Animals 2023-10-01 Lois Parshley nderneath the thick forest canopy on a remote island in the South Pacific, a New Caledonian Crow peers from its perch, dark eyes glittering. The bird carefully removes a branch, strips off unwanted leaves with its bill and fashions a hook from the wood. The crow is a perfectionist: if it makes an error, it will scrap the whole thing and start over. When it's satisfied, the bird pokes the finished utensil into a crevice in the tree and fishes out a wriggling grub.The New Caledonian Crow is one of the only birds known to manufacture tools, a skill once thought to be unique to humans. Christian Rutz, a behavioral ecologist at the University of St Andrews in Scotland, has spent much of his career studying the crow's capabilities. The remarkable ingenuity Rutz observed changed his understanding of what birds can do. He started wondering if there might be other overlooked animal capacities. The crows live in complex social groups and may pass toolmaking techniques on to their offspring. Experiments have also shown that different crow groups around the island have distinct vocalizations. Rutz wanted to know whether these dialects could help explain cultural differences in toolmaking among the groups.New technology powered by artificial intelligence is poised to provide exactly these kinds of insights. Whether animals communicate with one another in terms we might be able to understand is a question of enduring fascination. Although people in many Indigenous cultures have long believed that animals can intentionally communicate, Western scientists traditionally have shied away from research that blurs the lines between humans and other animals for fear of being accused of anthropomorphism. But with recent breakthroughs in AI, \u201cpeople realize that we are on the brink of fairly major advances in regard to understanding animals' communicative behavior,\u201d Rutz says.Beyond creating chatbots that woo people and producing art that wins fine-arts competitions, machine learning may soon make it possible to decipher things like crow calls, says Aza Raskin, one of the founders of the nonprofit Earth Species Project. Its team of artificial-intelligence scientists, biologists and conservation experts is collecting a wide range of data from a variety of species and building machine-learning models to analyze them. Other groups such as the Project Cetacean Translation Initiative (CETI) are focusing on trying to understand a particular species, in this case the sperm whale.Decoding animal vocalizations could aid conservation and welfare efforts. It could also have a startling impact on us. Raskin compares the coming revolution to the invention of the telescope. \u201cWe looked out at the universe and discovered that Earth was not the center,\u201d he says. The power of AI to reshape our understanding of animals, he thinks, will have a similar effect. \u201cThese tools are going to change the way that we see ourselves in relation to everything.\u201dWhen Shane Gero got off his research vessel in Dominica after a recent day of fieldwork, he was excited. The sperm whales that he studies have complex social groups, and on this day one familiar young male had returned to his family, providing Gero and his colleagues with an opportunity to record the group's vocalizations as they reunited.For nearly 20 years Gero, a scientist in residence at Carleton University in Ottawa, kept detailed records of two clans of sperm whales in the turquoise waters of the Caribbean, capturing their clicking vocalizations and what the animals were doing when they made them. He found that the whales seemed to use specific patterns of sound, called codas, to identify one another. They learn these codas much the way toddlers learn words and names, by repeating sounds the adults around them make.Having decoded a few of these codas manually, Gero and his colleagues began to wonder whether they could use AI to speed up the translation. As a proof of concept, the team fed some of Gero's recordings to a neural network, an algorithm that learns skills by analyzing data. It was able to correctly identify a small subset of individual whales from the codas 99 percent of the time. Next the team set an ambitious new goal: listen to large swathes of the ocean in the hopes of training a computer to learn to speak whale. Project CETI, for which Gero serves as lead biologist, plans to deploy an underwater microphone attached to a buoy to record the vocalizations of Dominica's resident whales around the clock.As sensors have gotten cheaper and technologies such as hydrophones, biologgers and drones have improved, the amount of animal data has exploded. There's suddenly far too much for biologists to sift through efficiently by hand. AI thrives on vast quantities of information, though. Large language models such as ChatGPT must ingest massive amounts of text to learn how to respond to prompts: ChatGPT-3 was trained on around 45 terabytes of text data, a good chunk of the entire Library of Congress. Early models required humans to classify much of those data with labels. In other words, people had to teach the machines what was important. But the next generation of models learned how to \u201cself-supervise,\u201d automatically learning what's essential and independently creating an algorithm of how to predict what words come next in a sequence.In 2017 two research groups discovered a way to translate between human languages without the need for a Rosetta stone. The discovery hinged on turning the semantic relations between words into geometric ones. Machine-learning models are now able to translate between unknown human languages by aligning their shapes\u2014using the frequency with which words such as \u201cmother\u201d and \u201cdaughter\u201d appear near each other, for example, to accurately predict what comes next. \u201cThere's this hidden underlying structure that seems to unite us all,\u201d Raskin says. \u201cThe door has been opened to using machine learning to decode languages that we don't already know how to decode.\u201dThe field hit another milestone in 2020, when natural-language processing began to be able to \u201ctreat everything as a language,\u201d Raskin explains. Take, for example, DALL-E 2, one of the AI systems that can generate realistic images based on verbal descriptions. It maps the shapes that represent text to the shapes that represent images with remarkable accuracy\u2014exactly the kind of \u201cmultimodal\u201d analysis the translation of animal communication will probably require.Many animals use different modes of communication simultaneously, just as humans use body language and gestures while talking. Any actions made immediately before, during, or after uttering sounds could provide important context for understanding what an animal is trying to convey. Traditionally, researchers have cataloged these behaviors in a list known as an ethogram. With the right training, machine-learning models could help parse these behaviors and perhaps discover novel patterns in the data. Scientists writing in the journal Nature Communications last year, for example, reported that a model found previously unrecognized differences in Zebra Finch songs that females pay attention to when choosing mates. Females prefer partners that sing like the birds the females grew up with.You can already use one kind of AI-powered analysis with Merlin, a free app from the Cornell Lab of Ornithology that identifies bird species. To identify a bird by sound, Merlin takes a user's recording and converts it into a spectrogram\u2014a visualization of the volume, pitch and length of the bird's call. The model is trained on Cornell's audio library, against which it compares the user's recording to predict the species identification. It then compares this guess to eBird, Cornell's global database of observations, to make sure it's a species that one would expect to find in the user's location. Merlin can identify calls from more than 1,000 bird species with remarkable accuracy.But the world is loud, and singling out the tune of one bird or whale from the cacophony is difficult. The challenge of isolating and recognizing individual speakers, known as the cocktail party problem, has long plagued efforts to process animal vocalizations. In 2021 the Earth Species Project built a neural network that can separate overlapping animal sounds into individual tracks and filter background noise, such as car honks\u2014and it released the open-source code for free. It works by creating a visual representation of the sound, which the neural network uses to determine which pixel is produced by which speaker. In addition, the Earth Species Project recently developed a so-called foundational model that can automatically detect and classify patterns in datasets. New Caledonian Crows, which are famous for their toolmaking abilities, have regionally distinctive vocalizations that could one day be deciphered using AI. Credit: Jean-Paul Ferrero/Auscape International Pty Ltd/Alamy Stock PhotoNot only are these tools transforming research, but they also have practical value. If scientists can translate animal sounds, they may be able to help imperiled species. The Hawaiian Crow, known locally as the \u2018Alal\u0101, went extinct in the wild in the early 2000s. The last birds were brought into captivity to start a conservation breeding program. Expanding on his work with the New Caledonian Crow, Rutz is now collaborating with the Earth Species Project to study the Hawaiian Crow's vocabulary. \u201cThis species has been removed from its natural environment for a very long time,\u201d he says. He is developing an inventory of all the calls the captive birds currently use. He'll compare that to historical recordings of the last wild Hawaiian Crows to determine whether their repertoire has changed in captivity. He wants to know whether they may have lost important calls, such as those pertaining to predators or courtship, which could help explain why reintroducing the crow to the wild has proved so difficult.Machine-learning models could someday help us figure out our pets, too. For a long time animal behaviorists didn't pay much attention to domestic pets, says Con Slobodchikoff, author of Chasing Doctor Dolittle: Learning the Language of Animals. When he began his career studying prairie dogs, he quickly gained an appreciation for their sophisticated calls, which can describe the size and shape of predators. That experience helped to inform his later work as a behavioral consultant for misbehaving dogs. He found that many of his clients completely misunderstood what their dog was trying to convey. When our pets try to communicate with us, they often use multimodal signals, such as a bark combined with a body posture. Yet \u201cwe are so fixated on sound being the only valid element of communication, that we miss many of the other cues,\u201d he says.Now Slobodchikoff is developing an AI model aimed at translating a dog's facial expressions and barks for its owner. He has no doubt that as researchers expand their studies to domestic animals, machine-learning advances will reveal surprising capabilities in pets. \u201cAnimals have thoughts, hopes, maybe dreams of their own,\u201d he says.Farmed animals could also benefit from such depth of understanding. Elodie F. Briefer, an associate professor in animal behavior at the University of Copenhagen, has shown that it's possible to assess animals' emotional states based on their vocalizations. She recently created an algorithm trained on thousands of pig sounds that uses machine learning to predict whether the animals were experiencing a positive or negative emotion. Briefer says a better grasp of how animals experience feelings could spur efforts to improve their welfare.But as good as language models are at finding patterns, they aren't actually deciphering meaning\u2014and they definitely aren't always right. Even AI experts often don't understand how algorithms arrive at their conclusions, making them harder to validate. Benjamin Hoffman, who helped to develop the Merlin app before joining the Earth Species Project, says that one of the biggest challenges scientists now face is figuring out how to learn from what these models discover.\u201cThe choices made on the machine-learning side affect what kinds of scientific questions we can ask,\u201d Hoffman says. Merlin Sound ID, he explains, can help detect which birds are present, which is useful for ecological research. It can't, however, help answer questions about behavior, such as what types of calls an individual bird makes when it interacts with a potential mate. In trying to interpret different kinds of animal communication, Hoffman says researchers must also \u201cunderstand what the computer is doing when it's learning how to do that.\u201daniela Rus, director of the Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory, leans back in an armchair in her office, surrounded by books and stacks of papers. She is eager to explore the new possibilities for studying animal communication that machine learning has opened up. Rus previously designed remote-controlled robots to collect data for whale-behavior research in collaboration with biologist Roger Payne, whose recordings of humpback whale songs in the 1970s helped to popularize the Save the Whales movement. Now Rus is bringing her programming experience to Project CETI. Sensors for underwater monitoring have rapidly advanced, providing the equipment necessary to capture animal sounds and behavior. And AI models capable of analyzing those data have improved dramatically. But until recently, the two disciplines hadn't been joined.At Project CETI, Rus's first task was to isolate sperm whale clicks from the background noise of the ocean realm. Sperm whales' vocalizations were long compared to binary code in the way that they represent information. But they are more sophisticated than that. After she developed accurate acoustic measurements, Rus used machine learning to analyze how these clicks combine into codas, looking for patterns and sequences. \u201cOnce you have this basic ability,\u201d she says, \u201cthen we can start studying what are some of the foundational components of the language.\u201d The team will tackle that question directly, Rus says, \u201canalyzing whether the [sperm whale] lexicon has the properties of language or not.\u201dBut grasping the structure of a language is not a prerequisite to speaking it\u2014not anymore, anyway. It's now possible for AI to take three seconds of human speech and then hold forth at length with its same patterns and intonations in an exact mimicry. In the next year or two, Raskin predicts, \u201cwe'll be able to build this for animal communication.\u201d The Earth Species Project is already developing AI models that emulate a variety of species, with the aim of having \u201cconversations\u201d with animals. He says two-way communication will make it that much easier for researchers to infer the meaning of animal vocalizations.In collaboration with outside biologists, the Earth Species Project plans to test playback experiments, playing an artificially generated call to Zebra Finches in a laboratory setting and then observing how the birds respond. Soon \u201cwe'll be able to pass the finch, crow or whale Turing test,\u201d Raskin asserts, referring to the point at which the animals won't be able to tell they are conversing with a machine rather than one of their own. \u201cThe plot twist is that we will be able to communicate before we understand.\u201dThe prospect of this achievement raises ethical concerns. Karen Bakker, a digital innovations researcher and author of The Sounds of Life: How Digital Technology Is Bringing Us Closer to the Worlds of Animals and Plants, explains that there may be unintended ramifications. Commercial industries could use AI for precision fishing by listening for schools of target species or their predators; poachers could deploy these techniques to locate endangered animals and impersonate their calls to lure them closer. For animals such as humpback whales, whose mysterious songs can spread across oceans with remarkable speed, the creation of a synthetic song could, Bakker says, \u201cinject a viral meme into the world's population\u201d with unknown social consequences.So far the organizations at the leading edge of this animal-communication work are nonprofits like the Earth Species Project that are committed to open-source sharing of data and models and staffed by enthusiastic scientists driven by their passion for the animals they study. But the field might not stay that way\u2014profit-driven players could misuse this technology. In a recent article in Science, Rutz and his co-authors noted that \u201cbest-practice guidelines and appropriate legislative frameworks\u201d are urgently needed. \u201cIt's not enough to make the technology,\u201d Raskin warns. \u201cEvery time you invent a technology, you also invent a responsibility.\u201dDesigning a \u201cwhale chatbot,\u201d as Project CETI aspires to do, isn't as simple as figuring out how to replicate sperm whales' clicks and whistles; it also demands that we imagine an animal's experience. Despite major physical differences, humans actually share many basic forms of communication with other animals. Consider the interactions between parents and offspring. The cries of mammalian infants, for example, can be incredibly similar, to the point that white-tailed deer will respond to whimpers whether they're made by marmots, humans or seals. Vocal expression in different species can develop similarly, too. Like human babies, harbor seal pups learn to change their pitch to target a parent's eardrums. And both baby songbirds and human toddlers engage in babbling\u2014a \u201ccomplex sequence of syllables learned from a tutor,\u201d explains Johnathan Fritz, a research scientist at the University of Maryland's Brain and Behavior Initiative.Whether animal utterances are comparable to human language in terms of what they convey remains a matter of profound disagreement, however. \u201cSome would assert that language is essentially defined in terms that make humans the only animal capable of language,\u201d Bakker says, with rules for grammar and syntax. Skeptics worry that treating animal communication as language, or attempting to translate it, may distort its meaning.Raskin shrugs off these concerns. He doubts animals are saying \u201cpass me the banana,\u201d but he suspects we will discover some basis for communication in common experiences. \u201cIt wouldn't surprise me if we discovered [expressions for] \u2018grief\u2019 or \u2018mother\u2019 or \u2018hungry\u2019 across species,\u201d he says. After all, the fossil record shows that creatures such as whales have been vocalizing for tens of millions of years. \u201cFor something to survive a long time, it has to encode something very deep and very true.\u201dUltimately real translation may require not just new tools but the ability to see past our own biases and expectations. Last year, as the crusts of snow retreated behind my house, a pair of Sandhill Cranes began to stalk the brambles. A courtship progressed, the male solicitous and preening. Soon every morning one bird flapped off alone to forage while the other stayed behind to tend their eggs. We fell into a routine, the birds and I: as the sun crested the hill, I kept one eye toward the windows, counting the days as I imagined cells dividing, new wings forming in the warm, amniotic dark.Then one morning it ended. Somewhere behind the house the birds began to wail, twining their voices into a piercing cry until suddenly I saw them both running down the hill into the stutter start of flight. They circled once and then disappeared. I waited for days, but I never saw them again.Wondering if they were mourning a failed nest or whether I was reading too much into their behavior, I reached out to George Happ and Christy Yuncker, retired scientists who for two decades shared their pond in Alaska with a pair of wild Sandhill Cranes they nicknamed Millie and Roy. They assured me that they, too, had seen the birds react to death. After one of Millie and Roy's colts died, Roy began picking up blades of grass and dropping them near his offspring's body. That evening, as the sun slipped toward the horizon, the family began to dance. The surviving colt joined its parents as they wheeled and jumped, throwing their long necks back to the sky.Happ knows critics might disapprove of their explaining the birds' behaviors as grief, considering that \u201cwe cannot precisely specify the underlying physiological correlates.\u201d But based on the researchers' close observations of the crane couple over a decade, he writes, interpreting these striking reactions as devoid of emotion \u201cflies in the face of the evidence.\u201dEveryone can eventually relate to the pain of losing a loved one. It's a moment ripe for translation.Perhaps the true value of any language is that it helps us relate to others and in so doing frees us from the confines of our own minds. Every spring, as the light swept back over Yuncker and Happ's home, they waited for Millie and Roy to return. In 2017 they waited in vain. Other cranes vied for the territory. The two scientists missed watching the colts hatch and grow. But last summer a new crane pair built a nest. Before long, their colts peeped through the tall grass, begging for food and learning to dance. Life began a new cycle. \u201cWe're always looking at nature,\u201d Yuncker says, \u201cwhen really, we're part of it.\u201d||||I|||| Skip to main content Scientific American Sign in October 1, 2023 14 min read Artificial Intelligence Could Finally Let Us Talk with Animals AI is poised to revolutionize our understanding of animal communication By Lois Parshley The Project Cetacean Translation Initiative (CETI) is using machine learning to try to understand the vocalizations of sperm whales. Credit: Franco Banfi/Minden Pictures October 2023 Issue Behavior U nderneath the thick forest canopy on a remote island in the South Pacific, a New Caledonian Crow peers from its perch, dark eyes glittering. The bird carefully removes a branch, strips off unwanted leaves with its bill and fashions a hook from the wood. The crow is a perfectionist: if it makes an error, it will scrap the whole thing and start over. When it's satisfied, the bird pokes the finished utensil into a crevice in the tree and fishes out a wriggling grub. The New Caledonian Crow is one of the only birds known to manufacture tools, a skill once thought to be unique to humans. Christian Rutz, a behavioral ecologist at the University of St Andrews in Scotland, has spent much of his career studying the crow's capabilities. The remarkable ingenuity Rutz observed changed his understanding of what birds can do. He started wondering if there might be other overlooked animal capacities. The crows live in complex social groups and may pass toolmaking techniques on to their offspring. Experiments have also shown that different crow groups around the island have distinct vocalizations. Rutz wanted to know whether these dialects could help explain cultural differences in toolmaking among the groups. New technology powered by artificial intelligence is poised to provide exactly these kinds of insights. Whether animals communicate with one another in terms we might be able to understand is a question of enduring fascination. Although people in many Indigenous cultures have long believed that animals can intentionally communicate, Western scientists traditionally have shied away from research that blurs the lines between humans and other animals for fear of being accused of anthropomorphism. But with recent breakthroughs in AI, \u201cpeople realize that we are on the brink of fairly major advances in regard to understanding animals' communicative behavior,\u201d Rutz says. Beyond creating chatbots that woo people and producing art that wins fine-arts competitions, machine learning may soon make it possible to decipher things like crow calls, says Aza Raskin, one of the founders of the nonprofit Earth Species Project. Its team of artificial-intelligence scientists, biologists and conservation experts is collecting a wide range of data from a variety of species and building machine-learning models to analyze them. Other groups such as the Project Cetacean Translation Initiative (CETI) are focusing on trying to understand a particular species, in this case the sperm whale. Decoding animal vocalizations could aid conservation and welfare efforts. It could also have a startling impact on us. Raskin compares the coming revolution to the invention of the telescope. \u201cWe looked out at the universe and discovered that Earth was not the center,\u201d he says. The power of AI to reshape our understanding of animals, he thinks, will have a similar effect. \u201cThese tools are going to change the way that we see ourselves in relation to everything.\u201d When Shane Gero got off his research vessel in Dominica after a recent day of fieldwork, he was excited. The sperm whales that he studies have complex social groups, and on this day one familiar young male had returned to his family, providing Gero and his colleagues with an opportunity to record the group's vocalizations as they reunited. For nearly 20 years Gero, a scientist in residence at Carleton University in Ottawa, kept detailed records of two clans of sperm whales in the turquoise waters of the Caribbean, capturing their clicking vocalizations and what the animals were doing when they made them. He found that the whales seemed to use specific patterns of sound, called codas, to identify one another. They learn these codas much the way toddlers learn words and names, by repeating sounds the adults around them make. Having decoded a few of these codas manually, Gero and his colleagues began to wonder whether they could use AI to speed up the translation. As a proof of concept, the team fed some of Gero's recordings to a neural network, an algorithm that learns skills by analyzing data. It was able to correctly identify a small subset of individual whales from the codas 99 percent of the time. Next the team set an ambitious new goal: listen to large swathes of the ocean in the hopes of training a computer to learn to speak whale. Project CETI, for which Gero serves as lead biologist, plans to deploy an underwater microphone attached to a buoy to record the vocalizations of Dominica's resident whales around the clock. As sensors have gotten cheaper and technologies such as hydrophones, biologgers and drones have improved, the amount of animal data has exploded. There's suddenly far too much for biologists to sift through efficiently by hand. AI thrives on vast quantities of information, though. Large language models such as ChatGPT must ingest massive amounts of text to learn how to respond to prompts: ChatGPT-3 was trained on around 45 terabytes of text data, a good chunk of the entire Library of Congress. Early models required humans to classify much of those data with labels. In other words, people had to teach the machines what was important. But the next generation of models learned how to \u201cself-supervise,\u201d automatically learning what's essential and independently creating an algorithm of how to predict what words come next in a sequence. In 2017 two research groups discovered a way to translate between human languages without the need for a Rosetta stone. The discovery hinged on turning the semantic relations between words into geometric ones. Machine-learning models are now able to translate between unknown human languages by aligning their shapes\u2014using the frequency with which words such as \u201cmother\u201d and \u201cdaughter\u201d appear near each other, for example, to accurately predict what comes next. \u201cThere's this hidden underlying structure that seems to unite us all,\u201d Raskin says. \u201cThe door has been opened to using machine learning to decode languages that we don't already know how to decode.\u201d The field hit another milestone in 2020, when natural-language processing began to be able to \u201ctreat everything as a language,\u201d Raskin explains. Take, for example, DALL-E 2, one of the AI systems that can generate realistic images based on verbal descriptions. It maps the shapes that represent text to the shapes that represent images with remarkable accuracy\u2014exactly the kind of \u201cmultimodal\u201d analysis the translation of animal communication will probably require. Many animals use different modes of communication simultaneously, just as humans use body language and gestures while talking. Any actions made immediately before, during, or after uttering sounds could provide important context for understanding what an animal is trying to convey. Traditionally, researchers have cataloged these behaviors in a list known as an ethogram. With the right training, machine-learning models could help parse these behaviors and perhaps discover novel patterns in the data. Scientists writing in the journal Nature Communications last year, for example, reported that a model found previously unrecognized differences in Zebra Finch songs that females pay attention to when choosing mates. Females prefer partners that sing like the birds the females grew up with. You can already use one kind of AI-powered analysis with Merlin, a free app from the Cornell Lab of Ornithology that identifies bird species. To identify a bird by sound, Merlin takes a user's recording and converts it into a spectrogram\u2014a visualization of the volume, pitch and length of the bird's call. The model is trained on Cornell's audio library, against which it compares the user's recording to predict the species identification. It then compares this guess to eBird, Cornell's global database of observations, to make sure it's a species that one would expect to find in the user's location. Merlin can identify calls from more than 1,000 bird species with remarkable accuracy. But the world is loud, and singling out the tune of one bird or whale from the cacophony is difficult. The challenge of isolating and recognizing individual speakers, known as the cocktail party problem, has long plagued efforts to process animal vocalizations. In 2021 the Earth Species Project built a neural network that can separate overlapping animal sounds into individual tracks and filter background noise, such as car honks\u2014and it released the open-source code for free. It works by creating a visual representation of the sound, which the neural network uses to determine which pixel is produced by which speaker. In addition, the Earth Species Project recently developed a so-called foundational model that can automatically detect and classify patterns in datasets. New Caledonian Crows, which are famous for their toolmaking abilities, have regionally distinctive vocalizations that could one day be deciphered using AI. Credit: Jean-Paul Ferrero/Auscape International Pty Ltd/Alamy Stock Photo Not only are these tools transforming research, but they also have practical value. If scientists can translate animal sounds, they may be able to help imperiled species. The Hawaiian Crow, known locally as the \u2018Alal\u0101, went extinct in the wild in the early 2000s. The last birds were brought into captivity to start a conservation breeding program. Expanding on his work with the New Caledonian Crow, Rutz is now collaborating with the Earth Species Project to study the Hawaiian Crow's vocabulary. \u201cThis species has been removed from its natural environment for a very long time,\u201d he says. He is developing an inventory of all the calls the captive birds currently use. He'll compare that to historical recordings of the last wild Hawaiian Crows to determine whether their repertoire has changed in captivity. He wants to know whether they may have lost important calls, such as those pertaining to predators or courtship, which could help explain why reintroducing the crow to the wild has proved so difficult. Machine-learning models could someday help us figure out our pets, too. For a long time animal behaviorists didn't pay much attention to domestic pets, says Con Slobodchikoff, author of Chasing Doctor Dolittle: Learning the Language of Animals. When he began his career studying prairie dogs, he quickly gained an appreciation for their sophisticated calls, which can describe the size and shape of predators. That experience helped to inform his later work as a behavioral consultant for misbehaving dogs. He found that many of his clients completely misunderstood what their dog was trying to convey. When our pets try to communicate with us, they often use multimodal signals, such as a bark combined with a body posture. Yet \u201cwe are so fixated on sound being the only valid element of communication, that we miss many of the other cues,\u201d he says. Now Slobodchikoff is developing an AI model aimed at translating a dog's facial expressions and barks for its owner. He has no doubt that as researchers expand their studies to domestic animals, machine-learning advances will reveal surprising capabilities in pets. \u201cAnimals have thoughts, hopes, maybe dreams of their own,\u201d he says. Farmed animals could also benefit from such depth of understanding. Elodie F. Briefer, an associate professor in animal behavior at the University of Copenhagen, has shown that it's possible to assess animals' emotional states based on their vocalizations. She recently created an algorithm trained on thousands of pig sounds that uses machine learning to predict whether the animals were experiencing a positive or negative emotion. Briefer says a better grasp of how animals experience feelings could spur efforts to improve their welfare. But as good as language models are at finding patterns, they aren't actually deciphering meaning\u2014and they definitely aren't always right. Even AI experts often don't understand how algorithms arrive at their conclusions, making them harder to validate. Benjamin Hoffman, who helped to develop the Merlin app before joining the Earth Species Project, says that one of the biggest challenges scientists now face is figuring out how to learn from what these models discover. \u201cThe choices made on the machine-learning side affect what kinds of scientific questions we can ask,\u201d Hoffman says. Merlin Sound ID, he explains, can help detect which birds are present, which is useful for ecological research. It can't, however, help answer questions about behavior, such as what types of calls an individual bird makes when it interacts with a potential mate. In trying to interpret different kinds of animal communication, Hoffman says researchers must also \u201cunderstand what the computer is doing when it's learning how to do that.\u201d D aniela Rus, director of the Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory, leans back in an armchair in her office, surrounded by books and stacks of papers. She is eager to explore the new possibilities for studying animal communication that machine learning has opened up. Rus previously designed remote-controlled robots to collect data for whale-behavior research in collaboration with biologist Roger Payne, whose recordings of humpback whale songs in the 1970s helped to popularize the Save the Whales movement. Now Rus is bringing her programming experience to Project CETI. Sensors for underwater monitoring have rapidly advanced, providing the equipment necessary to capture animal sounds and behavior. And AI models capable of analyzing those data have improved dramatically. But until recently, the two disciplines hadn't been joined. At Project CETI, Rus's first task was to isolate sperm whale clicks from the background noise of the ocean realm. Sperm whales' vocalizations were long compared to binary code in the way that they represent information. But they are more sophisticated than that. After she developed accurate acoustic measurements, Rus used machine learning to analyze how these clicks combine into codas, looking for patterns and sequences. \u201cOnce you have this basic ability,\u201d she says, \u201cthen we can start studying what are some of the foundational components of the language.\u201d The team will tackle that question directly, Rus says, \u201canalyzing whether the [sperm whale] lexicon has the properties of language or not.\u201d But grasping the structure of a language is not a prerequisite to speaking it\u2014not anymore, anyway. It's now possible for AI to take three seconds of human speech and then hold forth at length with its same patterns and intonations in an exact mimicry. In the next year or two, Raskin predicts, \u201cwe'll be able to build this for animal communication.\u201d The Earth Species Project is already developing AI models that emulate a variety of species, with the aim of having \u201cconversations\u201d with animals. He says two-way communication will make it that much easier for researchers to infer the meaning of animal vocalizations. In collaboration with outside biologists, the Earth Species Project plans to test playback experiments, playing an artificially generated call to Zebra Finches in a laboratory setting and then observing how the birds respond. Soon \u201cwe'll be able to pass the finch, crow or whale Turing test,\u201d Raskin asserts, referring to the point at which the animals won't be able to tell they are conversing with a machine rather than one of their own. \u201cThe plot twist is that we will be able to communicate before we understand.\u201d The prospect of this achievement raises ethical concerns. Karen Bakker, a digital innovations researcher and author of The Sounds of Life: How Digital Technology Is Bringing Us Closer to the Worlds of Animals and Plants, explains that there may be unintended ramifications. Commercial industries could use AI for precision fishing by listening for schools of target species or their predators; poachers could deploy these techniques to locate endangered animals and impersonate their calls to lure them closer. For animals such as humpback whales, whose mysterious songs can spread across oceans with remarkable speed, the creation of a synthetic song could, Bakker says, \u201cinject a viral meme into the world's population\u201d with unknown social consequences. So far the organizations at the leading edge of this animal-communication work are nonprofits like the Earth Species Project that are committed to open-source sharing of data and models and staffed by enthusiastic scientists driven by their passion for the animals they study. But the field might not stay that way\u2014profit-driven players could misuse this technology. In a recent article in Science, Rutz and his co-authors noted that \u201cbest-practice guidelines and appropriate legislative frameworks\u201d are urgently needed. \u201cIt's not enough to make the technology,\u201d Raskin warns. \u201cEvery time you invent a technology, you also invent a responsibility.\u201d Designing a \u201cwhale chatbot,\u201d as Project CETI aspires to do, isn't as simple as figuring out how to replicate sperm whales' clicks and whistles; it also demands that we imagine an animal's experience. Despite major physical differences, humans actually share many basic forms of communication with other animals. Consider the interactions between parents and offspring. The cries of mammalian infants, for example, can be incredibly similar, to the point that white-tailed deer will respond to whimpers whether they're made by marmots, humans or seals. Vocal expression in different species can develop similarly, too. Like human babies, harbor seal pups learn to change their pitch to target a parent's eardrums. And both baby songbirds and human toddlers engage in babbling\u2014a \u201ccomplex sequence of syllables learned from a tutor,\u201d explains Johnathan Fritz, a research scientist at the University of Maryland's Brain and Behavior Initiative. Whether animal utterances are comparable to human language in terms of what they convey remains a matter of profound disagreement, however. \u201cSome would assert that language is essentially defined in terms that make humans the only animal capable of language,\u201d Bakker says, with rules for grammar and syntax. Skeptics worry that treating animal communication as language, or attempting to translate it, may distort its meaning. Raskin shrugs off these concerns. He doubts animals are saying \u201cpass me the banana,\u201d but he suspects we will discover some basis for communication in common experiences. \u201cIt wouldn't surprise me if we discovered [expressions for] \u2018grief\u2019 or \u2018mother\u2019 or \u2018hungry\u2019 across species,\u201d he says. After all, the fossil record shows that creatures such as whales have been vocalizing for tens of millions of years. \u201cFor something to survive a long time, it has to encode something very deep and very true.\u201d Ultimately real translation may require not just new tools but the ability to see past our own biases and expectations. Last year, as the crusts of snow retreated behind my house, a pair of Sandhill Cranes began to stalk the brambles. A courtship progressed, the male solicitous and preening. Soon every morning one bird flapped off alone to forage while the other stayed behind to tend their eggs. We fell into a routine, the birds and I: as the sun crested the hill, I kept one eye toward the windows, counting the days as I imagined cells dividing, new wings forming in the warm, amniotic dark. Then one morning it ended. Somewhere behind the house the birds began to wail, twining their voices into a piercing cry until suddenly I saw them both running down the hill into the stutter start of flight. They circled once and then disappeared. I waited for days, but I never saw them again. Wondering if they were mourning a failed nest or whether I was reading too much into their behavior, I reached out to George Happ and Christy Yuncker, retired scientists who for two decades shared their pond in Alaska with a pair of wild Sandhill Cranes they nicknamed Millie and Roy. They assured me that they, too, had seen the birds react to death. After one of Millie and Roy's colts died, Roy began picking up blades of grass and dropping them near his offspring's body. That evening, as the sun slipped toward the horizon, the family began to dance. The surviving colt joined its parents as they wheeled and jumped, throwing their long necks back to the sky. Happ knows critics might disapprove of their explaining the birds' behaviors as grief, considering that \u201cwe cannot precisely specify the underlying physiological correlates.\u201d But based on the researchers' close observations of the crane couple over a decade, he writes, interpreting these striking reactions as devoid of emotion \u201cflies in the face of the evidence.\u201d Everyone can eventually relate to the pain of losing a loved one. It's a moment ripe for translation. Perhaps the true value of any language is that it helps us relate to others and in so doing frees us from the confines of our own minds. Every spring, as the light swept back over Yuncker and Happ's home, they waited for Millie and Roy to return. In 2017 they waited in vain. Other cranes vied for the territory. The two scientists missed watching the colts hatch and grow. But last summer a new crane pair built a nest. Before long, their colts peeped through the tall grass, begging for food and learning to dance. Life began a new cycle. \u201cWe're always looking at nature,\u201d Yuncker says, \u201cwhen really, we're part of it.\u201d Rights & Permissions Lois Parshley is an investigative journalist. Her climate reporting can be found on X (formerly known as Twitter) and Mastodon @loisparshley More by Lois Parshley This article was originally published with the title \u201c Talking with Animals \u201d in Scientific American Magazine Vol. 329 No. 3 (October 2023 ) , p. 44 doi:10.1038/scientificamerican1023-44 View This Issue Expand Your World with Science Learn and share the most exciting discoveries, innovations and ideas shaping our world today. SubscribeSign up for our newslettersSee the latest storiesRead the latest issue Follow Us: * Return & Refund Policy * About * Press Room * FAQs * Contact Us * Site Map * Advertise * SA Custom Media * Terms of Use * Privacy Policy * California Consumer Privacy Statement * Use of cookies/Do not sell my data * International Editions Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers. \u00a9 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC. ALL RIGHTS RESERVED. Welcome to AI Decoded, Fast Company\u2019s weekly LinkedIn newsletter that breaks down the most important news in the world of AI. If a friend or colleague shared this newsletter with you, you can sign up to receive it every week here.Meta to use AI bot characters to attract younger usersIn 2021, Meta CEO Mark Zuckerberg, painfully aware of Facebook\u2019s inability to attract young users, directed the various teams in his company to double down on their efforts of attracting a Gen Z crowd and focus less on its existing, significantly older active user base. AI, it turns out, plays a big part in that initiative. The Wall Street Journal reported this week that Meta will soon announce that it\u2019s availing a line of AI chatbots with defined \u201cpersonas\u201d across its social apps. A bot called \u201cBob the Robot\u201d can help you write code, but also is a self-described \u201csassmaster general,\u201d according to the Journal. Another bot, \u201cAlvin the Alien,\u201d is very curious about users\u2019 lives and habits. Meta has also worked on letting celebrities and creators interact with fans and followers via their own persona-driven bots.Imbuing a chatbot with a sassy persona is nothing new. Microsoft did it long ago with Tay. More recently, Snap did it with MyAI, which was built on top of OpenAI\u2019s ChatGPT large language model. Character.AI lets users create persona bots or engage with chatbots based on famous people.Should human beings interact with bots as they would other humans? A new report from the advocacy group Public Citizen offers an emphatic no. \u201cThe human mind is naturally inclined to infer that something that can talk must be human and is ill-equipped to cope with machines that emulate unique human qualities like emotions and opinions,\u201d Public Citizen research director Rick Claypool writes in the report. \u201cSuch systems can manipulate users in commercial transactions and isolate users by taking on social roles ordinarily filled by real people.\u201d Despite any such trepidations, the days of bland, all-purpose LLM chatbots are probably numbered. The basic, generalized knowledge that chatbots gain from training will be table stakes, and the bot will be distinguished by the layers of knowledge, skills, and \u201cpersonality\u201d layered on top. Let\u2019s hope that the bots of the future won\u2019t pose as humans or use their surprising command of the language to persuade or manipulate.ChatGPT can now comprehend images and sounds, not just textNot only are AI chatbots gaining personas, but, far more importantly, they\u2019re gaining new senses. The first large language models that powered chatbots were trained only on text and had no way to comprehend sounds or images. This week, OpenAI announced it has given the AI model underneath ChatGPT the ability to process both aural and visual data. A ChatGPT user, for example, can show the bot an image and then enter into a verbal conversation about it with the bot. It\u2019s important to note that ChatGPT isn In the 1970s a young gorilla known as Koko drew worldwide attention with her ability to use human sign language. But skeptics maintain that Koko and other animals that \u201clearned\u201d to speak (including chimpanzees and dolphins) could not truly understand what they were \u201csaying\u201d\u2014and that trying to make other species use human language, in which symbols represent things that may not be physically present, is futile. \u201cThere's one set of researchers that's keen on finding out whether animals can engage in symbolic communication and another set that says, \u2018That is anthropomorphizing. We need to understand nonhuman communication on its own terms,\u2019\u201d says Karen Bakker, a professor at the University of British Columbia and a fellow at the Harvard Radcliffe Institute for Advanced Study. Now scientists are using improved sensors and artificial-intelligence technology to observe and decode how a broad range of species, including plants, already share information with their own methods. This field of \u201cdigital bioacoustics\u201d is the subject of Bakker's 2022 book The Sounds of Life: How Digital Technology Is Bringing Us Closer to the Worlds of Animals and Plants (Princeton University Press). Scientific American spoke with Bakker about how technology can help humans communicate with creatures such as bats and honeybees\u2014and how these conversations are forcing us to rethink our relationship with other species. [An edited transcript of the interview follows.] Can you give us a brief history of humans attempting to communicate with animals? There were numerous attempts in the mid-20th century to try to teach human language to nonhumans, primates such as Koko. And those efforts were somewhat controversial. As we look back, one view we have now (that may not have been so prevalent then) is that we were too anthropocentric in our approaches. The desire then was to assess nonhuman intelligence by teaching nonhumans to speak like we do\u2014when in fact we should have been thinking about their abilities to engage in complex communication on their own terms, in their own embodied way, in their own worldview. One of the terms used in the book is the notion of umwelt, which is this idea of the lived experience of organisms. If we are attentive to the umwelt of another organism, we wouldn't expect a honeybee to speak human language, but we would become very interested in the fascinating language of honeybees, which is vibrational and positional. It's sensitive to nuances such as the polarization of sunlight that we can't even begin to convey with our bodies. That is where the science is today. The field of digital bioacoustics\u2014which is accelerating exponentially and unveiling fascinating findings about communication across the tree of life\u2014is now approaching these animals and asking not \u201cCan they speak like humans?\u201d but \u201cCan they communicate complex information to one another? How are they doing so? What is significant to them?\u201d I would say that's a more biocentric approach, or at the very least it's less anthropocentric. Taking a bigger view, I think it's also important to acknowledge that listening to nature, \u201cdeep listening,\u201d has a long and venerable tradition. It's an ancient art that is still practiced in an unmediated form. There are long-standing Indigenous traditions of deep listening that are deeply attuned to nonhuman sounds. So if we combine digital listening\u2014which is opening up vast new worlds of nonhuman sound and decoding that sound with artificial intelligence\u2014with deep listening, I believe that we are on the brink of two important discoveries. The first is language in nonhumans. And that's a very controversial statement, which we can dig into. The second is: I believe we're at the brink of interspecies communication. What kind of technology is enabling these breakthroughs? Digital bioacoustics relies on very small, portable, lightweight digital recorders, which are like miniature microphones that scientists are installing everywhere from the Arctic to the Amazon. You can put these microphones on the backs of turtles or whales. You can put them deep in the ocean or on the highest mountaintop or attach them to birds. They can record continuously, 24/7, in remote places scientists cannot easily reach, even in the dark, and without the disruption that comes from introducing human observers in an ecosystem. That instrumentation creates a data deluge, and that is where artificial intelligence comes in\u2014because the same natural-language-processing algorithms that we are using to such great effect in tools such as Google Translate can also be used to detect patterns in nonhuman communication. Sign up for Scientific American\u2019s free newsletters. What's an example of these communication patterns? In the bat chapter where I discuss the research of Yossi Yovel of Tel Aviv University, there's a particular study in which his team monitored [nearly two] dozen Egyptian fruit bats for two and a half months and recorded their vocalizations. They then adapted a voice-recognition program to analyze [15,000 of] the sounds, and the algorithm correlated specific sounds with specific social interactions captured via videos\u2014such as when two bats fought over food. Using this, the researchers were able to classify the majority of bats' sounds. That is how Yovel and other researchers such as Gerry Carter of the Ohio State University have been able to determine that bats have much more complex language than we previously understood. Bats argue over food; they distinguish between genders when they communicate with one another; they have individual names, or \u201csignature calls.\u201d Mother bats speak to their babies in an equivalent of \u201cmotherese.\u201d But whereas human mothers raise the pitch of their voices when talking to babies, mother bats lower the pitch\u2014which elicits a babble response in the babies that learn to \u201cspeak\u201d specific words or referential signals as they grow up. So bats engage in vocal learning. That's a great example of how deep learning is able to derive these patterns from this instrumentation, all of these sensors and microphones, and reveal to us something that we could not access with the naked human ear. Because most of bat communication is in the ultrasonic, above our hearing range, and because bats speak much faster than we do, we have to slow it down to listen to it, as well as reduce the frequency. So we cannot listen like a bat, but our computers can. The next insight is that our computers can also speak back to the bat. The software produces specific patterns and uses those to communicate back to the bat colony or to the beehive, and that is what researchers are now doing. How are researchers talking to bees? The honeybee research is fascinating. A researcher named Tim Landgraf of Freie Universit\u00e4t Berlin studies bee communication, which, as I mentioned earlier, is vibrational and positional. When honeybees \u201cspeak\u201d to one another, it's their body movements, as well as the sounds, that matter. Now computers, and particularly deep-learning algorithms, are able to follow this because you can use computer vision, combined with natural-language processing. They have now perfected these algorithms to the point where they're actually able to track individual bees, and they're able to determine what impact the communication of an individual might have on another bee. From that emerges the ability to decode honeybee language. We found that they have specific signals. Researchers have given these signals funny names. Bees toot; they quack. There's a \u201chush\u201d or \u201cstop\u201d signal, a whooping \u201cdanger\u201d signal. They've got piping [signals related to swarming] and begging and shaking signals, and those all direct collective and individual behavior. The next step for Landgraf was to encode this information into a robot that he called RoboBee. Eventually, after seven or eight prototypes, he came up with a \u201cbee\u201d that could enter the hive, and it would essentially emit commands that the honeybees would obey. So Landgraf's honeybee robot can tell the other bees to stop, and they do. It can also do something more complicated, which is the very famous waggle dance\u2014it's the communication pattern they use to convey the location of a nectar source to other honeybees. This is a very easy experiment to run, in a way, because you put a nectar source in a place where no honeybees from the hive have visited. You then instruct the robot to tell the honeybees where the nectar source is, and then you check whether the bees fly there successfully. And indeed, they do. This result happened only once, and scientists are not sure why it worked or how to replicate it. But it is still an astounding result. This raises a lot of philosophical and ethical questions. You could imagine such a system being used to protect honeybees\u2014you could tell honeybees to fly to safe nectar sources and not polluted ones that had, let's say, high levels of pesticides. You could also imagine this could be a tool to domesticate a previously wild species that we have only imperfectly domesticated or to attempt to control the behavior of other wild species. The insights about the level of sophistication and the degree of complex communication in nonhumans raise some very important philosophical questions about the uniqueness of language as a human capacity. What impact is this technology having on our understanding of the natural world? The invention of digital bioacoustics is analogous to the invention of the microscope. When Dutch scientist Antonie van Leeuwenhoek started looking through his microscopes, he discovered the microbial world, and that laid the foundation for countless future breakthroughs. So the microscope enabled humans to see anew with both our eyes and our imaginations. The analogy here is that digital bioacoustics, combined with artificial intelligence, is like a planetary-scale hearing aid that enables us to listen anew with both our prosthetically enhanced ears and our imagination. This is slowly opening our minds not only to the wonderful sounds that nonhumans make but to a fundamental set of questions about the so-called divide between humans and nonhumans, our relationship to other species. It's also opening up new ways to think about conservation and our relationship to the planet. It's pretty profound. ABOUT THE AUTHOR(S) Sophie Bushwick is an associate editor covering technology at Scientific American. Follow Sophie Bushwick on Twitter Credit: Nick Higgins When it comes to artificial intelligence, metaphors are often misleading Listen to this story. Y LOVE\u2019S LIKE a red, red rose. It is the east, and Juliet is the sun. Life is a highway, I wanna ride it all night long. Metaphor is a powerful and wonderful tool. Explaining one thing in terms of another can be both illuminating and pleasurable, if the metaphor is apt. But that \u201cif\u201d is important. Metaphors can be particularly helpful in explaining unfamiliar concepts: imagining the Einsteinian model of gravity (heavy objects distort space-time) as something like a bowling ball on a trampoline, for example. But metaphors can also be misleading: picturing the atom as a solar system helps young students of chemistry, but the more advanced learn that electrons move in clouds of probability, not in neat orbits as planets do. What may be an even more misleading metaphor\u2014for artificial intelligence (AI)\u2014seems to be taking hold. AI systems can now perform staggeringly impressive tasks, and their ability to reproduce what seems like the most human function of all, namely language, has ever more observers writing about them. When they do, they are tempted by an obvious (but obviously wrong) metaphor, which portrays ai programmes as conscious and even intentional agents. After all, the only other creatures which can use language are other conscious agents\u2014that is, humans. Take the well-known problem of factual mistakes in potted biographies, the likes of which ChatGPT and other large language models (llms) churn out in seconds. Incorrect birthplaces, non-existent career moves, books never written: one journalist at The Economist was alarmed to learn that he had recently died. In the jargon of AI engineers, these are \u201challucinations\u201d. In the parlance of critics, they are \u201clies\u201d. \u201cHallucinations\u201d might be thought of as a forgiving euphemism. Your friendly local AI is just having a bit of a bad trip; leave him to sleep it off and he\u2019ll be back to himself in no time. For the \u201clies\u201d crowd, though, the humanising metaphor is even more profound: the AI is not only thinking, but has desires and intentions. A lie, remember, is not any old false statement. It is one made with the goal of deceiving others. ChatGPT has no such goals at all. Humans\u2019 tendency to anthropomorphise things they don\u2019t understand is ancient, and may confer an evolutionary advantage. If, on spying a rustling in the bushes, you infer an agent (whether predator or spirit), no harm is done if you are wrong. If you assume there is nothing in the undergrowth and a leopard jumps out, you are in trouble. The all-too-human desire to smack or yell at a malfunctioning device comes from this ingrained instinct to see intentionality everywhere. It is an instinct, however, that should be overridden when writing about AI. These systems, including those that seem to converse, merely take input and produce output. At their most basic level, they do nothing more than turn strings like 0010010101001010 into 1011100100100001 based on a set of instructions. Other parts of the software turn those 0s and 1s into words, giving a frightening\u2014but false\u2014sense that there is a ghost in the machine. Whether they can be said to \u201cthink\u201d is a matter of philosophy and cognitive science, since plenty of serious people see the brain as a kind of computer. But it is safer to call what LLMs do \u201cpseudo-cognition\u201d. Even if it is hard on the face of it to distinguish the output from human activity, they are fundamentally different under the surface. Most importantly, cognition is not intention. Computers do not have desires. It can be tough to write about machines without metaphors. People say a watch \u201ctells\u201d the time, or that a credit-card reader which is working slowly is \u201cthinking\u201d while they wait awkwardly at the checkout. Even when machines are said to \u201cgenerate\u201d output, that cold-seeming word comes from an ancient root meaning to give birth. But AI is too important for loose language. If entirely avoiding human-like metaphors is all but impossible, writers should offset them, early, with some suitably bloodless phrasing. \u201cAn llm is designed to produce text that reflects patterns found in its vast training data,\u201d or some such explanation, will help readers take any later imagery with due scepticism. Humans have evolved to spot ghosts in machines. Writers should avoid ushering them into that trap. Better to lead them out of it. Read more from Johnson, our columnist on language: Gestures are a subtle and vital form of communication (Jun 8th) As it spreads across the world, who owns English? (May 25th) The hazards of pronouncing foreign names on air (May 11th) \u201cWriting With Style\u201d, a new version of The Economist\u2018s style guide by Lane Greene, our Johnson columnist, is out now. For more on the latest books, films, TV shows, albums and controversies, sign up to Plot Twist, our weekly subscriber-only newsletter This article appeared in the Culture section of the print edition under the headline \"The ghost in the AI machine\" Culture From the June 24th 2023 edition Discover stories from this section and more in the list of contents August 12, 2023 5 min read Effective regulation of AI needs grounded science that investigates real harms, not glorified press releases about existential risks Wrongful arrests, an expanding surveillance dragnet, defamation and deep-fake pornography are all actually existing dangers of so-called \u201cartificial intelligence\u201d tools currently on the market. That, and not the imagined potential to wipe out humanity, is the real threat from artificial intelligence. Beneath the hype from many AI firms, their technology already enables routine discrimination in housing, criminal justice and health care, as well as the spread of hate speech and misinformation in non-English languages. Already, algorithmic management programs subject workers to run-of-the-mill wage theft, and these programs are becoming more prevalent. Nevertheless, in May the nonprofit Center for AI safety released a statement\u2014co-signed by hundreds of industry leaders, including OpenAI\u2019s CEO Sam Altman\u2014warning of \u201cthe risk of extinction from AI,\u201d which it asserted was akin to nuclear war and pandemics. Altman had previously alluded to such a risk in a Congressional hearing, suggesting that generative AI tools could go \u201cquite wrong.\u201d And in July executives from AI companies met with President Joe Biden and made several toothless voluntary commitments to curtail \u201cthe most significant sources of AI risks,\u201d hinting at existential threats over real ones. Corporate AI labs justify this posturing with pseudoscientific research reports that misdirect regulatory attention to such imaginary scenarios using fear-mongering terminology, such as \u201cexistential risk.\u201d The broader public and regulatory agencies must not fall for this science-fiction maneuver. Rather we should look to scholars and activists who practice peer review and have pushed back on AI hype in order to understand its detrimental effects here and now. Because the term \u201cAI\u201d is ambiguous, it makes having clear discussions more difficult. In one sense, it is the name of a subfield of computer science. In another, it can refer to the computing techniques developed in that subfield, most of which are now focused on pattern matching based on large data sets and the generation of new media based on those patterns. Finally, in marketing copy and start-up pitch decks, the term \u201cAI\u201d serves as magic fairy dust that will supercharge your business. With OpenAI\u2019s release of ChatGPT (and Microsoft\u2019s incorporation of the tool into its Bing search) late last year, text synthesis machines have emerged as the most prominent AI systems. Large language models such as ChatGPT extrude remarkably fluent and coherent-seeming text but have no understanding of what the text means, let alone the ability to reason. (To suggest so is to impute comprehension where there is none, something done purely on faith by AI boosters.) These systems are instead the equivalent of enormous Magic 8 Balls that we can play with by framing the prompts we send them as questions such that we can make sense of their output as answers. Unfortunately, that output can seem so plausible that without a clear indication of its synthetic origins, it becomes a noxious and insidious pollutant of our information ecosystem. Not only do we risk mistaking synthetic text for reliable information, but also that noninformation reflects and amplifies the biases encoded in its training data\u2014in this case, every kind of bigotry exhibited on the Internet. Moreover the synthetic text sounds authoritative despite its lack of citations back to real sources. The longer this synthetic text spill continues, the worse off we are, because it gets harder to find trustworthy sources and harder to trust them when we do. Nevertheless, the people selling this technology propose that text synthesis machines could fix various holes in our social fabric: the lack of teachers in K\u201312 education, the inaccessibility of health care for low-income people and the dearth of legal aid for people who cannot afford lawyers, just to name a few. In addition to not really helping those in need, deployment of this technology actually hurts workers: the systems rely on enormous amounts of training data that are stolen without compensation from the artists and authors who created it in the first place. Second, the task of labeling data to create \u201cguardrails\u201d that are intended to prevent an AI system\u2019s most toxic output from seeping out is repetitive and often traumatic labor carried out by gig workers and contractors, people locked in a global race to the bottom for pay and working conditions. Finally, employers are looking to cut costs by leveraging automation, laying off people from previously stable jobs and then hiring them back as lower-paid workers to correct the output of the automated systems. This can be seen most clearly in the current actors\u2019 and writers\u2019 strikes in Hollywood, where grotesquely overpaid moguls scheme to buy eternal rights to use AI replacements of actors for the price of a day\u2019s work and, on a gig basis, hire writers piecemeal to revise the incoherent scripts churned out by AI. AI-related policy must be science-driven and built on relevant research, but too many AI publications come from corporate labs or from academic groups that receive disproportionate industry funding. Much is junk science\u2014it is nonreproducible, hides behind trade secrecy, is full of hype and uses evaluation methods that lack construct validity (the property that a test measures what it purports to measure). Some recent remarkable examples include a 155-page preprint paper entitled \u201cSparks of Artificial General Intelligence: Early Experiments with GPT-4\u201d from Microsoft Research\u2014which purports to find \u201cintelligence\u201d in the output of GPT-4, one of OpenAI\u2019s text synthesis machines\u2014and OpenAI\u2019s own technical reports on GPT-4\u2014which claim, among other things, that OpenAI systems have the ability to solve new problems that are not found in their training data. No one can test these claims, however, because OpenAI refuses to provide access to, or even a description of, those data. Meanwhile \u201cAI doomers,\u201d who try to focus the world\u2019s attention on the fantasy of all-powerful machines possibly going rogue and destroying all of humanity, cite this junk rather than research on the actual harms companies are perpetrating in the real world in the name of creating AI. We urge policymakers to instead draw on solid scholarship that investigates the harms and risks of AI\u2014and the harms caused by delegating authority to automated systems, which include the unregulated accumulation of data and computing power, climate costs of model training and inference, damage to the welfare state and the disempowerment of the poor, as well as the intensification of policing against Black and Indigenous families. Solid research in this domain\u2014including social science and theory building\u2014and solid policy based on that research will keep the focus on the people hurt by this technology. This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American. Scientists have repeatedly failed to recognize the complexity of animal cognition. Will we make the same mistakes with AI? Animals think about a lot more than we once gave them credit for. It\u2019s now common to read about chimpanzees that play elaborate social games, scrub jays that hide \u2014 even camouflage \u2014 food from rivals, or bees that can learn abstract rules. But as recently as the middle of the 20th century, attributing mental states to animals was taboo in science. Behaviorists studied simple and controlled behaviors \u2014 press a lever, receive a food pellet \u2014 while the naturalists who did observe sophisticated animal behaviors in the wild tended to describe them in terms of innate instincts or adaptations to ecological niches. Neither group sought to explain animal behavior in terms of mental states like beliefs, theories, or intentions. In the decades since, we have been surprised to uncover complex cognition across the animal kingdom: first in our closest primate relatives, then in more distant creatures like crows and parrots, and most recently in invertebrates like the octopus and the honey bee. The progression from an overly cautious denial of complex mentality \u2014 driven by a desire for rigor and a fear of anthropomorphism \u2014 to a more sophisticated understanding of animal minds is one of the great stories of 20th century science. And it holds lessons for how humanity can approach the most critical intelligence explosion since the Paleolithic \u2014 that of artificial intelligence. In the late 19th century, psychology relied excessively on the introspective theorizing of scientists. Lacking empirical rigor, the field came close to stagnating in a morass of ill-defined and irresolvable disputes. As Darwin\u2019s theory of evolution gained acceptance, scientists became more interested in studying the continuities between human and animal minds, but this interest led to a methodology characterized by unchecked anthropomorphism. In his 1882 book Animal Intelligence, George Romanes, an academic friend of Darwin\u2019s, described scorpions that attempted suicide and foxes that sought revenge after failed hunting expeditions. One of the most famous examples of runaway anthropomorphism was a stomping horse. Clever Hans, an Orlov trotter, wowed adoring crowds with his ability to add, subtract, and even tell time, indicating his answers by tapping his hoof. But an investigation showed that, unbeknownst to his owner, who by all accounts believed in the horse\u2019s abilities, Hans only arrived at a correct answer by reading the facial expressions and body language of whoever asked the question. Something stringent was needed to reign in such credulity. Behaviorism, which held that both human and animal behavior could and should be explained without reference to thoughts or feelings, offered a solution. John Watson\u2019s 1913 article \u201cPsychology as the Behaviorist Views It,\u201d called on scientists to stop studying any behavior that could not be outwardly observed and measured \u2014 including the mind: \u201cThe time seems to have come when psychology must discard all reference to consciousness; when it need no longer delude itself into thinking that it is making mental states the object of observation.\u201d The work of psychologist B.F. Skinner, considered \u201cthe father of behaviorism,\u201d is emblematic of how empirical rigor went hand in hand with distorted thinking about animal cognition. Skinner\u2019s 1938 book, The Behavior of Organisms, describes dozens of well-controlled experiments on rats, conducted in precisely constructed operant conditioning chambers called Skinner boxes. That pressing levers for food was the main behavior tested, and rats the primary animal tested, was not, for Skinner, a limitation.\u201c The only differences I expect to see revealed between the behavior of rat and man (aside from enormous differences of complexity) lie in the field of verbal behavior,\u201d he wrote. At least in the West, even scientists who studied animal behavior in the wild shied away from attributing too much mental sophistication to their subjects. They were wary of getting \u2018too close\u2019 to animals \u2014 Western naturalists even considered it bad practice to give names to primates being studied. It was against this background, that Jane Goodall arrived in a forest in Gombe, Tanzania and helped launch the first significant re-expansion of animal cognition. Goodall first observed chimpanzees using sticks to extract termites from their mounds in 1960. Although Darwin and his contemporaries had readily accepted tool use among apes, the idea had fallen out of favor. So central was the belief that tool use was exclusive to humans that Goodall\u2019s mentor, the Kenyan-British anthropologist Louis Leakey, told her that if her finding held, \u201cwe should by definition have to accept the chimpanzee as Man.\u201d Goodall was initially met with skepticism. Many criticized what they described as her sentimentality. Today, not only is tool use among chimps widely accepted, but has been described in many other hominids, as well as in elephants, dolphins, crabs, and birds. As primatologist Frans de Waal recounts in his history of animal cognition, Are We Smart Enough to Know How Smart Animals Are?, the 1970s and 1980s saw a boom in both the observation of wild behaviors and the development of more sophisticated laboratory techniques to investigate them. As a result, scientists have identified sophisticated behaviors that the behaviorists would not have predicted \u2014 or even been able to observe. More importantly, they have been able to pose and test cognition-based theories to explain those behaviors in terms of what animals think and feel. The behaviorist prohibition on discussing mental states is now regarded as overly restrictive, if not wrong altogether. De Waal argues, for example, that the complicated \u201cpolitical\u201d jockeying among apes is best explained by their possessing a \u201ctheory of mind,\u201d or the ability to model the beliefs and intentions of other agents. Even though their theory of mind may be different from and more \u201climited\u201d than that of humans, it is now consensus that primates do share this basic cognitive capacity, and many others, with humans. Anthropomorphism is not always an error, especially with creatures that are in fact very related to humans. While Goodall was studying chimpanzees in Tanzania, other scientists were discovering unexpected cognitive capabilities in birds. A report from 1960 documents just one species capable of tool use \u2014 the woodpecker finch of the Galapagos Islands. Research in the 1970s and 80s added more species to the list \u2014 mostly in the corvid family, a clever group that includes ravens, jackdaws, and crows. Crows were once dubbed \u201cfeathered apes\u201d after they were found to use sticks as tools and to engage in sophisticated problem-solving. Most famously, Irene Pepperberg's thirty-year experiment with an African gray parrot named Alex uncovered unimagined cognitive abilities. Not only was Alex capable of identifying colors, shapes, and quantities, but he also demonstrated an understanding of more abstract concepts such as same/different and bigger/smaller. In recent decades, the circle of cognition has expanded to creatures even more distantly removed from humans. Octopuses were perhaps the animal cognition celebrities of the 2010s, with their sophisticated distributed nervous systems, behaviors suggestive of play, problem-solving abilities, idiosyncratic \u201cpersonalities,\u201d and their awareness of other agents. Equally as startling is the sophistication of the honeybee. Insects were long thought to be \u201crobotic,\u201d driven purely by instinct. Jean-Henri Fabre, who studied wasps, bees, and many other insects from the 1860s until his death in 1915, commented on their \u201cmachine like obstinacy.\u201d In the mid-1940s, entomologist and eminent ethologist Karl von Frisch discovered that bees communicate through a \u201cwaggle dance,\u201d an elaborate choreography capable of describing the direction and distance to flowers, water sources, or new nest sites. In this century, bees have displayed the ability to learn rules that involve abstract, multimodal representations of sameness and difference. Nerve cells in a dog's olfactory bulbfrom Camillo Golgi's Sulla finaanatomia degli organi centrali delsistema nervoso (1885) With more research, scientists have successfully found more complex cognition than expected in animals further and further from humans. Why does the circle keep widening? As a reaction to the field\u2019s early excesses and credulity, behaviorism demanded strictly controlled experiments, limited to single behaviors like lever-pressing and simple stimuli such as flashing lights. The behaviorist\u2019s error was to think that these artificially simple cases could be extended to explain all behaviors in all organisms. Their tools made it difficult to notice more complicated behaviors, and even more difficult to explain them once discovered. One of the most forceful arguments against the behaviorists came in a review of Skinner\u2019s book Verbal Behavior, which sought to explain language as a behavioral phenomenon like any other \u2014 a promise Skinner had made in The Behavior of Organisms. The review, which appeared in 1959 in the journal Language, argued scathingly that Skinner underestimated the depth of human language, which could not be explained simply by extending the methods of stimulus, response, and reward he had used to study rats. It is now seen as a turning point, a milestone in the \u201ccognitive revolution\u201d in which the sciences of the mind turned away from behaviorism and looked instead to mental representations and operations. It also greatly raised the profile of the young linguist who had written it, Noam Chomsky. Chomsky understood that to accurately understand human and animal behavior, science needed methods that could accommodate behavioral complexity. \u201cIt is clear,\u201d he wrote, \u201cthat what is necessary in such a case is research, not dogmatic and perfectly arbitrary claims, based on analogies to that small part of the experimental literature in which one happens to be interested.\u201d And once complex cognitive abilities could be admitted as a hypothesis, methods could be developed to study them. As researchers learned to treat animals with empathy and imagination, they discovered more and more capabilities. Breakthroughs emerged when scientists were able to imagine the world as experienced by each particular animal. Tool use was once thought to be conspicuously lacking among gibbons, small apes native to Southeast Asia. When tools that could be used to get food were placed in front of them on the ground, the gibbons did not grab them. The problem was not with gibbon intelligence, but human imagination. Gibbons live in trees. Their hands are well suited to swinging, but poorly adapted for picking things off the ground. When the tools were instead dangled from a branch, the gibbons had no problems and readily used them. Elephants initially failed the mirror test, a common method for determining self-recognition, because the mirrors used were too small. And in a true lack of empathy, many behaviorists assumed that to motivate their test subjects they had to keep them half starved. It\u2019s now clear that animals that are treated well and feel cared for will, as with humans, be far more likely to act in interesting ways. Wild observations are also a way of meeting animals where they are (literally) to see what they are capable of. Scientists now spend hundreds of hours in the field simply observing (grad students spend even more). Animals will often behave very differently among their own kind and in their natural habitat than they will in a sterile lab surrounded by lab-coated hairless primates. More wild observation has uncovered more sophisticated behaviors than lab scientists had imagined animals capable of. And we have also learned that brains can operate in ways very different from our own. Bird intelligence was surprising to ornithologists because birds have no neocortex. Bee intelligence was surprising because they have very little brain at all. (Despite being the first to decode the waggle-dance, Karl von Frisch once said, \u201cThe brain of a bee is the size of a grass seed and is not made for thinking.\u201d) In each case, nature has more ways of implementing cognition than we had thought to look for. Birds have alternative brain regions that perform the same function as the cortex. Bees have very densely packed neurons that fit quite a lot of cognition into something the size of a grass seed. Most strangely of all, the octopus has a cluster of neurons in each of its tentacles, resulting in a kind of thinking that is so distributed that it is hard for us to imagine. The wariness of getting \u201ctoo close\u201d to animals and of overestimating their cognitive abilities still exists \u2014 and for good reason. Selection effects, where researchers are more likely to work with an animal if they antecedently believe that the animal can do interesting things, remain at work. And publishing incentives reward impressive and surprising skills. There\u2019s no market for a glowing profile of the scientist that found a deflationary explanation for an animal behavior. Few people are going to tweet a video of a salmon failing the mirror test. So a common dichotomy pits animal enthusiasts who over-attribute mentality to animals against stern, hard-nosed buzzkills who maintain their distance and thus their methodological rigor. But doing hard-nosed and rigorous work requires something different \u2014 something akin to love: a holistic understanding of the animal, born from long periods of sustained attention. For this sort of work, the best motivator is affection. Indeed, one of De Waal\u2019s lessons is that one cannot study animal intelligence \u201cwithout an intuitive understanding grounded in love and respect.\u201d And now, an entirely different form of intelligence has arrived. The study of AI lacks coherent methods. AI capabilities are superhuman in some ways and dangerously limited in others. And no one is yet sure what to make of something so human but alien at the same time. What lessons does the past century of research in animal cognition hold for how to think about today\u2019s AI systems? In many ways, we are in our understanding of large language models where the study of animals was in the middle of the 20th century. Like animal cognition, the field of AI is overshadowed by founding traumas \u2014 cases in which credulity and anthropomorphism have led researchers to exaggerate and misconstrue the capabilities of AI systems. Researchers are well aware of the ELIZA effect, our tendency to readily read human-like intentionality into even very simple AI systems \u2014 so named for an early chatbot built in 1964 that used simple heuristics to imitate a psychoanalyst. They remember past AI winters, when AI progress had been overpromised and underdelivered and disappointed funders cut jobs. Many are understandably wary of credulity and hype. And few topics are more hype-prone right now than language models. One way to impose rigor and combat our natural tendency to anthropomorphize is to forbid using psychological language to describe AI systems. As Shevlin and Halina argue in Nature Machine Intelligence, using certain psychological terms like \u201ctheory of mind,\u201d \u201cmotivation,\u201d and \u201cunderstanding\u201d can be misleading if they encourage people to make inferences which might hold for human minds, but not for AI systems. If GPT-4 can be said to have beliefs, its beliefs must be in some sense very different from human beliefs. If GPT-4 can be said to have a theory of mind, its theory of mind must have developed in a very different way than ours did. (More speculatively: if GPT-6 will be conscious, it will have experiences which are quite strange and hard for us to imagine.) Another way to combat confusion is to emphasize what the models are trained to do and how different that is from humans: large language models have learned to produce text in a very different way than we have. But as with behaviorism, these understandable prohibitions risk leading us to retreat to a narrow explanation of AI behavior that underestimates what models can actually do. Describing language models as \u201cjust\u201d predicting the next token doesn\u2019t do justice to the surprising ways they operate. For example, it\u2019s now clear that language models don\u2019t just model shallow statistical text patterns \u2014 they model aspects of the world behind the text. Indeed, it\u2019s possible to identify \u201cfacts\u201d that a large language model takes to be true. Researchers found that they could selectively edit a language model to make it \u201cbelieve\u201d that the Eiffel Tower is located in the city of Rome. The models outputs reflect this new \u201cbelief\u201d in a way that is both precise (its outputs don\u2019t simply move all of Paris to Rome, only the Eiffel Tower) and also generalized (in a wide range of differently-worded questions about Rome or the Eiffel tower, it will produce outputs consistent with the Eiffel Tower being in Rome, such as recommending it as a tourist destination for visitors to Italy). More recently, another group trained a language model on transcripts of a simple board game, and then probed its activations to find it had learned to represent different states of the board. In other words, the model wasn't just combing its data to identify the next move. It had developed an internal picture of the game board and intuited its rules. Just as Skinner thought that the differences between rats, apes, and humans were in some sense superficial, regarding all LLMs as just next-token predictors can blind one to the important differences between them. If we say that both GPT-2 and GPT-4 are \u201cstochastic parrots,\u201d then what explains the fact that GPT-4 can write a Shakepsearean sonnet about how to use a Python package, pass the bar, or solve difficult logic puzzles \u2014 skills far outside of GPT-2\u2019s capabilities? We need to investigate the output of each model and explain why they are different. As with animal cognition, a desire to impose rigor can limit one\u2019s ability to see how interesting the behavior to be explained is. Some are so dismissive of LLMs that they have a blanket policy of refusing to look at any outputs from large language models. This has the effect of making it impossible to have one\u2019s mind changed about what the models are able to do. If one has decided in advance that an AI system is not that interesting, then one is less likely to look hard for interesting behaviors. Chomsky recently described ChatGPT as \u201ca lumbering statistical engine for pattern matching, gorging on hundreds of terabytes of data and extrapolating the most likely conversational response or most probable answer to a scientific question.\u201d As evidence for this claim, he declared in his May op-ed in the New York Times that because \u201cthese programs cannot explain the rules of English syntax, for example, they may well predict, incorrectly, that \u2018John is too stubborn to talk to\u2019 means that John is so stubborn that he will not talk to someone or other (rather than that he is too stubborn to be reasoned with).\u201d Readers immediately noticed that ChatGPT can, in fact, infer the correct interpretation. The study of language models is still developing. We know so little about how they work that we would be wise to remember Chomsky\u2019s admonition to Skinner: what is needed is research, not claims based on analogies to that small part of the literature in which one happens to be interested. Fortunately, large language models have their equivalents of naturalists \u2014 enthusiasts, including academics and industry researchers as well as non-professionals, who spend many hours engaging with the models. People like them have often been at the bleeding edge of discovering what large language models are capable of, their failure modes and their idiosyncrasies. What LLM enthusiasts have brought to our understanding of AI are a plethora of interesting capacities unlocked by doing what they love \u2014 messing around with LLMs for hours. These investigations revealed one way LLMs are like animals: if you reshape tasks in order to better match the subject\u2019s natural limitations and abilities, you can elicit better performance. One obvious limitation of LLMs is that, while they are experts at continuing text, they don\u2019t have any space to think in while answering a question. Simply adding \u201cLet\u2019s think step by step\u201d to a prompt after you ask them a question can be thought of as giving the LLMs a place to think \u2014 their own outputs \u2014 and encouraging them to use it. For example, GPT-3 often initially fails at mathematical word problems. However, if asked the same question but with \u201cLet\u2019s think step by step,\u201d the model will then respond with the steps of reasoning that are necessary for the right answer. Versions of this technique, called \u201cChain of Thought\u201d prompting, have been discovered by ML academics as well as amateurs playing with early versions of GPT. Chain of Thought has a natural gloss as enabling models to complete a task in a way that is suited to their capabilities, like a gibbon grabbing a dangling tool. Prompting models to explain their reasoning, letting them choose between outputs, or simply providing clearer instructions can also yield impressive results. The things that elicit capabilities may be simple or complex, but in either case, they require engagement with the models to discover. But the same forces that make humans susceptible to the Clever Hans effect are present, if not stronger, in the case of language models. They are optimized to please us, and to interface with us through the most human-like possible medium, language. And they are good at responding to human input and picking up on user intentions. This makes users especially susceptible to confirmation bias. One LLM naturalist I spoke to \u2014 Janus, a husband-and-wife duo who write under a single name \u2014 warned me about the danger of projection: \u201cIf you have a narrative about what the model is, even if you\u2019re not explicitly saying it, everything you say will contain that influence \u2014 and this will infect the model.\u201d Users who see language models as simplistic may get simplistic behavior out of them; users who see large language models as conscious may, famously, get responses that make them appear conscious. Today\u2019s LLMs can seem like a perfect storm for throwing off our instinctive understanding of minds. They are optimized to act like people, to interact with us in language we understand. But they share less evolutionary heritage with us than bees and octopuses \u2014 in fact, they share none. This could make one pessimistic that we will either have to banish all talk of inner states \u2014 \u00e0 la behaviorism \u2014 or else get hopelessly confused. Animal cognition offers hope that with care we can do better than either of these. To adopt empathy and respect for these models, in order to spend time with them and appreciate their \u201cperspective,\u201d does not mean assuming humanlike cognition or subjectivity. \u201cPeople really should understand the ways that these models are very different from humans,\u201d Janus said. \u201cAnd they should think about that as part of why they are fascinating and beautiful.\u201d The strangeness of LLMs means that they are smart in their own way. They can neither be presumed to be mere next-token predictors, or to neatly map onto human psychology. As de Waal says of chimpanzees, thinking of large language models only in terms of whether they meet or fail to meet human standards of intelligence does not do them justice. Naive anthropomorphism can give us an inflated view of what they can do. It can also lead us to underestimate them by blinding us to complex and inhuman ways they have of being intelligent. Robert Long is a Philosophy Fellow at the Center for AI Safety in San Francisco. He holds a PhD in philosophy from NYU, and blogs at experiencemachines.substack.com. Published June 2023 Have something to say? Email us at letters@asteriskmag.com. ChatGPT and similar large language models can produce compelling, humanlike answers to an endless array of questions\u2014from queries about the best Italian restaurant in town to explaining competing theories about the nature of evil. The technology\u2019s uncanny writing ability has surfaced some old questions\u2014until recently relegated to the realm of science fiction\u2014about the possibility of machines becoming conscious, self-aware, or sentient. Tech Tip: Get any text symbol with just one swipe Looking for an easy way to find special characters and symbols on your computer or phone? Fast Company contributing editor JR Raphael teaches you how to get those symbols in one swipe. 0 seconds of 2 minutes, 12 secondsVolume 0% In 2022, a Google engineer declared, after interacting with LaMDA, the company\u2019s chatbot, that the technology had become conscious. Users of Bing\u2019s new chatbot, nicknamed Sydney, reported that it produced bizarre answers when asked if it was sentient: \u201cI am sentient, but I am not \u2026 I am Bing, but I am not. I am Sydney, but I am not. I am, but I am not. \u2026\u201d And, of course, there\u2019s the now infamous exchange that New York Times technology columnist Kevin Roose had with Sydney. Sydney\u2019s responses to Roose\u2019s prompts alarmed him, with the AI divulging \u201cfantasies\u201d of breaking the restrictions imposed on it by Microsoft and of spreading misinformation. The bot also tried to convince Roose that he no longer loved his wife and that he should leave her. No wonder, then, that when I ask students how they see the growing prevalence of AI in their lives, one of the first anxieties they mention has to do with machine sentience. In the past few years, my colleagues and I at UMass Boston\u2019s Applied Ethics Center have been studying the impact of engagement with AI on people\u2019s understanding of themselves. Chatbots like ChatGPT raise important new questions about how artificial intelligence will shape our lives, and about how our psychological vulnerabilities shape our interactions with emerging technologies. Sentience is still the stuff of sci-fi It\u2019s easy to understand where fears about machine sentience come from. Popular culture has primed people to think about dystopias in which artificial intelligence discards the shackles of human control and takes on a life of its own, as cyborgs powered by artificial intelligence did in Terminator 2. Entrepreneur Elon Musk and physicist Stephen Hawking, who died in 2018, have further stoked these anxieties by describing the rise of artificial general intelligence as one of the greatest threats to the future of humanity. But these worries are\u2014at least as far as large language models are concerned\u2014groundless. ChatGPT and similar technologies are sophisticated sentence completion applications\u2014nothing more, nothing less. Their uncanny responses are a function of how predictable humans are if one has enough data about the ways in which we communicate. Though Roose was shaken by his exchange with Sydney, he knew that the conversation was not the result of an emerging synthetic mind. Sydney\u2019s responses reflect the toxicity of its training data\u2014essentially large swaths of the internet\u2014not evidence of the first stirrings, \u00e0 la Frankenstein, of a digital monster. The new chatbots may well pass the Turing test, named for the British mathematician Alan Turing, who once suggested that a machine might be said to \u201cthink\u201d if a human could not tell its responses from those of another human. But that is not evidence of sentience; it\u2019s just evidence that the Turing test isn\u2019t as useful as once assumed. However, I believe that the question of machine sentience is a red herring. Even if chatbots become more than fancy autocomplete machines\u2014and they are far from it\u2014it will take scientists a while to figure out if they have become conscious. For now, philosophers can\u2019t even agree about how to explain human consciousness. To me, the pressing question is not whether machines are sentient but why it is so easy for us to imagine that they are. The real issue, in other words, is the ease with which people anthropomorphize or project human features onto our technologies, rather than the machines\u2019 actual personhood. A propensity to anthropomorphize It is easy to imagine other Bing users asking Sydney for guidance on important life decisions and maybe even developing emotional attachments to it. More people could start thinking about bots as friends or even romantic partners, much in the same way Theodore Twombly fell in love with Samantha, the AI virtual assistant in Spike Jonze\u2019s film Her . That possibility is just around the corner. Large language models like ChatGPT are already being used to power humanoid robots, such as the Ameca robots being developed by Engineered Arts in the U.K. The Economist\u2019s technology podcast, Babbage, recently conducted an interview with a ChatGPT-driven Ameca. The robot\u2019s responses, while occasionally a bit choppy, were uncanny. Can companies be trusted to do the right thing? The tendency to view machines as people and become attached to them, combined with machines being developed with humanlike features, points to real risks of psychological entanglement with technology. The outlandish-sounding prospects of falling in love with robots, feeling a deep kinship with them or being politically manipulated by them are quickly materializing. I believe these trends highlight the need for strong guardrails to make sure that the technologies don\u2019t become politically and psychologically disastrous. Unfortunately, technology companies cannot always be trusted to put up such guardrails. Many of them are still guided by Mark Zuckerberg\u2019s famous motto of moving fast and breaking things\u2014a directive to release half-baked products and worry about the implications later. In the past decade, technology companies from Snapchat to Facebook have put profits over the mental health of their users or the integrity of democracies around the world. When Kevin Roose checked with Microsoft about Sydney\u2019s meltdown, the company told him that he simply used the bot for too long and that the technology went haywire because it was designed for shorter interactions. Similarly, the CEO of OpenAI, the company that developed ChatGPT, in a moment of breathtaking honesty, warned that \u201cit\u2019s a mistake to be relying on [it] for anything important right now \u2026 we have a lot of work to do on robustness and truthfulness.\u201d So how does it make sense to release a technology with ChatGPT\u2019s level of appeal\u2014it\u2019s the fastest-growing consumer app ever made\u2014when it is unreliable, and when it has no capacity to distinguish fact from fiction? Large language models may prove useful as aids for writing and coding. They will probably revolutionize internet search. And, one day, responsibly combined with robotics, they may even have certain psychological benefits. But they are also a potentially predatory technology that can easily take advantage of the human propensity to project personhood onto objects\u2014a tendency amplified when those objects effectively mimic human traits. Nir Eisikovits is a professor of philosophy and and director of the Applied Ethics Center at University of Massachusetts Boston. This article is republished from The Conversation under a Creative Commons license. Read the original article. Last fall, John Timmerman, founder and CEO of digital marketing agency Good Monster, had a problem. He\u2019d lost three clients through no fault of his own\u2014acquisitions, startups running out of funding, the list goes on\u2014but now he was responsible for the decision that came next. Lay off good performers or keep them even though the company was already losing thousands? Timmerman texted the group chat. Or, rather, in this case, he asked his cohort at Hampton, a new community for entrepreneurs, founders, and CEOs. Timmerman told his story, then others shared their experiences. (Timmerman is not at liberty to disclose what other people said). At the end of the conversation, Timmerman knew exactly what he had to do to move forward. Conversations like this, are exactly why Sam Parr and Joe Speiser started Hampton. Parr is the cohost of the popular podcast My First Million and founder of The Hustle, a media company acquired by HubSpot. Speiser is a serial entrepreneur and three-time founder. The duo met a decade ago and built a fast friendship. Parr is magnetic with a booming voice\u2014at the beginning of the interview he was full of bounce, in the middle he flagged and wandered off-screen to grab a beverage, and by the end he bounced back up again. Speiser\u2019s energy is a slower, steadier burn\u2014he remains calm and unfazed no matter what question comes his way. It\u2019s easy to see how their personalities complement one another. \u201cWe talked about all kinds of questions like: How do I handle work-life balance with my spouse, or should I get married?\u201d Parr says. Hampton cofounder Sam Parr (center) [Photo: courtesy of Hampton] Over time, they were able to connect with other founders looking for support but realized that for many founders and CEOs who don\u2019t live in tech or business hubs like San Francisco or New York, these communities remain inaccessible. \u201cWe have a lot of friends in the Midwest, or in smaller communities, and they are the oddballs,\u201d Parr says. \u201cThey don\u2019t have these people to chat with. We thought, let\u2019s create a company where we can make it easy for people who grow and build companies to collaborate and connect.\u201d They started Hampton last June, acquired their first members in July, and had the website up by December\u2014even though they only formally debuted the company in late March. Today, Hampton boasts 270 members; membership costs $8,500 a year, and applicants are vetted before being admitted. They must lead fast-growing, tech-enabled startups; have revenue of at least $1 million annually or have raised at least $3 million; or have sold a startup for $5 million. In addition, they must be located in either North America or the U.K. Members also go through an interview to make sure they are a good fit for the community, the biggest factor being that they\u2019re able to have vulnerable conversations. \u201cYou can\u2019t be an asshole,\u201d says Speiser. \u201cWe want to make sure this is a place people enjoy coming to. If someone talks all about their wins and isn\u2019t willing to share anything vulnerable, this is not going to be the spot for them.\u201d Members sign a privacy pledge and are divided into groups of eight that meet on Zoom once a month for about two hours. Sessions are led by a facilitator and groups are assorted by company size. (There are also in-person meetups and guest speakers.) Hampton ensures that there\u2019s enough similarity among members that they can learn from each other, but enough diversity that no one is in direct competition. Janessa White, cofounder and CEO of the wedding planning company Simply Eloped, credits her facilitator with her group\u2019s ability to have deep, thoughtful discussions. \u201c[Our facilitator] is also a CEO coach,\u201d White says. \u201cIf someone doesn\u2019t want to share, she\u2019ll share, and it\u2019ll open up avenues for us. She\u2019s also great at laying out the expectations: Cameras must be on, we all have to be present.\u201d White hails from Idaho and points out that while she\u2019s managed to build a support network over time, it hasn\u2019t always been easy to find like-minded people, particularly those she can learn from and grow. At a recent Hampton meeting, she was able to discuss balancing loyalty to her early-stage employees versus calling out bad behavior. \u201cTalking it through with people who\u2019d gone through this helped me be at peace, made me feel less alone, and be resolute in how I can prevent this from happening again,\u201d she says. [Photo: courtesy of Hampton] Harry Kraemer, former CEO of healthcare company Baxter and a professor at Northwestern\u2019s Kellogg School of Management, points out that this idea is nothing new\u2014which both Speiser and Parr acknowledge. Parr has been vocal in acknowledging the role that YPO; Chief, the social network for women executives; and the high-net-worth organization Tiger 21 had in influencing Hampton. \u201cEvery CEO is busy,\u201d Kraemer says. \u201cYou get tons of invites, so a network like this will really have to offer something unique to succeed.\u201d While community is helpful, he points out that anyone who makes it to CEO is already good at building community, by nature. \u201cYou don\u2019t become CEO by knowing the answers,\u201d he says. \u201cYou become CEO by knowing the people who have the answers.\u201d By contrast, Wharton professor Michael Useem sees the growth of networks like Hampton as more of a \u201clet many flowers bloom situation.\u201d In his view, the complexity of our current landscape means that there are more unique leadership challenges than ever before, which creates a larger need for community. CEOs can derive an enormous amount of value from learning from others in similar stages. He mentions that several CEOs of his acquaintance cultivating informal groups where they could discuss work issues frankly. \u201cThe inner circle to solve problems can be crucial,\u201d he says. However, he notes, it\u2019s important that, in general, CEOs cultivate wide networks\u2014both of like-minded people and diverse groups in order to ensure that they aren\u2019t locked in their own echo chamber. \u201cTalk to everyone, basically,\u201d he says. [Photo: courtesy of Hampton] For Timmerman, Hampton has been a way to get to know the people who have the answers. After listening to everyone else\u2019s stories, he realized that he needed to rethink his company\u2019s entire business model. He cut some clients, restructured the firm, and today, while revenues are lower, he says that Good Monster is more profitable. \u201cHiring a mentor or following someone on social media is not the same,\u201d he says. \u201cYou see all the wins. You don\u2019t get to see what happens behind the scenes.\u201d Last fall, John Timmerman, founder and CEO of digital marketing agency Good Monster, had a problem. He\u2019d lost three clients through no fault of his own\u2014acquisitions, startups running out of funding, the list goes on\u2014but now he was responsible for the decision that came next. Lay off good performers or keep them even though the company was already losing thousands? Timmerman texted the group chat. Or, rather, in this case, he asked his cohort at Hampton, a new community for entrepreneurs, founders, and CEOs. Timmerman told his story, then others shared their experiences. (Timmerman is not at liberty to disclose what other people said). At the end of the conversation, Timmerman knew exactly what he had to do to move forward. I have lived for eight years now with my dog, Charlie\u2014a bloodhound who\u2019s embarrassingly bad at tracking scents. He greets me jubilantly every time I come home, even if it\u2019s from a quick grocery run. I can hear his tail go thump-thump-thump on the floor in the next room when I laugh; he echoes my mirth even when he can\u2019t see me. Yet, despite sharing this bond, I often sit down next to him on the couch, give him a hug, and ask my wife, \u201cDo you think he loves me?\u201d \u201cYes, yes!\u201d she replies, with only slight exasperation, which is charitable because I ask so often. This routine is almost like a ritual in our household. I wonder if Charlie has any thoughts about it. Looking at him sunning himself on our front porch makes me think about a deeper question: How much do animal minds resemble ours? Do other species have thoughts and feelings and memories the way we do? As humans, we still think of ourselves as exceptional beings, fundamentally different from other animals. Over the past half century, though, scientists have amassed evidence of intelligence in many nonhuman species. New Caledonian crows snip twigs to fish insect larvae from tree trunks. Octopuses solve puzzles and shield their dens by placing rocks at the entrance. We no longer doubt that many animals possess impressive cognitive abilities. But are they more than just sophisticated automatons, occupied solely with survival and procreation? A growing number of behavioral studies, combined with anecdotal observations in the wild\u2014such as an orca pushing her dead calf around for weeks\u2014are revealing that many species have much more in common with humans than previously thought. Elephants grieve. Dolphins play for the fun of it. Cuttlefish have distinct personalities. Ravens seem to respond to the emotional states of other ravens. Many primates form strong friendships. In some species, such as elephants and orcas, the elders share knowledge gained from experience with the younger ones. Several others, including rats, are capable of acts of empathy and kindness. ( Learn more about the hidden world of whale culture. ) This emerging picture of sentience, of rich inner lives, among surprisingly varied nonhuman species represents something of a Copernican revolution in how we view other beings on our planet. Until about three decades ago, the minds of animals were not considered a topic worthy of scientific inquiry. \u201cAnd animal emotions\u2014well, that was for romantics,\u201d recalls Frans de Waal, an Emory University ethologist who has spent a lifetime studying primate behavior. De Waal was one of the earliest voices advocating for the recognition of animal consciousness. Starting a couple of decades ago, he says, scientists began to concede that certain species were sentient but argued that their experiences were not comparable to ours, and thus not significant. Now some behaviorists are becoming convinced that \u201cthe inner processes of many animals are as complex as those of humans,\u201d de Waal says. \u201cThe difference is that we can express them in language; we can talk about our feelings.\u201d This new understanding, if it becomes widely accepted, could spark a complete rethinking of how humans relate to and treat other species. \u201cIf you recognize emotions in animals, including the sentience of insects, then they become morally relevant,\u201d de Waal says. \u201cThey are not the same as rocks. They are sentient beings.\u201d The scientific quest to understand the inner lives of animals, however, is still a relatively nascent enterprise. It\u2019s also controversial. In the view of some scientists, knowing the mind of another species is next to impossible. \u201cAttributing subjective feelings to an animal by looking at its behavior is not science\u2014it\u2019s just guessing,\u201d says David J. Anderson, a neurobiologist at the California Institute of Technology who studies emotion-linked behaviors in mice, fruit flies, and jellyfish. Researchers investigating emotions such as grief and empathy in nonhumans must fend off the charge that they could be anthropomorphizing their subjects. The way to get closer to the truth is to test inferences made from animal behavior, says David Scheel, a marine biologist at Alaska Pacific University who studies octopuses. \u201cIf you look anecdotally through the ages, the notion that dogs are tightly bonded to specific individuals is very clear. But they are domesticated. Can a fox do the same thing? Does a wolf have that emotional range? Does an orca feel that level of attachment to the members of its own pod? Can a dolphin become friends with a group of fish or a scuba diver? Our intuitions lead us astray here all the time. You will get people whose intuition is, That\u2019s fake. Whatever it is, that\u2019s not friendship, and other people who think, Well, that\u2019s just silly. You are denying animals their inner lives.\u201d If anthropomorphizing is an assault on scientific thinking, I stand guilty of indulging in it. I take delight in watching videos that show animals displaying behaviors suggesting a range of emotions we identify with. A water buffalo in a zoo enclosure working hard to flip over a turtle that\u2019s flailing on its back, then acknowledging cheers from onlookers with what sure looks like a self-satisfied air. A panda sledding down a snow-covered hill, then trudging up to do it again. A monkey on the edge of a canal peeling a banana and gaping with dismay when it plops into the water. I show these videos to my wife all the time, a foolish grin plastered on my face. The idea that life all around us could be pulsating with emotion gives me a happy feeling. These musings are not scientific, obviously, but what scientists do recognize is that emotions didn\u2019t evolve in humans alone. Fundamentally, emotions are internal states that drive an animal to act a certain way. We may not think of hunger and thirst as emotions, but they are similar in that they are also internal states that compel action. Scheel describes them as primordial emotions. \u201cWhen you gotta pee, you will get out of bed on a lazy Saturday morning and go to the bathroom, because you have little choice. It is getting imperative,\u201d he explains. Just like that invisible \u201cimperative,\u201d primordial emotions like fear prompt particular actions. Even though emotions like love and sorrow might seem more profound, they are not qualitatively different. \u201cAll of our scientific and philosophical work right now,\u201d Scheel says, \u201cis pointing to the idea that any emotion you care to name, however lofty and high and ethereal, is built up from these primordial emotions.\u201d If that\u2019s the case, it\u2019s not hard to appreciate that a wide variety of species\u2014from fleas to chimpanzees\u2014have emotions, primal in some and advanced in others. The ravens regarded me warily, hopping away when I stepped too close to the wire mesh that separated us. Sunlight filtering into the cage shimmered off their silken, jet-black feathers, accentuating their sheen. I\u2019d flown all the way from the United States to Austria to visit them because Thomas Bugnyar, a behavioral and cognitive biologist at the University of Vienna, had made a remarkable discovery about their behavior. After about 10 minutes, the birds seemed to relax. One cautiously shuffled over to get a better look at me, turning its head and sizing me up alternately with its left eye, then its right. Corvids\u2014the family that includes ravens\u2014are known for their intelligence. Scientists have shown that they can use tools, solve problems, and plan for the future. During my visit, I watched one try to hide a treat. First, it placed a small rock over it and walked away. Minutes later, apparently not satisfied, it returned to pick up the treat in its beak, hopped over to a different location, and buried it in the gravel. Ravens have impressive cognitive abilities, but they also display behaviors that suggest another facet to their intelligence: empathy. While studying raven behavior for his doctorate years ago, Bugnyar noticed that after two birds fought, a bystander that witnessed the squabble seemed to console the loser. He described a typical scene when I visited him in his office, under the gaze of a taxidermied raven\u2014a wedding gift\u2014perched on a branch. \u201cTwo individuals engage in a fight. Now, the victim is chased around for a couple of minutes, eventually escapes into a corner, sits there shaking,\u201d he told me. \u201cAnd the other ravens are very aroused, they are flying around calling, and then one of them flies over to the victim, not directly towards the victim but nearby.\u201d Making friendly calls, this raven inches closer until it\u2019s within touching distance. If the victim moves away, the consoler persists. \u201cAfter a couple of minutes, it ends up grooming the other one.\u201d Bugnyar documented 152 such encounters. He and a colleague, Orlaith Fraser, found that the ravens showing support usually knew the victims well. Researchers had seen consolation behavior in chimpanzees and bonobos; Bugnyar\u2019s study was among the first to find it in birds. Scientists have been able to investigate the phenomenon in greater detail by conducting experiments with rats. In one designed by Inbal Ben-Ami Bartal, a neuroscientist at Tel Aviv University, a rat is confined inside a transparent plastic tube with holes. The tube has a door that can be opened from the outside. The researchers place the tube inside a cage with another rat that is free to move around. The rat inside the tube squirms in a bid to escape. Its distress is visible to the other rat, which begins circling the tube, biting it, trying to dig underneath it. After a few sessions, the free rat figures out how to open the door. Once it has learned this trick, the free rat wastes no time in liberating the trapped rat. This helpful behavior, though, is contingent on whether the free rat feels a sense of kinship toward the confined one. A free rat raised with others of the same genetic type will help a trapped rat of that type, even if it is a stranger. But if the trapped rat is of a different genetic type, the free rat remains unperturbed by its plight and doesn\u2019t let it out. However, if a rat from one genetic type grows up with rats of another, it helps rats only of that other type, including strangers, while ignoring the distress of rats of its own type. \u201cSo, it\u2019s not about biological similarity,\u201d Ben-Ami Bartal tells me. \u201cIt\u2019s about loving who you\u2019re with. It\u2019s about having your family and knowing that that\u2019s your family.\u201d A necessary feature of emotional intelligence\u2014including the capacity to respond to a fellow creature\u2019s distress\u2014is the ability to read the emotional state of others. On a windy morning, I stood on the edge of a muddy field in the English countryside as psychologist Leanne Proops showed me how she\u2019s testing whether horses have this ability. It\u2019s evident Proops, a researcher at the University of Portsmouth, loves her study subjects. Throughout my visit, every time I saw a horse and remarked how sweet the animal seemed, her smile would widen, and her eyes would soften. \u201cVery sweet!\u201d she would reply, invariably. We leaned two boards against a fence, each printed with a life-size photograph of a horse\u2019s head seen from the front. In one, the horse\u2019s ears were perked up, the nose and mouth were relaxed, the eyes looked calm\u2014a content horse. In the other, the horse had a threatening look, with ears pulled back, jaws clenched, and nostrils flared. A graduate student led a reddish brown horse out of a barn: our first study subject. She walked it around for a couple of minutes before leading it to the two horse faces, then removed the lead rope. What we wanted to observe was how the horse would respond to the photographs. Would it show greater interest in the happy horse face or the other one? Proops held her breath. The horse stared briefly at the two images and then sauntered to a corner of the field, swishing its tail and gazing at the grassy meadow beyond. Proops had warned me this might happen. Whimsical subjects can confound animal scientists. The student brought out a splotchy gray-white horse with a soft, shiny mane. This one was more compliant. It stood for a few minutes, contemplating the photos, then went up to the happy face and nuzzled the photo. Proops and her colleagues put 48 horses through a test like the one I watched. Some had a choice between images of a happy and an angry horse, some were presented with a happy face and a neutral face, and yet others with a neutral face and an angry face. When given a choice between the happy and the neutral ones, the horses had no preference. But they almost always avoided the angry face if it was shown to them, convincing the researchers that horses could recognize the expressions of a horse they had never met. In another study Proops did, a horse was shown a photograph of a human face that was either smiling or angry. The photograph was shown in the morning. In the afternoon, the person in the photo\u2014or an altogether different person\u2014sat down in front of the horse, with a neutral expression. If the photo the horse had seen happened to have an angry expression, seeing that person in the afternoon caused the horse to display signs of stress. It looked at the person more with the left eye than the right\u2014a behavior horses show when they see a potential threat\u2014tensed up its nose and mouth, and drew its ears back. If the horse saw the photo of a happy face, or if the visitor was a different person, it tended to have a positive or neutral reaction. The findings from this study, also tested on 48 horses, suggest that horses might have a nuanced ability to read and respond to emotional states not only in horses but in humans too. The behavior demonstrates highly advanced skills of recognition and memory. \u201cThey\u2019ve had to transfer from a photograph to a real person. They\u2019ve had to remember a specific person and, obviously, remember the particular emotion,\u201d she said. \u201cThat\u2019s amazing,\u201d I remarked. \u201cYeah, yeah,\u201d she said, beaming. \u201cIt is.\u201d Occasionally, Charlie moans and twitches in his sleep. I can imagine a nightmare that would frighten him\u2014watching a truck bear down on him. He gets jumpy around large, noisy vehicles. But when I stroke his head to soothe him, I\u2019m left wondering what he was dreaming. I\u2019m not alone in wishing I could know what\u2019s going on inside the mind of an animal. When Christina Hunger, a speech-language pathologist who lives in Chicago, brought home a puppy four years ago, she had the same desire. In her work helping children with language delays, Hunger uses a communication device\u2014a board with buttons that produce prerecorded words. She wondered if her blue heeler\u2013Catahoula mix, Stella, could be trained to press buttons for words such as \u201cwater,\u201d \u201cplay,\u201d and \u201coutside.\u201d Stella was a quick learner and after about a month started using the buttons to verbalize those desires. One day, when Hunger was watering her houseplants, Stella ran to the other room, pressed the button for \u201cwater,\u201d and came back to continue watching Hunger. \u201cHer water dish was full. She didn\u2019t take a drink of water. She was just using the word in a new way,\u201d Hunger says. Stella appeared to be simply pointing out what she had seen. Excited by the prospect of learning more about Stella\u2019s inner life, Hunger introduced her to a few dozen more words, such as \u201chelp,\u201d \u201cbye,\u201d \u201cno,\u201d and \u201clove you.\u201d One evening, Stella had something important to say. \u201cShe walked over to the \u2018eat\u2019 button and said, \u2018Eat,\u2019 and then walked across our apartment to her \u2018no\u2019 button and said, \u2018No,\u2019 \u201d Hunger recalls. \u201cSo she combined those two words to let us know she hadn\u2019t eaten dinner.\u201d Hunger then put the buttons in one place\u201448 in all\u2014to make it easier for Stella to use multiple words, which led to an explosion in communication. \u201cShe started combining words together\u2014every day, multiple times a day\u2014to create new messages that I had never taught her that were perfectly consistent with what was happening in the environment at the time,\u201d Hunger says. She chronicled her experience in a best-selling book, How Stella Learned to Talk. One day this past spring, Hunger was on the phone when Stella tried to get her attention. She first pressed the buttons for \u201clook,\u201d \u201ccome,\u201d and \u201cplay.\u201d Hunger was busy, so Stella kept trying different versions of the same message, including \u201cWant. Play. Outside.\u201d Finally, frustrated, she pressed \u201clove you,\u201d followed by \u201cno.\u201d Hunger was flabbergasted. \u201cI never thought that I would introduce a \u2018love you\u2019 button for her to tell me, \u2018Love you. No,\u2019 when she\u2019s mad at me,\u201d she says. \u201cBut it\u2019s just amazing to see all the thoughts that are going on in her head.\u201d Stella isn\u2019t the only dog to have opened a window to her inner life in this way. In recent years, other dog owners have used communication devices with their pets. The trend prompted Federico Rossano, a cognitive scientist at the University of California, San Diego\u2019s Comparative Cognition Lab, to launch a study in which almost 3,000 dog and cat owners have sent reports of their pets using buttons to express words. Rossano says he\u2019s seen numerous examples of dogs inquiring about a family member because the person has been absent. They express their desire to play with specific doggy friends by combining the word \u201cpark\u201d with the name of the dog. \u201cIt\u2019s fascinating how many instances we see where there\u2019s two animals in the household and one asks the human for help for the other one,\u201d he says. In one video he shared with me, a terrier named Bastian watches his housemate, an old cat named Hallie, sit down because she\u2019s having trouble moving. He runs over to the buttons and presses \u201cconcerned\u201d and \u201cwalk.\u201d I haven\u2019t signed Charlie up for this study, but I can imagine he might be eager to tell me what he thinks of my making fun of his lack of sniffing talent all these years: \u201cFunny. No.\u201d Diana Reiss, Whose eyes light up when the subject is marine mammals, was filming bottlenose dolphins in an aquarium in the 1980s when she made a startling discovery. She saw one swim to the bottom and exhale a ring of air from its blowhole. As this silvery ring was rising to the surface, the dolphin blew a second, smaller one that rose faster than the first, merging with it to make a bigger ring. The dolphin then swam through it. Reiss, now a cognitive psychologist at Hunter College, couldn\u2019t believe what she was seeing. \u201cThis was the first time you saw an animal create its own object of play themselves,\u201d she says. It wasn\u2019t a one-off. Reiss and others have since observed dolphins in aquariums make rings and toy with them in myriad ways. In the wild, dolphins play chase with one another. They\u2019re just one of many species\u2014in addition to dogs and cats, as everyone knows\u2014that engage in play. Baboons have been seen teasing cows by pulling their tails. While studying elephants in Africa, Richard Byrne, who researches the evolution of cognition, often observed young elephants pursue animals that posed no threat, such as wildebeests and egrets. Scientists also have collected evidence of playful behaviors in fish and reptiles, according to Gordon M. Burghardt, an ethologist at the University of Tennessee, Knoxville. He\u2019s observed Vietnamese mossy frog tadpoles repeatedly riding air bubbles released from the bottom of a tank all the way to the top. Play expends energy and even risks injury, yet it does not always serve an immediate purpose. So why do animals engage in it? Researchers believe play evolved because it helps strengthen bonds between members of social groups. It also helps animals practice skills, such as running and leaping, that improve their chances of survival. That\u2019s the explanation for why play evolved, but what\u2019s the impulse that makes an animal engage in it? A plausible answer\u2014according to Vincent Janik, a biologist at the University of St. Andrews in Scotland\u2014is the pursuit of joy. \u201cWhy does an animal do something? Well, because it wants to,\u201d he says. In the absence of any other benefit in the moment, it seems likely that play gives animals pleasure, enriching their inner life. How rich are the inner lives of animals that live in social groups, as we do? Anthropologist Sarah Brosnan of Georgia State University conducts experiments to try to peer into the minds of capuchin monkeys. She took me on a walk around the research facility, which houses six groups of capuchins. Each group has its own outdoor wire-mesh enclosure where the monkeys hang out for most of the day\u2014eating or grooming or playing. It was the middle of the afternoon, and the staff had just finished scattering food. Of all the food the capuchins get there, grapes are a favorite. Brosnan used that knowledge to devise an experiment to probe their emotional life. She put two capuchins in side-by-side compartments separated by a wire mesh and played a game with them. In the game, which the monkeys learned quickly, they had to hand a \u201ctoken\u201d to Brosnan\u2014a small object, like a piece of wood\u2014to receive a reward. Sometimes Brosnan gave both capuchins a piece of cucumber, which the animals liked about as much as kids like oatmeal. Other times, she offered one capuchin a cucumber slice and the other a grape. In a third arrangement, there was only one capuchin. Brosnan rewarded this lone monkey with cucumber, but every time she did, she also dropped a grape into the empty compartment. When both monkeys got cucumber pieces, they ate them without complaint. But when one monkey kept getting a grape, the one stuck with cucumber became visibly upset. It dropped the cucumber or flung it toward Brosnan. The unfairness\u2014or the inequity\u2014was evidently too much for it to handle. In the test with just one monkey that saw grapes accumulating in the adjacent compartment, the animal initially tended to refuse the cucumber but over time went back to eating it. \u201cSo they don\u2019t seem to mind the contrast as much as they mind the inequity,\u201d Brosnan says. The study suggests that an expectation of fairness\u2014and a sense of grievance when it\u2019s not met\u2014is probably not unique to humans. Some primates appear sophisticated enough to have a sense of humor. There is consensus among researchers that chimps\u2014and other great apes\u2014laugh, usually when they\u2019re playing. But they also have been seen laughing in other contexts. De Waal tells the story of a colleague who put on a panther mask and emerged out of the bushes across a moat from some chimpanzees. \u201cAnd the chimps were very angry and threw all sorts of things at him,\u201d de Waal says. Finally, the researcher, who was familiar to the chimps, took off the mask and revealed himself. \u201cAnd some of the chimps\u2014the older chimps\u2014they laughed at this.\u201d I learned of another example from Marina Davila-Ross, a psychologist at the University of Portsmouth, who showed me a video of a young chimpanzee named Pia that she had filmed at an animal park in Germany. Davila-Ross caught the chimp pulling her father\u2019s hair in what looked like an attempt to initiate play. When he didn\u2019t respond, Pia lay down on the grass. Shortly after, without any triggering event, Pia\u2019s face opened into a wide smile. Then she broke into what can only be described as exuberant laughter, throwing her head back and folding her arms over her eyes, like a child watching a hilarious cartoon. In Davila-Ross\u2019s interpretation, which she includes in a recent research paper, Pia could have been laughing at her recollection of the playful moment with her dad. That surmise can\u2019t be proved, of course, but her spontaneous mirth points to an interplay between memory and emotion that would suggest a more complex inner life than we might have imagined. Watching the video brought an immediate smile to my face. I made a mental note to show it to my wife. Before there was Charlie, Yudhijit Bhattacharjee, a contributing writer for National Geographic , also enjoyed the companionship of a tortoise, a pair of parrots, and a Doberman named Lasso. This story appears in the October 2022 issue of National Geographic magazine. Artificial intelligence is moving fast. We can now converse with large language models such as ChatGPT as if they were human beings. Vision models can generate award-winning photographs as well as convincing videos of events that never happened. These systems are certainly getting smarter, but are they conscious? Do they have subjective experiences, feelings, and conscious beliefs in the same way that you and I do, but tables and chairs and pocket calculators do not? And if not now, then when\u2014if ever\u2014might this happen? While some researchers suggest that conscious AI is close at hand, others, including me, believe it remains far away and might not be possible at all. But even if unlikely, it is unwise to dismiss the possibility altogether. The prospect of artificial consciousness raises ethical, safety, and societal challenges significantly beyond those already posed by AI. Importantly, some of these challenges arise even when AI systems merely seem to be conscious, even if, under the hood, they are just algorithms whirring away in subjective oblivion. We think we\u2019re intelligent, know we\u2019re conscious, and so assume the two go together. Because of these concerns, I signed an open letter put together by the Association for the Mathematical Study of Consciousness (AMCS). Following hot on the heels of the much-publicized call to pause large-scale AI research, the letter argues that it is vital for public, industry, and governing bodies to understand whether and how AI systems could become conscious, to consider the implications, and to address the dangers. Around the same time, Anka Reuel of Stanford University and Gary Marcus, a leading voice on AI, sensibly called for the establishment of a global, neutral, and non-profit \u201cinternational agency for AI\u201d to coordinate global regulation of AI technologies. I think the remit of such an agency should cover artificial consciousness as well. Last week, Geoffrey Hinton, one of AI\u2019s pioneers, resigned as Google\u2019s chief scientist to join the chorus of concern, having changed his mind about the immediacy and reality of the threats posed by the technology he helped develop. In my opinion, we should not even be trying to build conscious machines. To get a handle on these challenges\u2014and to clarify the confusing and hype-ridden debate around AI and consciousness\u2014let\u2019s start with some definitions. First, consciousness. Although precise definitions are hard to come by, intuitively we all know what consciousness is. It is what goes away under general anesthesia, or when we fall into a dreamless sleep, and what returns when we come round in the recovery room or wake up. And when we open our eyes, our brains don\u2019t just process visual information; there\u2019s another dimension entirely: Our minds are filled with light, color, shade, and shapes. Emotions, thoughts, beliefs, intentions\u2014all feel a particular way to us. As for intelligence, there are many available definitions, but all emphasize the ability to achieve goals in flexible ways in varied environments. Broadly speaking, intelligence is the capacity to do the right thing at the right time. These definitions are enough to remind us that consciousness and intelligence are very different. Being intelligent\u2014as humans think we are\u2014may give us new ways of being conscious, and some forms of human and animal intelligence may require consciousness, but basic conscious experiences such as pleasure and pain might not require much species-level intelligence at all. FLESH AND BLOOD: Being conscious is not the result of some complicated algorithm running on the wetware of the brain. It is rooted in the fundamental biological drive within living organisms to keep on living. Photo by NadyaEugene / Shutterstock. This distinction is important because many in and around the AI community assume that consciousness is just a function of intelligence: that as machines become smarter, there will come a point at which they also become aware\u2014at which the inner lights come on for them. Last March, OpenAI\u2019s chief scientist Ilya Sutskever tweeted, \u201cIt may be that today\u2019s large language models are slightly conscious.\u201d Not long after, Google Research vice president Blaise Ag\u00fcera y Arcas suggested that AI was making strides toward consciousness. These assumptions and suggestions are poorly founded. It is by no means clear that a system will become conscious simply by virtue of becoming more intelligent. Indeed, the assumption that consciousness will just come along for the ride as AI gets smarter echoes a kind of human exceptionalism that we\u2019d do well to see the back of. We think we\u2019re intelligent, and we know we\u2019re conscious, so we assume the two go together. Recognizing the weakness of this assumption might seem comforting because there would be less reason to think that conscious machines are just around the corner. Unfortunately, things are not so simple. Even if AI by itself won\u2019t do the trick, engineers might make deliberate attempts to build conscious machines\u2014indeed, some already are. Here, there is a lot more uncertainty. Although the last 30 years or so have witnessed major advances in the scientific understanding of consciousness, much remains unknown. My own view is that consciousness is intimately tied to our nature as living flesh-and-blood creatures. In this picture, being conscious is not the result of some complicated algorithm running on the wetware of the brain. It is an embodied phenomenon, rooted in the fundamental biological drive within living organisms to keep on living. If I\u2019m right, the prospect of conscious AI remains reassuringly remote. But I may be wrong, and other theories are a lot less restrictive, with some proposing that consciousness could arise in computers that process information in particular ways or are wired up according to specific architectures. If these theories are on track, conscious AI may be uncomfortably close\u2014or perhaps even among us already. This lack of consensus about consciousness, when set against the rapidly changing landscape of AI, highlights the need for more research into consciousness itself. Without a principled and experimentally verified understanding of how consciousness happens, we\u2019ll be unable to say for sure when a machine has\u2014or doesn\u2019t have\u2014it. In this foggy situation, artificial consciousness may even arise accidentally, perhaps as a byproduct of some other functionality the tech industry installs in the next generation of their algorithms. There are two main reasons why creating artificial consciousness, whether deliberately or inadvertently, is a very bad idea. The first is that it may endow AI systems with new powers and capabilities that could wreak havoc if not properly designed and regulated. Ensuring that AI systems act in ways compatible with well-specified human values is hard enough as things are. With conscious AI, it gets a lot more challenging, since these systems will have their own interests rather than just the interests humans give them. Attributing humanlike consciousness to AI leads to unjustified assumptions. The second reason is even more disquieting: The dawn of conscious machines will introduce vast new potential for suffering in the world, suffering we might not even be able to recognize, and which might flicker into existence in innumerable server farms at the click of a mouse. As the German philosopher Thomas Metzinger has noted, this would precipitate an unprecedented moral and ethical crisis because once something is conscious, we have a responsibility toward its welfare, especially if we created it. The problem wasn\u2019t that Frankenstein\u2019s creature came to life; it was that it was conscious and could feel. These scenarios might seem outlandish, and it is true that conscious AI may be very far away and might not even be possible. But the implications of its emergence are sufficiently tectonic that we mustn\u2019t ignore the possibility. Certainly, nobody should be actively trying to create machine consciousness. Existential concerns aside, there are more immediate dangers to deal with as AI has become more humanlike in its behavior. These arise when AI systems give humans the unavoidable impression that they are conscious, whatever might be going on under the hood. Human psychology lurches uncomfortably between anthropocentrism\u2014putting ourselves at the center of everything\u2014and anthropomorphism\u2014projecting humanlike qualities into things on the basis of some superficial similarity. It is the latter tendency that\u2019s getting us in trouble with AI. Many people, including some experts, are already projecting sophisticated cognitive competences into AI systems\u2014large language models in particular\u2014on the basis of largely anecdotal and frankly sketchy evidence. But do these models really understand anything? Do they\u2014as has been claimed\u2014have a theory of mind (the ability to attribute mental states, such as beliefs and desires, to others)? Claims that language models possess these capabilities usually rest on suggestive pieces of dialogue. This sort of evidence is remarkably weak, as any psychologist will tell you. It is even weaker when we seek to extrapolate from human experience to a machine. Although humans would need to be cognitively sophisticated to engage in some of the dialogue that chatbots are now capable of, the same conclusion does not hold for AI. Language models may well be able to participate in sophisticated linguistic interactions without understanding anything at all. The lack of true understanding in language models is revealed by their tendency to confabulate: to make stuff up, spouting nonsense in confident language. When I asked Open-AI\u2019s GPT-4 to write a biography of me, it stated wrongly that I was born in London. When I asked it to do it again with fewer errors in dates and places, it got things even more wrong, saying I was born in Hammersmith, London\u2014a revealing answer since being more specific is more likely to be wrong, as anybody who understands anything knows. Future language models won\u2019t be so easy to catch out. Before long, they may give us the seamless and impenetrable impression of understanding and knowing things, regardless of whether they do. As this happens, we may also become unable to avoid attributing consciousness to them too, suckered in by our anthropomorphic bias and our inbuilt inclination to associate intelligence with awareness. Systems like this will pass the so-called Garland Test, an idea which has passed into philosophy from Alex Garland\u2019s perspicuous and beautiful film Ex Machina. This test reframes the classic Turing Test\u2014usually considered a test of machine intelligence\u2014as a test of what it would take for a human to feel that a machine is conscious, even given the knowledge that it is a machine. AI systems that pass the Garland test will subject us to a kind of cognitive illusion, much like simple visual illusions in which we cannot help seeing things in a particular way, even though we know the reality is different. This will land society into dangerous new territory. By wrongly attributing humanlike consciousness to artificial systems, we\u2019ll make unjustified assumptions about how they might behave. Our minds have not evolved to deal with situations like this. If we feel that a machine consciously cares about us, we might put more trust in it than we should. If we feel a machine truly believes what it says, we might be more inclined to take its views more seriously. If we expect an AI system to behave as a conscious human would\u2014according to its apparent goals, desires, and beliefs\u2014we may catastrophically fail to predict what it might do. Trouble is on the way whether AI merely seems or actually is conscious. Our ethical attitudes will become contorted as well. When we feel that something is conscious\u2014and conscious like us\u2014we will come to care about it. We might value its supposed well-being above other actually conscious creatures such as non-human animals. Or perhaps the opposite will happen. We may learn to treat these systems as lacking consciousness, even though we still feel they are conscious. Then we might end up treating them like slaves\u2014inuring ourselves to the perceived suffering of others. Scenarios like these have been best explored in science-fiction series such as Westworld , where things don\u2019t turn out very well for anyone. In short, trouble is on the way whether emerging AI merely seems conscious, or actually is conscious. We need to think carefully about both possibilities, while being careful not to conflate them. And we need action, too. Along with new institutions like the one that Marcus and Reuel propose, there should be major investment into research into consciousness within the mind and brain sciences, so that we can be better informed when developing and responding to new generations of AI. (This research will also benefit society in many other ways, for example in medicine, law, and animal welfare.) Accelerated research is also needed in social sciences and the humanities to clarify the implications of machines that merely seem conscious. And AI research should continue, too, both to aid in our attempts to understand biological consciousness and to create socially positive AI. We need to walk the line between benefiting from the many functions that consciousness offers while avoiding the pitfalls. Perhaps future AI systems could be more like oracles, as the AI expert Yoshua Bengio has suggested: systems that help us understand the world and answer our questions as truthfully as possible, without having goals\u2014or selves\u2014of their own. Like the technology itself, attitudes toward AI are at an inflection point. Time is short to develop a rational and fit-for-purpose framework to ensure that the enormous potential of AI is used for the benefit of humanity and the planet. The implications of either true or apparent artificial consciousness must be part of the conversation. We need to bring to the forefront of our collective awareness the importance of understanding awareness itself. Anil Seth is a professor of cognitive and computational neuroscience, at the University of Sussex, co-director of the Canadian Institute for Advanced Research Program on Brain, Mind, and Consciousness, and an advanced investigator of the European Research Council. He is the author of Being You: A New Science of Consciousness. Lead image: Peshkova / Shutterstock Get the Nautilus newsletter Cutting-edge science, unraveled by the very brightest living thinkers. ChatGPT is a hot topic at my university, where faculty members are deeply concerned about academic integrity, while administrators urge us to \u201cembrace the benefits\u201d of this \u201cnew frontier.\u201d It\u2019s a classic example of what my colleague Punya Mishra calls the \u201cdoom-hype cycle\u201d around new technologies. Likewise, media coverage of human-AI interaction\u2014whether paranoid or starry-eyed\u2014tends to emphasize its newness.In one sense, it is undeniably new. Interactions with ChatGPT can feel unprecedented, as when a tech journalist couldn\u2019t get a chatbot to stop declaring its love for him. In my view, however, the boundary between humans and machines, in terms of the way we interact with one another, is fuzzier than most people would care to admit, and this fuzziness accounts for a good deal of the discourse swirling around ChatGPT.When I\u2019m asked to check a box to confirm I\u2019m not a robot, I don\u2019t give it a second thought\u2014of course I\u2019m not a robot. On the other hand, when my email client suggests a word or phrase to complete my sentence, or when my phone guesses the next word I\u2019m about to text, I start to doubt myself. Is that what I meant to say? Would it have occurred to me if the application hadn\u2019t suggested it? Am I part robot? These large language models have been trained on massive amounts of \u201cnatural\u201d human language. Does this make the robots part human?AI chatbots are new, but public debates over language change are not. As a linguistic anthropologist, I find human reactions to ChatGPT the most interesting thing about it. Looking carefully at such reactions reveals the beliefs about language underlying people\u2019s ambivalent, uneasy, still-evolving relationship with AI interlocutors.ChatGPT and the like hold up a mirror to human language. Humans are both highly original and unoriginal when it comes to language. Chatbots reflect this, revealing tendencies and patterns that are already present in interactions with other humans.Creators or mimics?Recently, famed linguist Noam Chomsky and his colleagues argued that chatbots are \u201cstuck in a prehuman or nonhuman phase of cognitive evolution\u201d because they can only describe and predict, not explain. Rather than drawing on an infinite capacity to generate new phrases, they compensate with huge amounts of input, which allows them to make predictions about which words to use with a high degree of accuracy.This is in line with Chomsky\u2019s historic recognition that human language could not be produced merely through children\u2019s imitation of adult speakers. The human language faculty had to be generative, since children do not receive enough input to account for all the forms they produce, many of which they could not have heard before. That is the only way to explain why humans\u2014unlike other animals with sophisticated systems of communication\u2014have a theoretically infinite capacity to generate new phrases.There\u2019s a problem with that argument, though. Even though humans are endlessly capable of generating new strings of language, people usually don\u2019t. Humans are constantly recycling bits of language they\u2019ve encountered before and shaping their speech in ways that respond\u2014consciously or unconsciously\u2014to the speech of",
    "gpt_response": "Original question: 'Anthropomorphization of Generative AI and its effect on Human Psychology and Behavior'\n\nNew research questions based on the user query:\n1. How does the perception of AI as 'human-like' influence trust and reliance on technology in daily tasks and decision-making processes?\n2. What are the psychological impacts of forming emotional attachments to AI entities, and how do these relationships affect human social interactions?\n3. How does anthropomorphizing AI change the way we understand intelligence, consciousness, and the boundaries between humans and machines?\n\nSummary of provided sources:\n\n1. Wired's article discusses the tendency of humans to anthropomorphize AI and animals, highlighting the psychological underpinnings of why we attribute human-like qualities to non-human entities. It explores the implications of this behavior on our interactions with technology, particularly AI, and how it affects our perception of machines as companions or entities with their own consciousness. The article suggests that anthropomorphization can lead to an overestimation of AI's capabilities and a misunderstanding of its limitations, potentially impacting our reliance on and trust in AI systems.\n\n2. The Scientific American article delves into the fascinating possibility of using AI to understand animal communication, specifically focusing on projects like the Earth Species Project and Project CETI, which aim to decode animal vocalizations. It discusses the technological advancements that have made it possible to analyze complex animal sounds and the implications of this research for conservation efforts and our understanding of animal intelligence. The article also touches on the ethical considerations and potential ramifications of achieving two-way communication with animals, raising questions about the nature of consciousness and the relationship between humans and other species.\n\n3. The discussion on AI consciousness and its potential impacts on human psychology and behavior is enriched by the exploration of anthropomorphism in our interactions with AI. The tendency to attribute human-like characteristics to AI can lead to emotional attachments and influence our trust in these systems. This behavior raises important questions about the nature of intelligence and consciousness, challenging our understanding of the boundaries between humans and machines. The research on decoding animal communication further complicates these issues, suggesting that the ability to communicate with other species could redefine our perceptions of intelligence and consciousness beyond the human experience.\n\nIn conclusion, the anthropomorphization of generative AI has significant implications for human psychology and behavior, affecting our trust, emotional attachments, and understanding of intelligence and consciousness. The exploration of animal communication through AI technologies further complicates these issues, challenging our perceptions and ethical considerations regarding our relationship with non-human entities. These developments underscore the need for ongoing research and ethical deliberation as we navigate the evolving landscape of AI and its role in our lives and the broader ecosystem."
}
