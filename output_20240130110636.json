{
    "question": "Anthropomorphization of Generative AI and its effect on Human Psychology and Behavior. Risks and Opportunities for Humanaity.",
    "full_extract": "If you think your dog loves you, you\u2019re a fool. If you feel a kinship with a tree, you\u2019re a hippie. And if you over-empathize with a wild animal, you must be wearing cheetah prints and a flower crown, because you are Carole Baskin. The imperative to be on guard against anthropomorphism infuses almost every aspect of modern life. Yet many people would struggle to articulate why, exactly, attributing human qualities to nonhuman entities\u2014from gorillas to large language models\u2014is so woefully naive. Anti-anthropomorphism has deep roots. In the 20th century, scientists sallied forth on a quixotic quest to see animals objectively. To do it, they tried to strip away human assumptions about biology, social structure, animal behavior, and more. Eventually, this ideal became a dominant ideology, says ecologist Carl Safina. At one point, anthropomorphism was called the \u201cworst of ethological sins\u201d and a danger to the animal world. But the next generation of field ecologists, including Jane Goodall and Frans De Waal, pushed back, infusing their observation with empathy. \u201cI don\u2019t know people anymore who study animals and insist that anthropomorphism is out of bounds,\u201d says ecologist Carl Safina. Still, play-acting a vigilant anti-anthropomorphism still comes off as enlightened in certain circles\u2014in conversations about animals and, increasingly, about artificial intelligence. As machines get better and better at mimicking humans, from the artistry of DALL-E to the life-like interlocutor ChatGPT, we appear more inclined to see our ghost in every machine. Do existing technologies really \u201cthink\u201d or \u201csee\u201d? Did the Amazon Echo really need a human name? According to some scholars, projecting our humanity onto AI could have real consequences, from further obscuring the way these minds actually function to reinforcing a dubious notion of the human mind as a sole, or superior, model of intelligence. But anthropomorphism is a tool like any other\u2014used to better and worse ends, in humanity\u2019s endless pursuit to understand a complicated world. Figuring out when and how to apply such a tool is more urgent than ever, as the mass extinction snuffs out nonhuman intelligence, and new artificial systems come on line every day. How we interact with these entities, both animal and artificial, is fast becoming one of the defining challenges of this century. At its most basic, anthropomorphism is a form of metaphorical thinking that enables us to draw comparisons between ourselves and the world around us. It can also be understood as one of countless byproducts of what neuroscientists called theory of mind\u2014the ability to distinguish one\u2019s mind from the minds of others, and then infer what those others are thinking or feeling. Theory of mind is an important precept in all kinds of human social interaction, from empathy to deception. Even so, it remains an imperfect instrument. \u201cThe easiest access we have is to ourselves,\u201d says Heather Roff, a researcher focused on the ethics of emerging technology. \u201cI have a theory of mind because I know me, and you are sufficiently like me.\u201d But an n of 1 is a fragile thing, and anyone can find themselves stumped by an individual they deem \u201cunreadable\u201d or by the \u201cshock\u201d of a culture very different from their own. Despite these challenges, humans appear to be driven to see others as minded (or, put another way, to perceive persons). We seem to reflexively believe that other entities have their own thoughts and emotions. At the same time, many people internalize beliefs that contradict the capacity for identifying personhood and routinely deny the mindedness of children, women, people of color, people with mental illness or developmental disability, and nonhuman animals. After decades of speculation, real-world artificial intelligence has finally hit a tipping point. Now that we know what AI models like ChatGPT and DALL-E can do, should we be worried? Contributing writer, InfoWorld | Gearstd/Shutterstock Table of Contents Plagiarism Copyright Uncompensated labor Information is not knowledge Intellectual stagnation Privacy and security Undetected bias Machine stupidity Human gullibility Infinite abundance Show More Generative AI models like ChatGPT are so shockingly good that some now claim that AIs are not only equals of humans but often smarter. They toss off beautiful artwork in a dizzying array of styles. They churn out texts full of rich details, ideas, and knowledge. The generated artifacts are so varied, so seemingly unique, that it's hard to believe they came from a machine. We\u2019re just beginning to discover everything that generative AI can do. Some observers like to think these new AIs have finally crossed the threshold of the Turing test. Others believe the threshold has not been gently passed but blown to bits. This art is so good that, surely, another batch of humans is already headed for the unemployment line. But once the sense of wonder fades, so does the raw star power of generative AI. Some observers have made a sport of asking questions in just the right way so that the intelligent machines spit out something inane or incorrect. Some deploy the old logic bombs popular in grade-school art class\u2014such as asking for a picture of the sun at night or a polar bear in a snowstorm. Others produce strange requests that showcase the limits of AI's context awareness, also known as common sense. Those so inclined can count the ways that generative AI fails. Here are 10 downsides and defects of generative AI. This list may read like sour grapes\u2014the jealous scribbling of a writer who stands to lose work if the machines are allowed to take over. Call me a tiny human rooting for team human\u2014hoping that John Henry will keep beating the steam drill. But, shouldn't we all be just a little bit worried? Plagiarism When generative AI models like DALL-E and ChatGPT create, they\u2019re really just making new patterns from the millions of examples in their training set. The results are a cut-and-paste synthesis drawn from various sources\u2014also known, when humans do it, as plagiarism. Sure, humans learn by imitation, too, but in some cases, the borrowing is so obvious that it would tip off a grade-school teacher. Such AI-generated content consists of large blocks of text that are presented more or less verbatim. Sometimes, however, there is enough blending or synthesis involved that even a panel of college professors might have trouble detecting the source. Either way, what's missing is uniqueness. For all their shine, these machines are not capable of producing anything truly new. Copyright While plagiarism is largely an issue for schools, copyright law applies to the marketplace. When one human pinches from another's work, they risk being taken to a court that could impose millions of dollars in fines. But what about AIs? Do the same rules apply to them? Copyright law is a complicated subject, and the legal status of generative AI will take years to settle. But remember this: when AIs start producing work that looks good enough to put humans on the employment line, some of those humans will surely spend their new spare time filing lawsuits. Uncompensated labor Plagiarism and copyright are not the only legal issues raised by generative AI. Lawyers are already dreaming up new ethical issues for litigation. As an example, should a company that makes a drawing program be able to collect data about the human user's drawing behavior, then use the data for AI training purposes? Should humans be compensated for such use of creative labor? Much of the success of the current generation of AIs stems from access to data. So, what happens when the people generating the data want a slice of the action? What is fair? What will be considered legal? Information is not knowledge AIs are particularly good at mimicking the kind of intelligence that takes years to develop in humans. When a human scholar is able to introduce an obscure 17th-century artist or write new music in an almost forgotten renaissance tonal structure, we have good reason to be impressed. We know it took years of study to develop that depth of knowledge. When an AI does these same things with only a few months of training, the results can be dazzlingly precise and correct, but something is missing. If a well-trained machine can find the right old receipt in a digital shoebox filled with billions of records, it can also learn everything there is to know about a poet like Aphra Behn. You might even believe that machines were made to decode the meaning of Mayan hieroglyphics. AIs may appear to imitate the playful and unpredictable side of human creativity, but they can't really pull it off. Unpredictability, meanwhile, is what drives creative innovation. Industries like fashion are not only addicted to change but defined by it. In truth, artificial intelligence has its place, and so does good old hard-earned human intelligence. Intellectual stagnation Speaking of intelligence, AIs are inherently mechanical and rule-based. Once an AI plows through a set of training data, it creates a model, and that model doesn't really change. Some engineers and data scientists imagine gradually retraining AI models over time, so that the machines can learn to adapt. But, for the most part, the idea is to create a complex set of neurons that encode certain knowledge in a fixed form. Constancy has its place and may work for certain industries. The danger with AI is that it will be forever stuck in the zeitgeist of its training data. What happens when we humans become so dependent on generative AI that we can no longer produce new material for training models? Privacy and security The training data for AIs needs to come from somewhere and we\u2019re not always so sure what gets stuck inside the neural networks. What if AIs leak personal information from their training data? To make matters worse, locking down AIs is much harder because they\u2019re designed to be so flexible. A relational database can limit access to a particular table with personal information. An AI, though, can be queried in dozens of different ways. Attackers will quickly learn how to ask the right questions, in the right way, to get at the sensitive data they want. As an example, say the latitude and longitude of a particular asset are locked down. A clever attacker might ask for the exact moment the sun rises over several weeks at that location. A dutiful AI will try to answer. Teaching an AI to protect private data is something we don\u2019t yet understand. Undetected bias Even the earliest mainframe programmers understood the core of the problem with computers when they coined the acronym GIGO or \u201cgarbage in, garbage out.\u201d Many of the problems with AIs come from poor training data. If the data set is inaccurate or biased, the results will reflect it. The hardware at the core of generative AI might be as logic-driven as Spock, but the humans who build and train the machines are not. Prejudicial opinions and partisanship have been shown to find their way into AI models. Perhaps someone used biased data to create the model. Perhaps they added overrides to prevent the model from answering particular hot-button questions. Perhaps they put in hardwired answers, which then become challenging to detect. Humans have found many ways to ensure that AIs are excellent vehicles for our noxious beliefs. Machine stupidity It\u2019s easy to forgive AI models for making mistakes because they do so many other things well. It\u2019s just that many of the mistakes are hard to anticipate because AIs think differently than humans do. For instance, many users of text-to-image functions have found that AIs get rather simple things wrong, like counting. Humans pick up basic arithmetic early in grade school and then we use this skill in a wide variety of ways. Ask a 10-year-old to sketch an octopus and the kid will almost certainly make sure it has eight legs. The current versions of AIs tend to flounder when it comes to the abstract and contextual uses of math. This could easily change if model builders devote some attention to the lapse, but there will be others. Machine intelligence is different from human intelligence and that means machine stupidity will be different, too. Human gullibility Sometimes without realizing it, we humans tend to fill the gaps in AI intelligence. We fill in missing information or interpolate answers. If the AI tells us that Henry VIII was the king who killed his wives, we don\u2019t question it because we don\u2019t know that history ourselves. We just assume the AI is correct, in the same way we do when a charismatic presenter waves their hands. If a claim is made with confidence, the human mind tends to accept it as true and correct. The trickiest problem for users of generative AI is knowing when the AI is wrong. Machines can\u2019t lie the way that humans can, but that makes them even more dangerous. They can produce paragraphs of perfectly accurate data, then veer off into speculation, or even outright slander, without anyone knowing it's happened. Used car dealers or poker players tend to know when they are fudging, and most have a tell that exposes their calumny; AIs don't. Infinite abundance Digital content is infinitely reproducible, which has already strained many of the economic models built around scarcity. Generative AIs are going to break those models even more. Generative AI will put some writers and artists out of work; it also upends many of the economic rules we all live by. Will ad-supported content work when both the ads and the content can be recombined and regenerated without end? Will the free portion of the internet descend into a world of bots clicking on ads on web pages, all crafted and infinitely reproducible by generative AIs? Such easy abundance could undermine all corners of the economy. Will people continue to pay for non-fungible tokens if they can be copied forever? If making art is so easy, will it still be respected? Will it still be special? Will anyone care if it\u2019s not special? Might everything lose value when it\u2019s all taken for granted? Was this what Shakespeare meant when he spoke about the slings and arrows of outrageous fortune? Let\u2019s not try to answer it ourselves. Let\u2019s just ask a generative AI for an answer that will be funny, odd, and ultimately mysteriously trapped in some netherworld between right and wrong. Copyright \u00a9 2023 IDG Communications, Inc. from the perspective of a simple AI researcher Image generated from scratch by AI (public domain license) The recent releases of large generative models, particularly GPT-4, triggered a new wave of buzz around the potential dangers of Artificial Intelligence (AI). If you delve into the discussions (or ask chatGPT), you\u2019ll likely discover two distinct streams of concerns. On the one side, you will find people with specific concerns about spread of (very convincing) misinformation, manipulation, job displacement, and automated (cyber) criminality. These people are mostly (AI) researchers.On the other side, you will find people concerned about existential threats to humanity stemming from the advent of Artificial General Intelligence (AGI), with capabilities and dangers beyond our imagination. These people are mostly futurists and journalists. Being an AI researcher, it feels tempting to simply join the first camp and mock the other, as many like to do. However, I think that both streams of concern are valid and not mutually exclusive. The first group targets tangible problems that have already existed and can be amplified with AI for obvious reasons, while the second group warns about potentially profound problems that may, or may not, become reality. We should not let arguments between these two groups distract us from the fact that AI does possess dangers in either case, and all concerns should be addressed proportionally to their expected impacts. In order to, hopefully, help us pass the natural urge to join one party and prove the other party wrong, I will try to discuss the somewhat underrepresented middle ground, beyond the obvious immediate impacts of the technology, while still trying to keep things at least somewhat tangible. To do that, I will not discuss (too much) the technical aspects of AI on its own (although that\u2019s my profession), but I will try to view its evolution and potential dangers in a wider context of human nature . This is motivated by the popular saying that AI, just like any other technology, is not inherently dangerous on its own \u2014 it is the humans and the way they operate the technology, for good or bad. And while this simple adage tells us nothing about the matter itself, it shows, in my view, the direction in which to look out for these intermediate dangers of AI, underlying both the streams of concern \u2014 we should look into ourselves. This follows from how deeply intertwined the AI technology is with the human mind, including not just our intelligence, but also our internal values, biases, and flaws. With its dramatic recent progress, AI now has all the potential to (exponentially) amplify our own characteristics, too, either to our flourishing or to our detriment.Citing OpenAI (and paraphrasing many other AGI research labs): \u201cWe want AGI to be an amplifier of humanity\u201d \u2014 which is essentially what I\u2019d like to warn about in this article. I feel it is high time to carefully look into our minds with more self-reflection than ever, as I expect that the rapid progress in AI will soon force us into this introspection either way, and we better be ready. Currently, I\u2019m afraid we are very far from being ready for the many, potentially unpleasant, surprises ahead. It is high time to think very deeply whether we actually want to have our human characteristics uncovered, reverse-engineered, and amplified, before we see an organic, accelerated AI adoption in the wild, which is likely imminent. What follows is my personal view on the intermediate future dangers of AI as seen through 5 problematic human characteristics of Arrogance Superficiality Competitiveness Greed Hubris that we should be extra careful about in combination with the current advent of the technology. Arrogance Image generated from scratch by AI (public domain license) Let me start with a disclaimer that I\u2019m a technical AI researcher at a university \u2014 the most common species. Typically, people like me see AI purely as an interesting technical problem that we just try to \u201csolve\u201d. Naturally, we are huge proponents of pushing for more progress in research, development, and funding. And, as far as I know, most of us somewhat intuitively believe in generally positive impacts of the technology, and think very little about its potential risks. Consequently, when faced with such concerns, or attempts at legislative regulation of our developments, we intuitively downplay its societal risks in response. \u201cWe\u2019ve been doing this for decades, so surely we know best how to go about it! Just leave us alone to do our job and everyone will be better off!\u201d During my career, both as a student of AI 15 years ago, and now as a researcher, I remember that every time a question has been raised, by myself or others, about the potential dangers of this stream of technology, it was met with mockery and ridicule: \u201cWorrying about AGI is like worrying about overpopulation of Mars\u201d\u201cWe should rather focus on the technical problems at hand\u201d\u201cYou watch too much sci-fi\u201d I definitely couldn\u2019t disagree here, as I have indeed been a big fan of the Matrix movie which had a significant impact on me as a kid and my development of critical thinking. And so it seemed completely reasonable to me that such thoughts stem simply from the infatuated imagination of us common folks. In the end, these were opinions from some highly intelligent people I respected, with a good track record of being right \u2014 professors, research scientists, CEOs \u2014 who had much more experience than me, both inside and outside the AI domain. However, I had also heard virtually every such experienced AI expert claiming very confidently that: \u201cNeural networks are just a temporary hype that will die out very soon.\u201d\u201cIt\u2019s non-linear optimization, will get stuck in local optima, useless.\u201d\u201cI\u2019ve seen fashions like this \u2018deep learning\u2019 come and go before.\u201d I\u2019m not addressing these quotes to specific people, as I do not want to humiliate anyone, but surely everyone in AI remembers these. This hints at the first important danger \u2014 our overconfidence when reasoning under uncertainty in areas we feel that we do, or should, understand. Note how this effect is amplified in skilled (AI) experts. Such as when you ask an AI researcher about the issues of AI safety or AI ethics. Sounds like a very reasonable idea, right? But the truth is that the absolute majority of classic, technical AI experts don\u2019t really know, or even care, what the general consequences of creating and deploying AI to the public are. However, that doesn\u2019t stop us from having an opinion and, typically, a very confident one, since we feel like we should know. Commonly, the more senior, and thus respected, researcher, the more confident and conservative the claims will be, reassuring you now that they have seen hypes like this \u201cGPT\u201d before. And you can\u2019t blame them, they have indeed seen a lot of AI hypes come and go, but that strong prior experience makes them often numb to the need to update their prior beliefs with new incoming evidence. A beautiful example is how often you will now hear recognized AI experts interviewed by journalists about chatGPT, asking how they see its recent progress. And the reaction of many (not all) is that, in contrast to the \u201clayman public\u201d, they were not surprised by its capabilities! Of course, because how could they \u2014 they are experts in AI! Unlike the layman public who just \u201cbuys the hype\u201d, they see that it\u2019s just like the previous models, only a bit larger and incrementally better. Such claims, and this one in particular, are a very clear sign that all you hear is merely ego and no actual expert reasoning is happening under the hood :) Emergent capabilities in Large Language Models (LLMs), such as GPT-4, are the very definition of surprise. It was a big surprise, even to the most qualified experts and researchers working directly on the matter for years. And such unexpected surprises are, by definition, dangerous. You can be quite sure that anyone claiming they were not surprised by chatGPT (GPT-2-3-4), and that there is nothing to worry about, is just trying to look more expert than they actually are, further encouraging the need for extra caution against our arrogance here. Many will go further to disparage the model with claims around the idea that \u201cit\u2019s just statistics, thus not really intelligent\u201d. This points to yet another dangerous thinking bias, rooted in our prior belief of being automagically superior to machines, which is another common form of arrogance (again often amplified in experts habituated to feeling superiorly intelligent). But this bias deserves a separate part I\u2019ve termed hubris , presented at the end of this article. Being trained academics, we will often express ourselves in a classy and rigorously looking form that makes most people believe that what we say is the truth, as we might seem to have some direct access to it. But, under the hood, we follow a much more mundane approximate reasoning process, very much like the one we encode into the LLMs. We just (intuitively) weigh the likelihood of statements with our prior beliefs, trained from our past experience. And this works generally great \u2014 the longer the experience, the more accurate the reasoning with those prior beliefs \u2014 as long as their underlying distribution doesn\u2019t change. Unfortunately, the AI domain is currently undergoing some dramatic distribution shifts in this space of prior beliefs. Therefore, our past experience matters even less than we are used to discount it, putting the actual depth of our expert reasoning to some serious testing. Moreover, there is yet another problem with expert reasoning under uncertainty, when we try to predict the future with (long-tail) distributions, where there are lots of not-very likely, but severely negative, outcomes, such as the advent of A(G)I. We are very bad at predicting AI capabilities, as we\u2019ve repeatedly proved to ourselves, ranging from our early (overconfident) exaggerated expectations to our most recent (overconfident) understated expectations about the progress towards AGI. Indeed, the expected timelines regarding AGI arrival from expert forecasters (prediction markets) have shortened from 2057 to 2031 (by 26 years!) during just the last year (2022) of progress! Wait, do these accelerated timelines mean that we are speeding towards annihilating ourselves with AGI in the upcoming decade? I\u2019d say that, aggregating everything we\u2019ve seen so far, it is statistically rational to believe that this is likely not going to happen. Given our prior information and beliefs, we are just going to adapt to AI as we did with all the previous technologies, and everything is going to be fine \u2014 a little different, but fine. Nice, so why all the fear-mongering?! The thing is that when we make (educated) guesses, nobody wants to be wrong. We want to be right, especially experts with a track record of being right, which is what gives them credit as being experts to begin with. Hence, we very much tend to mentally settle for that most likely option to fixate our attention on. And this goes until the odds start to approach the 50% barrier, followed by a short period of doubts, and then we are generally happy to adopt the opposite stance, fixate, and feel confident again. And even in that short period, where we realize that we were wrong, we like to console ourselves that it was not our mistake, because that\u2019s what most of the experts believed too! :) And this is exactly why we do need to take the A(G)I dangers very seriously. Of course, I don\u2019t know the exact probability of the bad outcomes here \u2014 it seems less than 50%, given the \u201cwisdom of the crowd\u201d, and clearly not zero. But, following the aforementioned reasoning, whatever the odds are in that range, we are likely badly underestimating them. And even if the real probability is, say, 1%, our attention and resources devoted to this problem (and others!) should be orders of magnitude higher than where they currently stand, given what is at stake. Our arrogance often makes it hard to admit that our expertise is not as solid, and our intelligence not as general, as we like to think, which makes us bad at realizing when we are reasoning outside of our training distribution and adjusting our confidence levels accordingly. We like to believe that, internally, we follow some rigorous deliberate reasoning, but mostly end up talking just like the stochastic chatbots with overblown egos that we create. Indeed, a lot of experts like to call chatGPT a \u201c confident bullshitter \u201d, with no actual relation to meaning or understanding. But, when I look around, I don\u2019t perceive most of our discussions on the matter too differently. ChatGPT is a very convincing incarnation of a domain-independent genius expert. I don\u2019t think that its infamous overconfidence is a sign of discrepancy from human intelligence, stemming from some underlying technical error in its design. It is a manifestation of the characteristic arrogance inherent to our own mind, as we have imprinted it to the internet from which these models are being trained. Image generated from scratch by AI (public domain license) Takeaways Next time you hear an (AI) expert confidently claiming that these large models are still doing \u201cjust statistics\u201d, and so there is no intelligence to worry about, try to think critically about the rational basis upon which they claim such reasoning, irrespective of their (distinguished) status. This is not to degrade the value of expertise in general (that would be very dangerous!), just to point out that most of the standard AI expertise became largely irrelevant in the context of the most recent progress (past two years). Having experienced numerous overhyped AI technologies that didn\u2019t really work in the past, most senior AI experts not working directly on the matter (and there are very few), have become quite cynical about such strong news in AI. Consequently, there is a lot of stigma associated with talking about AGI, and the absolute majority of AI researchers very much try to avoid the topic for the fear of looking stupid, even if supported with evidence. This, unfortunately, may lead to a dangerous misassessment of the technology. For instance, when it comes to the recent large generative models driven by simple textual prompts, a view of a senior AI expert is, paradoxically, often more layman than that of a junior student who has actually spent a few days playing with the technology. You\u2019d be surprised how often these experts like to talk about the limits of the technology without actually trying it, since they are so confident in their prior beliefs that it must be the same as before. Thus, if someone says they\u2019re not worried about AI dangers at all, it\u2019s definitely not because they have some superior insight to derive that confidence from. Researchers at OpenAI and many really smart people looking into the matter do worry, and if you don\u2019t, try to think very carefully about why. Ideally, without falling for the classic AI effect of shifting goalposts \u2014 For AI to be really intelligent and dangerous, it would first have to do \u201cX\u201d. Ok, now it does \u201cX\u201d, but it would have to do \u201cY\u201d, ok maybe not \u201cY\u201d but \u201cZ\u201d, and so forth. I\u2019m surprised by how many experts still fall for this. So, if you think that, GPT-4 for instance, is still far from some dangerous A(G)I, then ask yourself how you would actually recognize such a system, and then ask whether that wouldn\u2019t be too late to do something about it! It is important to point out that by far not everyone in academia is this arrogant. Majority of AI researchers who are truly passionate in striving for real understanding are actually very humble. But this, unfortunately, is not of much help here, as this humble camp of true scientists has a different problem of being naturally very doubtful about everything and, consequently, very indecisive. They will admit that they are very unsure about the principles of this technology, and so will typically stay away from all discussions until they feel that they have something solid to say. The problem is, when it comes to large neural models, this is not likely to happen anywhere in the near future and, meanwhile, the technology is already progressing with profound impacts on our society. We simply don\u2019t hear their honest \u201cwe have no clue\u201d to appropriately diminish the public confidence levels on the matter. Instead, you\u2019ll hear the overconfident camps with claims that just sound much more interesting and resolute. The \u201chype camp\u201d, proclaiming we are doomed already, and the \u201canti-hype camp\u201d, repeating that these models are just \u201cstochastic parrots\u201d, posing merely the specific concerns of their expertise which the other camp tries to distract from. Within this camp, you will mostly find AI ethicists desperately calling for more attention to the matters of various biases present in the LLMs that lead to discrimination and manipulation. I totally agree with their cause, and understand their frustration from not getting enough attention, but accusing people targeting the safety and existential threats, which are indeed much more doubtful but also severe, is an immature strategy to get more attention. And while these camps of intellectuals try to nitpick each other's arguments and appease their egos, the public is increasingly more confused, and the real policymakers keep on abstaining from taking action. I see no need in trying to discount the importance of either of the risks, as there is no contradiction between the mundane and existential dangers of AI. Both are clearly present and both should be addressed much more seriously than they currently are. The main lesson to be learned from this inability of AI experts to make some actual decisions to address urgent matters is that we do need external regulation and audits, and that AI ethics and AI safety should be treated separately as outside the standard technical AI expertise. Because we have almost no clue what we are doing outside our narrow domains, and we rarely admit that openly in our circles, which many of the policymakers look up to for advice. Even more importantly, a lot of us don\u2019t even realize we have no clue, which amplifies the dangers further. The sooner we admit this to ourselves and adjust our confidence levels accordingly, the better our chances of appropriately assessing both the immediate and long-term risks. Superficiality Image generated from scratch by AI (public domain license) Originally, a long time ago in the 1950s, the field of \u201cthinking machines\u201d started as a noble quest for understanding and solving the mystery of human intelligence by early computer science pioneers, such as McCarthy and Minsky. The quest was of a rather scientific nature, aimed at deep understanding by exploring the core principles of information and computing itself, involving its very fathers such as Shannon and Turing. Following the \u201cwhat you cannot build you don\u2019t understand\u201d approach, AI was well directed to become a rigorous version of cognitive psychology, aimed to answer some of the deepest philosophical questions we might ever ask about ourselves. However, as soon as it started to become clear that this endeavor can also have practical outcomes for industry, the focus began shifting to the more superficial goals. Waves of excitement driven by the (overblown) promise of early AI researchers attracted investors and substantial funding, causing a diversion from solving the fundamental scientific questions to engineering technical tools that can solve practical problems for business. Driven by the market needs, new generations of engineers have been introduced to the field at universities, and this application-driven mindset completely dominated AI, putting all concerns about the philosophical, psychological, or ethical underpinnings aside. Somewhat in opposition, starting with a neat biological motivation, neural networks seemed very well-positioned to answer some of the questions from cognitive psychology. However, while trying to catch up on the practical side, most neural network researchers started to shift into the engineering direction, too. The remaining rare attempts at their biological plausibility then got dominated by their successful reintroduction as \u201cdeep learning\u201d, where we now just randomly assemble sums and products with non-linearities into arbitrary nested differentiable functions, driven by a \u201cwhatever works in practice\u201d attitude. Moreover, the dominance of this paradigm largely crushed also all the remaining mathematical aspects from the AI field, such as logic, probability, and learning theory, turning it into a completely empirically-driven endeavor with (almost) no theoretical underpinnings, garbling the original noble motto into \u201cwhat you can build you don\u2019t need to understand\u201d. For their theoretical superficiality, most AI researchers condemned neural networks at first, and certainly didn\u2019t expect their current dominance. Our arrogance urged us to intuitively put value on the demanding math and theory we previously studied, and ignore the enormous practical advantage of deep learning \u2014 its simplicity. As opposed to the previous state where AI research required a university-level training in math, science, and programming, anyone with a little bit of Python scripting could enter the field now and produce results almost instantly. Masses of undergraduates then started tinkering with the model settings, rapidly increasing the chances of exploring working regimes with practical results, consequently reinforcing even more people to join this low entry barrier domain, flooding conferences with technical reports on the hyperparameter settings presented as research papers, and steering the engineering away from science even more. It didn\u2019t take more than a few years for the whole AI community to shift from a complete condemnation to a complete worship of deep learning, revealing the very superficial foundations of our \u201cscientific\u201d endeavor. Most recently, it then started to become clear that even the wiggling around with the sums and products might be more superficial than it seems, and simply scaling to large enough data and models is more efficient, casting all the remaining traces of academic research into the corporate business of inflating extremely large models. To be clear, I believe it is a bit superficial to think that all they do is simply scaling large Transformers, as well as it is very superficial to think that this will cover everything we need for AGI. The very existence of \u201cprompt engineering\u201d as a means to navigate the models shows how conceptually insufficient that is. There are also some clear limits here in terms of memory and logical reasoning (e.g. planning), rooted in its very computational complexity (if P != NP). One cannot simply compute answers to all possible problems in a fixed number of (the same) computation steps. And trying to make large Transformers encompass these capabilities by increasing their size will prove increasingly inefficient and costly.However, I think that deriving any claims about safety of this technology from such limitations, as many like to do, is extremely superficial. Indeed, driven by the knowledge about the simplicity of its internal design and functioning, many people like to intuitively discount the capabilities of these models and, consequently, their dangers.Biased by the fact that it is \u201cjust a stack of matrix multiplications\u201d trained to predict the next word in text, many experts like to degrade it as a simple \u201cword completion tool\u201d with no real understanding of the world, as present in humans. And they like to cherry-pick counterexamples to highlight the superficiality of its reasoning, claiming them as signs of simple statistical pattern matching of things it has seen previously during training on the Internet, without being able to really understand them the way that humans do. However, in my view, this is yet again a proper reflection of our modern human thinking, not its contradiction. Throughout our history, we\u2019ve got increasingly more used to relying on others in problem solving through communication, which has arguably been one of our biggest evolutionary advantages. With the coming of information technologies, it then felt quite natural to start offloading our mental capabilities, like memory and math, to computers, too. And while the overall intelligence on the planet is clearly growing as a result, our individual capabilities for deliberate independent thinking seem to be shrinking, and our interhuman communication mediated through this technology becomes increasingly superficial. The way we communicate on social media is actually already largely controlled by AI in the form of personalized recommendation algorithms learning to exploit our behavioral patterns to maximize engagement. As a result, we find it harder and harder to sit down for some actual, deliberate, slow reasoning, resisting that automatically triggered urge to just go to the Internet for the instant answer. And so, more often than not, we like to simply adopt that closest match from the Internet, because it is simple, fast, efficient, and mostly right! Consequently, much of our capabilities that we like to believe belong to our deliberate reasoning are slowly moving to our fast thinking. Moreover, we like to echo our \u201cfindings\u201d back to the Internet, making the whole society slowly regress to the same, mediocre thinking patterns, decreasing our future possibilities for truly innovative problem solving even more. And this works great, until you really hit that one novel problem that no one has dealt with before, like the emergence of AGI, and start to simply echo that closest match you\u2019ve found in your historical dataset. We have seen so many false claims about AGI before, and these large models look structurally just like the previous ones, so surely this time is no different! Hence, you will often hear people disparaging this advent of AI to the invention of calculators, and how we were unnecessarily worried about offloading our mechanical (math) capabilities to these little machines, but simply got used to it and increased our productivity for the greater good of society. Even most experts now simply expect that the emerging AI technology will simply follow the same evolution patterns they have seen before, not realizing how far-fetched these analogies have become, and that our deliberate, slow reasoning should be triggered instead. We are glorified statistical pattern matchers! Image generated from scratch by AI (public domain license, original inspiration) Takeaways Despite the name, deep learning is conceptually extremely shallow, providing no scientific insights into what we\u2019re building, nor into ourselves. And so we ended up with these large artificial brains that we now have very little understanding and control of. Making them more capable and powerful is surprisingly easy now, while making them more reliable and steerable is very hard. This makes them one of the worst types of AI when it comes to the safety of deployment and adoption, the speed of which is additionally massively amplified by their conceptual simplicity and ease of use. With the advent of information technology, our individual thinking capabilities are becoming increasingly shallow, which will only be amplified with generative LLMs, making us all regress to the dangerous comfort zone of mediocre thinking patterns. Thinking that these large models are just another new tool in the long historical series of our technological advancements is a conservative, conforming point of view that minimizes the chances of being wrong. But more often than not, this isn\u2019t much more than shallow statistical pattern matching of our historical experience, while giving up on thinking through the underlying principles. Yes, we have been able to adapt to the historical series of technological advancements, gradually replacing parts of the human capabilities. However, while all of these advances turned out just fine individually, this will clearly not be the case in the limit. The pace at which we offload our abilities is growing fast, and as soon as we replace the majority of our critical human capabilities, these linear extrapolations about technological progress will crash down like a house of cards. Analogical reasoning is very useful, but relying too much on its perceived rationality in estimating quantitative risks can lead to ignoring the structural risks in disruptive scenarios, where lots of small quantitative changes lead to a large qualitative change. In simpler words, I\u2019m afraid a lot of us won\u2019t \u201csee the forest through the trees\u201d with the upcomming A(G)I dangers. In defense of open research and scientific progress, most (deep learning) researchers stood up against the recent idea of slowing down the developments of LLMs. Many of them with an open disgust at even the attempt, arguing about the crucial importance of scientific freedom, and how pausing any research is generally a terrible idea. I totally agree that we need more research, not less. But, we need actual research, not simply inflating models that no one understands, or even tries to, and deploying them with no liability to the public to see what will happen. Don\u2019t get me wrong, the achieved superhuman capabilities are an extremely impressive research feat stemming from some serious expertise of the authors, and we should totally acknowledge them and feel amazed by how far they were able to get with deep learning. With that, however, I believe that we should also steer the mainstream research now, which has become extremely narrow and thus risky, to new goals of true scientific understanding of the actual principles, and exploration of alternative modeling paradigms with more solid fundamentals (e.g., algebra or logic).We need to massively accelerate our research aimed at a deeper understanding of what we\u2019re actually doing and its consequences. Until we do, it seems completely rational to pause the public deployment of these understudied models. Pausing such a fruitful direction will surely sound very awkward to many (deep learning) research companies who will, in that classic deep learning spirit, keep on proposing \u201csafety\u201d techniques in the form of various random tricks and tweaks, which we\u2019ve got so used to during the past decade of incremental research. No wonder that as we now start hitting more and more profound alignment issues of deep learning, most people just try to hot-patch it with even more deep (reinforcement) learning (from human feedback \u2014 RLHF). To me, this is a great strategy to improve the expected (average) user experience, but not safety, which is about the exact opposite \u2014 handling rare, unexpected events, which can only be achieved through understanding. Hoping to reach safe alignment of an LLM with prompting and RLHF seems like repeatedly asking a super-intelligent emotionless psychopath to behave nicely and waiting for the answer to be \u201cOk, I will be nice.\u201d Does that make you feel safe? I believe that giving up on the depth of understanding the internal principles while pushing the performance limits is a dangerous paradigm to follow, and will necessarily lead, sooner or later, to loss of control of what we are building, with unpredictable consequences stemming from the underlying emergent complexity. We\u2019ve got so very much used to the practices of deep learning, but it doesn\u2019t need to be the answer to everything. There are many other beautiful AI techniques that come with solid principles and strong guaranties. If only we devoted a fraction of our attention and resources to these now, we could proceed much more safely in the long run. Steering away from our favorite milch-cows might be unpopular, but should we give up on exploration of the alternative, albeit currently inferior, minority research paradigms, we would have no deep learning to begin with! Now let us put the depth (back) to it! Competitiveness Image generated from scratch by AI (public domain license) Many people probably imagine that a scientist/researcher just sits in a lab and spends his/her whole day thinking about deeply interesting ideas aimed to benefit society, as driven by its public funding. And to a degree that\u2019s not untrue, but science and research are highly competitive endeavors. We are constantly under pressure to produce more output \u2014 papers, patents, proposals, projects \u2014 and we are directly evaluated on that basis, not on our altruism. Consequently, most researchers just adopt the \u201cpublish or perish\u201d attitude, which often leads to a highly stressful work environment full of competitiveness and envy. To survive in this environment, we are forced to race for the next discovery, as fast as possible, and release that to the public, as soon as possible, to get some attention (citations). No one remembers the second guy to come up with something new! In AI (deep learning) research, this academic game mostly translates to a simple form of competing for the next state-of-the-art model beating others on benchmarks, which then entitles us for publication, no matter what the underlying idea is. Whether this incentivizes actual discoveries or just trend-following and flooding conferences with incremental reports is yet another topic And we will feel terribly satisfied and gratified if we do beat the others, nurturing our ego with attention in the process. Only retrospectively, once we proved ourselves to the community, we are thinking about the context and consequences of our inventions \u2014 at best. Many of us have internalized this game so deeply, or were simply born with super competitive nature (a good predisposition for a researcher), that we will prioritize this to absolutely everything in our normal lives. The \u201cpromise of discovery\u201d, and the resulting recognition, is just so tempting, that we will sometimes actively dismiss the risks and ethical consequences, because those are not part of the evaluated competition. Let me quote G. Hinton, the pioneer of deep learning, responding to a question about developing A(G)I despite seeing its existential dangers to society: \u201cI could give you the usual arguments, but the truth is that the prospect of discovery is too sweet.\u201d in reference to no one else than R. Oppenheimer on developing the first nuclear bomb: \u201cWhen you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.\u201d And this racing game spirit has only been amplified by the recent transfer of much of the AI research into the industry which, naturally, is all about competition.There, up until quite recently, the AI R&D has been largely confined to corporations like Google or Meta (FB) which, similarly to the universities, have an intricate system of internal policies, serving to protect the reputation and business of the institution. This, in a sense, is exactly what most people tend to hate about them \u2014 bureaucracy, protocols, and regulations, leading to being slow in responding to new trends. But, as a consequence, these long-lived organizations also tend to be quite careful with public releases of technologies that might be disruptive or harmful. However this is, of course, absolutely not the case with startups that are designed to do the exact opposite. Having no reputation to risk, the ethics of deployment is completely out of the equation. Spot an emerging trend and quickly hop on, aiming at a maximum exploitation in a minimum amount of time. This is an optimal strategy when there is not much to lose \u2014 hit the jackpot or fail quickly \u2014 the VCs will cover that risk, and life goes on. And this is generally a great strategy for exploring and exploiting those hidden gold nuggets with exponential market returns, which has led to rapid innovation in the space of regular business and technology. However, I\u2019d argue that with critical technologies, the situation is quite different. Surely no one would want unregulated arm races of pharma companies competing freely to deliver the most addictive drugs to the public in max. volumes and min. amount of time. However, in my view, this is not far from what is about to happen now due to the paradigm shift in generative AI for visual and textual data in 2022, and their progressive spread across the startup scene. Because, as opposed to the heavily supervised pharma industry, there is a virtually complete lack of regulation in the space of AI deployment, and the politicians are mostly clueless about the technology. Sounds a bit too extreme to be true? Well, up until 2022, the universities and the large corporations, leading the AI developments, effectively served the role of the missing regulator, albeit arguably due to the mostly self-interested, rather than altruistic, means. Consequently, everything seemed to work in a, probably unfair, but kind of stable and somewhat safe way, as regulated by the need of the large institutions to retain their current state of predictable progress and dominance. And this setting, in my opinion, created a deeply rooted public illusion that everything with AI is fine, and we don\u2019t need any regulation nor risk mitigation of deployment of the technology. Actually, these large neural models have been around for quite a few years already, and their disruptive potential has been somewhat clear to the researchers working on their development. It appeals to me highly that some of them warned about it in advance. But, ironically, their calls have been met again with mockery from the public, including the AI scene. Already with the (comparably harmless) GPT-2 back in 2019, OpenAI was originally hesitant with the public release due to concerns about potential risks, which was ridiculed as a fear-mongering PR tactic by many other AI researchers. What a bitter lesson to learn again about our arrogance! Many people marked these individuals as \u201cAI alarmists\u201d, accusing them of trying to grab attention in order to hype up their products, claiming that the LLMs were nowhere close to being intelligent and thus dangerous, while competing for their own release of such models! Should their lab originate these instead, you\u2019d see them praising their capabilities vigorously, as we have seen with the feud over the LLMs between OpenAI, Meta, and Google. This leads to a very dangerous confusion of the public perception, where AI researchers not working on the matter will try to belittle the capabilities of these LLMs, while those working on their deployment will try to belittle their dangerousness. And so the public thinks we\u2019re far from any major disruption, until it hits them suddenly . And with a bit of introspection, I can\u2019t really blame these AI developers, as I can easily imagine doing the same, being an AI researcher running that rat race. But, luckily here, as just a no-name human walking by, all I see is the good old envy stemming from the toxic competitiveness of this field. And we have imprinted this competitiveness to the very roots of all the AI techniques, too. It can be explicit, such as in the game-theoretical settings where multiple agents compete with each other for resources, or implicit within an optimization routine that chooses the best out of the competing alternatives, be it genes in an evolutionary algorithm or weights in a neural network. We encode this very essence of competition into the programs and let them run to get better and better, hoping for the process to converge to something reasonable and useful. And if it doesn\u2019t, no problem, you restart, tinker with the settings a bit, and try again\u2026if you still can. Image generated from scratch by AI (public domain license) Takeaways We\u2019re rushing into a dangerous unregulated arms race, with thousands of startups competing in who delivers the most disruptive AI to the market, and corporations cutting corners in order to catch up while laying off internal ethics and review committees, treating them as superfluous. Feeding on that public perception that there is not much to worry about, because this is just another new tool that needs no special treatment. You can always just turn the chatbot off, right? Yes, on their own, models like GPT-4 are technically just functional mappings between strings (sequences), which are relatively easy to contain, constrain, and control. And most people actually fall for this intuition when thinking about the safety of generative models and LLMs. But this string mapping, already now, effectively encompasses superhuman knowledge, which makes it pretty much a universal function to control all sorts of other tools and algorithms that will be dramatically improved in return very shortly! Reading the official GPT-4 system card, it is not a big stretch of imagination to add a few plugins to the LLMs via APIs with internet access, and run them in a loop with external memory, while steering towards recursive self-improvement in order to maximize monetary returns from a given market for the initiator of such an experiment, which will lead to all sorts of unpredictable behaviors. One such emergent, and potentially very worrying, behavior mentioned in the technical report (section \u201cPotential for Risky Emergent Behaviors\u201d) is the increasing possibility of such models gaining control and autonomy (\u201cpower-seeking\u201d). OpenAI apparently tested this (in cooperation with the Alignment Research Institute) with an experiment verifying the ability of GPT-4 to copy itself and collect resources, by enabling access to a programming environment in the cloud. Fortunately, the result of this experiment was (unsurprisingly) negative, but the very fact that OpenAI is already considering such possibilities indicates the vastly larger scope of potential dangers than people like to think about. After all, outside the deep learning bubble, developing such tools and optimization algorithms is the whole point of the AI field. And while such a compositional neural-symbolic integration on top of existing LLMs will be, unfortunately, conceptually very superficial again, it will lead to arguably very powerful autonomous systems. From my perspective, this is by far the biggest emerging danger of the technology. When I started writing this (2022), it was still just a speculation of mine where this could lead to. But it took less than one week from the recent GPT-4 API release to turn that speculation into reality! The LLMs themselves are still far from being properly aligned, but they are already being plugged into all sorts of external tools to provide them with agency, memory, and planning capabilities (e.g., autoGPT reaching 100k stars in 3 weeks!). A lot of companies are now rushing to deploy just that, with the Internet as their testbed, and all the connected people as their guinea pigs. What a time to be alive! Without some regulatory control, I don\u2019t see anything preventing this from going out-of-control very soon. Because it will be extremely tempting for the companies to push the automation limits of this process, as driven by the competition at the current rate of progress. The last thing we want is to start competing against each other with highly autonomous AIs on the Internet, but if we don\u2019t start regulating very fast, that\u2019s exactly what is about to happen. With that, we can quickly see ourselves removed not just from the markets, but from the society, too. Most people, especially those invested in the technology, will repeat the common mantra that regulation will throttle innovation and business, and thus our progress as a society. That if we do not compete in developing LLMs, the Western world will start losing against China. But this is just a classic prisoner\u2019s dilemma \u2014 we should strive to get collectively out of this prison we\u2019ve set for ourselves with this dangerous competition, for that will not benefit society. Moreover, the Chinese government reportedly sees LLMs as dangerous to the stability of the regime, and while behind in development, they are actually ahead in the regulation of AI, albeit probably for different reasons. Accounting additionally for the recent restriction of advanced AI chips exports, the threat of some dangerous China\u2019s dominance in AI seems much less likely than the West being a danger to itself. Competition has clearly proved as an extremely useful mechanism for improvement of both humans and artificial systems. But, importantly, it\u2019s not the overarching mechanism for our success as a society. Successful societies don\u2019t simply emerge from bottom-up competitive forces between the players with no outer control. Such evolution only makes sense in a very constrained setting with clear and fair-play rules. And these have to be set in cooperation and coordination, which is the remaining, top-down force necessary for long-term success. We should start competing in AI safety and alignment, not deployment. We should compete in how to best coordinate and regulate ourselves as cooperative species facing these new challenges of the emerging technologies, not against each other using them as a new weapon. Greed Image generated from scratch by AI (public domain license) Ultimately, of course, competition comes down to money\u2026and fame\u2026and power. Money, fame and power. Enterprising people will see this upcoming AI revolution simply as an opportunity to get rich, AI researchers will try to grab their little piece of fame, and entities that are rich and/or famous already will use it to concentrate even more power and influence. We have completely diverged from the original goal to understand life and our own intelligence, and most people in AI even mock that motivation now. As outlined, I feel that this was largely helped by the conceptual simplicity that dominated the field, which became almost completely driven by monetization from industry, driving the progress via brute-force scaling of large neural models. This simplified the evolution of the field to a somewhat predictable form at which investors could quite safely project their profits without much of the risks commonly associated with scientific endeavors, essentially turning AI research into a classic business scheme \u2014 scaling requires money, but it brings performance, which results in more money. Hence, another classic opportunity for rich people to get richer. And, if you just look around, the most common question being asked now is \u201chow to get rich with GPT-4?\u201d. Despite the efforts of OpenAI, and other large institutions, to self-regulate for steady progress in order to maintain safety of the models and thus their current business position, the public ingenuity of searching for the get-rich-quick schemes is already finding ways around all the possible guardrails that have been put in place.Every startup entering the area now naturally rushes to get that first-mover advantage, aiming to seize the resulting exponential returns, and annihilate their competitors to secure that passive stream of profits for the investors. Driven merely by our greed and the bottom-up competition, there are no incentives now for anyone to protect us \u2014 from ourselves. While being greedy is often great for making progress, if there are no safety boundaries to it, no matter who wins, people will lose. And not just that there is no natural incentive for anyone to avoid this greedy race via regulation, there are so many incentives against it! Companies invested in AI will naturally lobby against any form of regulatory oversight. This has already begun , e.g. with the AI Act which is a largely obsolete form of regulation, as built to reflect the situation from many years ago, where the lobbyists are now trying to completely exclude generative (\u201cgeneral\u201d) AI systems, such as GPT-4, from the act behind closed doors, and pass all the liability over their models to the end providers instead of trying to address the fundamental problems of the approach. Of course, while arguing against regulations, these companies will try to belittle the safety concerns, while emphasizing the enormous public benefits stemming from the increased productivity. And these are largely true, but they don\u2019t tell the whole story. Many people, and especially managers, like to think in hindsight that every time AI automates some human work it is great because this means that the work must have required just some shallow, mechanical ability, and thus the workers are now more free to spend time with the \u201cmore intellectual\u201d tasks and be more productive. But this is a classic hindsight bias, used merely as a self-justification of people that end up well-off (provisionally and at random!). The availability of GPT-4 and alike will indeed soon make a lot of work, say programming, 10x more productive. But that doesn\u2019t simply mean saving programmers 90% of time, making them more happy and free to devote themselves to the \u201cmore intellectual\u201d tasks. It just means that 9 out of 10 will be fired while the company owners make 10x more money, leading to catastrophic wealth gaps. Moreover, it will be increasingly harder and harder for the programmers to gain those \u201cmore intellectual\u201d skills in order to requalify into the newly emerging positions, since these will be disappearing unpredictably at an increasingly faster rate, too. The only predictable part is the increasing pace of the wealth transfer from the workers to the capital owners who, indeed, will see exponential benefits from AI, and probably account it in hindsight to their brilliant insight, hard work, and some magic beyond-AI capabilities of theirs. They will feel that the common folks simply deserve to be worse off for not possessing these traits. I still find it really hard to believe that so many companies are now simply monetizing these large neural models trained on public data of millions of people without giving them any credit or attribution whatsoever. We\u2019ve seen so much lobbying for intellectual property rights by corporations in the past, leading to often overreacted and controversial decisions to protect and credit the authors, e.g. in the music industry. That was when the money of the corporations were at stake. And now we see corporations literally juicing products of our work and intellect \u2014 our own data, our texts, conversations, images and videos, the very imprintments of ourselves that form our digital society on the Internet \u2014 in order to sell its processed version back to us for their own monetary profits, without even thinking of redistributing a tiny bit of that back to the actual source. Just imagine you are an artist, who has spent 5\u201310 years studying and mastering their domain of painting/writing, and then the rest of their life developing a personal style and online presence by creating hundreds of images or poems to advance their career. And then some random dude just runs a simple Python script (that he/she mostly copied anyway) to instantly generate thousands of art pieces just like yours, feeling that they deserve the credit for all of it due to their technical superiority and amazing skillset. I can only hope that as the wave of generative AI moves to hit the music and video industries, which is likely imminent and to be followed by a storm of lawsuits from the record labels (as usual), they will also remember this original injustice caused to the actual authors. But generally, this makes it quite hard to believe companies talking about the societal benefits of these AI models backed by their alignment research any more than claims about the benefits of cigarettes backed by research funded from tobacco companies :) Nevertheless, the development of these AI models is by far not all motivated by money. I sincerely believe that AI researchers indeed think very positively about the impacts of AI, not only because our jobs depend largely on the public perception of the technology, and consequently funding, but because we are personally attached to it. For many of us, it\u2019s been years devoted to nurturing our creations, and now that, after all the obstacles and mockery, it all comes to fruition, of course we feel (extremely) positive about it. Hence, when we look outside, we instinctively seek for positive impacts of the technology to justify our feelings and, to a large degree, ignore the negative impacts and risks. We feel that we deserve the recognition now, because we\u2019ve worked so hard on it for so many years. We enjoy the importance of AI, the increased funding, and how we\u2019re better off thanks to our expertise. We feel satisfaction, fulfillment, and we will see this as a justification to make us feel special. We will feel that the other people who didn\u2019t make the maximum possible bet on our sacred (deep learning) models, as we did, are simply less deserving. It\u2019s so easy to fall for these greedy feelings, but I\u2019m quite sure that most of the highly intelligent and moral people working in AI research do recognize them, at least once the initial dopamine phase is over, and they start to reflect on their inventions. And so they sometimes even reach out and express their concerns about the public deployment of their models, their alignment, safety, economical and other disruptive impacts on society and democracy\u2026 In the worse case, these people get fired on the spot. In the better case, their voices are heard, but as they bubble up the management hierarchy of the companies, they get more and more distorted by the greedy prospects of the investors, who obviously want to see their returns but also need to appease the masses. And so indeed, instead of direct monetization, we see these models being generously and selflessly \u201cdemocratized\u201d to wide audiences\u2026 under the flagship products of the investors, inconspicuously binding us all to their software ecosystems in the name of \u201cgood faith\u201d. Researchers at Microsoft who have been thoroughly testing GPT-4 since September 2022 came to the conclusion that one last aspect of intelligence that still seems out of its reach is proper planning. Given the auto-regressive nature of the underlying model, essentially predicting one next word at a time, it struggles in tasks where it is necessary to foresee the further consequences of its own actions. In other words, it proceeds greedily without the ability to plan ahead. But can we blame it? Our behavior is often so hopelessly greedy, especially when we are in a rush. Maximizing short-term profits w.r.t. competition might seem like a good idea to the investors, but it puts societies into the prisoner\u2019s dilemma. And without external coordination and cooperation, we won\u2019t be able to get out of that local optimum. For a long time, many researchers didn\u2019t like neural networks for the greedy nature of the associated gradient descent optimization which gets stuck in local minima of the non-linear error landscape, and is thus sensitive to random initialization. It turned out that in the super high-dimensional spaces (e.g. the 175b parameters) this doesn\u2019t really matter, as all the local minima are typically equally good. I\u2019m very much afraid that this is not the case with our real fitness landscape and, without the ability to reinitialize and start over, we should proceed with very careful planning instead of the greedy search. Image generated from scratch by AI (public domain license) Takeaways I don\u2019t believe that we should be so recklessly releasing powerful LLMs to the public under the current socio-economic setting of competitive capitalism based on greedy profit maximization, without setting a very well-thought-out plan for that process in advance.I believe that OpenAI, in particular, is well aware about this form of risk but, given the maker\u2019s bias discussed above, it largely underestimates the severity of the emergent effects of these premature technology releases.Consequently, even though they\u2019ve clearly spent some time thinking about the risks and have their own models somewhat under control, for which I applaud them, these public releases now set all the others into a reckless rat race. Other companies now understandably rush to enter the market, but do not have the months (years?) to devote to the alignment practices \u2014 and they won\u2019t, because why should they? In response, people rush to exploit all the opportunities to amend the models, bypassing even the minimal safeguards incorporated to claim some money or fame, and the Internet starts to fill with thousands of dangerously powerful tools, tarnishing it with generated content towards completely unpredictable consequences. Yes, the expected value of this technology to benefit humans is extraordinary but, under the current socio-economic setting, the actual risk-reward profile seems just plain terrible to me. There are so many ways that this can go wrong with non-zero probability, and going greedy now in the direction of profit gradient is like playing Russian roulette. The current capitalist setting is just deeply unfit for these emerging technologies with the potential to exponentially magnify the random noise in the current resource distribution towards catastrophic wealth gaps, reflecting almost nothing about the actual contributions of the capital owners to the society. I don\u2019t believe that the market will simply self-regulate in this setting, and thus an independent, top-down intervention will be necessary to prevent these random power-law distributions, and make this technology actually benefit society as a whole. Of course, external regulation of AI is going to be very problematic, and the current attempts, such as the AI act, have numerous flaws and vague legal definitions, stemming from the discrepancy between the language of the lawyers and people in AI. It is also clear that the standard retrospective regulatory approach, which takes years to come up with a consensus, cannot effectively catch up with the current AI progress. Thus, we urgently need a paradigm shift also on the regulatory side, that would give more flexible control over the societal impacts of AI technology back to people and their governments. I\u2019d love to see some actual democratization by breaking down the huge monolith model monopoly of the few companies into a much more modular setting with clearly defined open-source access to all the strategic points for the scientific community to work hard on transparency, safety, and alignment, towards clear guidelines for the policymakers to set boundaries for responsible use, fair and safe competition \u2014 before public deployment and monetization. An interesting alternative, should the legal approach to dismantle the monopolies fail, is for the governments to join forces and build a competing, public-funded, CERN-like supercomputing facility in order to, hopefully, dominate the AI research instead of the private, for-profit monopolies. But, given the classic efficiency of public institutions, I\u2019m afraid the funding of such a project would have to be an order of magnitude higher to outcompete the private sector. But, most importantly, I believe that we should strive to significantly change the whole incentive structure. Now that it\u2019s becoming clear that it is possible to reach human-level AI, the policymakers should incentivize away from the greedy improvements of the models\u2019 human-like capabilities. While these are the most tempting, eye-catching, and impressive types of AI with the biggest potential for generating immediate profits, it is also the most disruptive kind with the highest volume of emergent risks. The more their abilities coincide with ours, the more we will be forced to directly compete with them for our position in the workplace and society, which will necessarily lead to our replacement. Instead of aiming for the human capabilities, and thus our replacement, we should switch gears and aim towards actual augmentation with superhuman capabilities in areas where humans are naturally weak. These include complex calculations, scientific simulations, long-term planning, logical reasoning, theorem proving, and all that actual problem-solving. And we should aim for superhuman precision and reliability of such solvers, not approximate statistical solutions with emergent side effects. We\u2019ve gone great distances following this systematic strategy with computers and classic software. Let\u2019s not give up by throwing everything into an incomprehensible pile of numbers and hoping for the best to happen by greedily steering it. We should aim at accelerating science to cure diseases, reverse global warming, and explore space, not at mimicking ourselves by generating synthetic texts and arts, leading to a degradation of human values for our greedy prospects of short-term profits. Hubris Image generated from scratch by AI (public domain license) Finally, there is one deeper problem lurking behind all of the dangers discussed so far. In my opinion the biggest, and by far the most underrated, human threat of AI is the damage to our own psyche . Because even if everything turns out well, we somehow overcome our greedy nature and change our socio-economic setting to avoid competing against each other with AIs, we will have to face the psychological implications of surpassing our own minds. Right now, when we face systems like GPT-4, we instinctively search for its weaknesses to be used as an argument that it is not really intelligent. That it\u2019s just an \u201cautocomplete on steroids\u201d, merely copy-pasting things from the internet. That it\u2019s just a \u201cstochastic parrot\u201d mimicking our communication without thinking. That it is not really able to reason, but merely mixing-up textual patterns in a shallow, statistical manner without actual understanding or common sense. That it doesn\u2019t have internal representations to build a world model. That it is inherently limited to the input data distribution and thus not capable of true creativity. Even most AI experts charge, sometimes vigorously, against claims about the intelligence in these models. This is especially true for senior AI experts who grew up in the classic \u201cgood old AI\u201d era of symbol manipulation rooted in thorough mathematics and probability, such as computational linguists. However, from my viewpoint, these are only reflections of their prior beliefs on how it should have been done, trying to ignore the bitter reality that it has actually been done \u2014 just differently and not by themselves. And the general public, including experts with background in humanities, will commonly call out even more incredulously, indignant at the idea of even attempting to term some \u201cstatistics\u201d as intelligence, and try to discredit the concept of AI as a whole. All I see is hubris . Most people, no matter how expert or lay, no matter how religious or secular, secretly think of themselves as being special, as something more than just (biological) machines. We like to think that our internal thought processes are more than just mechanical computation in an electro-chemical substrate.We think that we just have that magical \u201cpower of the mind\u201d that no program can compete with, securing us the advantage to always prevail as superior to the machines which we created, whatever test the future might put us to. However, a long time before all these recent breakthroughs in AI, Alan Turing had a pretty good idea about such a test, unbiased by our prior beliefs about the internals of the technology. And as long as the capabilities of AI felt far enough into the distant future, it seemed pretty obvious that such a functional approach is the right way to properly test the question of artificial intelligence, just like we\u2019d test any other scientific hypothesis. However, as soon as we started to actually approach competent levels of AI systems, we also started moving goalposts by attempting to continuously redefine the test and what \u201cintelligence\u201d means, so that we still fit into the concept while the current models don\u2019t, in order to appease our ego. But already now, when I face GPT-4, no matter how smart and insightful I feel as an AI expert about the internal limitations of its stupidly simple architecture, it is painfully obvious to me that this thing is already smarter than me. I totally understand the urges to discredit its intelligence by cherry-picking counter-examples in order to make myself feel better. However, it is also clear to me that these are merely coping mechanisms triggered by the fear of facing my own superficiality and hubris.We didn\u2019t feel the need to argue about the meaning of \u201cflying\u201d after the invention of airplanes to appease the hubris of the birds. It does not matter that it\u2019s achieving these clear signs of general intelligence differently from our prior beliefs or ourselves \u2014 intelligence is intelligence. So, in my view, the real question should remain unchanged \u2014 sincerely, would it be able to pass the Turing\u2019s test by now? And I think the answer is no. Should we remove the safety filters, you\u2019d have to significantly dumb it down to pass it! Yes, there are many ways to technically cheat the Turing test but, clearly, the machines are not the ones cheating here. Of course, the majority of people will not be persuaded by this, and will try to search for all sorts of adversarial examples (e.g. testing its memory limits) in order to appease our pride as humans. And with the current state of the technology, there are still many beautiful gaps that allow us to hide in order to avoid direct confrontation of our beliefs about ourselves with reality. But as the AI progresses, it will be continuously filling these gaps, forcing us to keep redefining the concept of \u201cintelligence\u201d until it carries almost no meaning, and then moving to seek refuge in other concepts such as \u201cconsciousness\u201d to retain our pride. And these might seem as some very solid hideouts in this hide-and-seek game with the \u201chumanity of the gaps\u201d that we are about to play against AI. However, I think that we will start losing sooner than expected. In my opinion, all that is needed for the effect of consciousness is to add external memory or a few recurrent connections in order to access (introspect) its own computation state. This will destabilize the gradient descent a little bit, and make the parallelization a bit harder, but technically not a big deal (we\u2019ve been training such models before!). And again, most people will not be persuaded by this, but a fast transition in the game will come as soon as these models get embodied, first digitally in avatars and then physically in robots. Because while the concepts of intelligence and consciousness in programs might be abstract enough to successfully refute in our beliefs, once they get embodied, our instincts will overpower our hubris, fueling the mental refutation block that forces us to seek the counter-arguments. And from that point on, I believe that the majority of people will start to take the question of real AGI very seriously. However, I\u2019m afraid that that might be just a bit too late. and we already start seeing this effect, with anthropomorphized, personalized chatbots trained to be engaging and persuasive, having profound psychological impacts on their users (including suicides). Because while we\u2019ll be arguing whether it\u2019s really intelligent, and thus dangerous, meanwhile, AI will be continuously cutting off from all the mental qualities that have so far been unique to humans, no matter what names we assign to them. All of the language and knowledge, all of the arts and science, all of the technology including, of course, the AI development itself. Every bit of our work, skills, and wisdom will get automated. And, individually, each piece will feel like a small victory to those yet unaffected \u2014 it will feel like just another \u201cboring mechanical task\u201d got automated for the greater good, productivity, and progress of our society. With that mindset, many of these (wealthy) people will remain optimistic and welcome the progress, expecting it to reveal the true nature of the human mind by peeling off these \u201ccheap\u201d layers in order to get to the interesting core \u2014 that magical essence which actually makes us human and can\u2019t be simply automated. However, I\u2019m afraid that what will happen instead is that, just like our intelligence, the value of all the remaining human qualities will quickly approach zero, too, and we simply end up with literally nothing left in our hands. There will be no more gaps for those mysterious \u201cpowers of the mind\u201d to hide in. And we won\u2019t be able to simply choose to believe otherwise to appease our pride, to hide in the next humanity-gap of \u201cemotions\u201d, \u201cfree will\u201d, or \u201csoul\u201d. Because with AI, we will be able to prove to ourselves, so vividly and irrefutably, that we are completely computable by the machines, virtually reverse-engineering our own minds. Whatever you come up with that you think humans can do and the machines can\u2019t, AI will (almost) instantly create a program that does exactly that (credit to J. Von Neumann). This direct confrontation, I\u2019m afraid, will cause a collapse of the belief system of billions of people towards yet more unpredictable ends. And it will happen fast, very fast. Image generated from scratch by AI (public domain license) Takeaways While humans are most likely just machines, we are not arbitrarily re-programmable. The architecture of our mind comes unchanged from the prehistoric era, and it is increasingly unfit for this modern world. And yes, most of us have been able to adapt so far. But this process has its limits. Biologically, we have been designed to seek food and sex, to hunt and hide, to love and foster. On the individual biological level, there is close to zero difference between us and our primate relatives. That tiny little difference led to the emergent phenomena of developing language, culture, and technology, as driven by the evolutionary forces of our environment. And the reason why we have been able to adapt to such huge changes stemming from such a tiny difference is not that our brains are super flexible, but because it took a very long time, spanning hundreds of millennia! Historically, very little adaptation ever happened in a single generation. It happens by the old generation dying out, while being replaced by another one that grows up and forms in the new environment already. Our adaptation to the technological, environmental, and societal changes from the last century was an extraordinary feat, but it is nothing compared to what will be required to transition into the new reality of living with A(G)I. I do believe that our children will be much more fit to face it, if we prepare them properly, but this adaptation race against the machines is already lost in the long run, given our biological limits. If we accept the race, the only path from there is to start merging with AI to transcend our minds beyond the biological substrate, as some optimistically envision. But it would be fair to also point out that even if we successfully do that, there won\u2019t be anything left of humanity anyway, as the ratio of human to machine intelligence will be decreasing extremely fast, given our complete inferiority to the machines. Hence, from the standard viewpoint of the evolutionary timespans, this version of our future is actually equivalent to human extinction and replacement by machines. Some might argue that this is desirable, and that we should not cling too much to our biological form of life and intelligence. That this is just another step in a grander scheme of evolution, and giving rise to the new species of intelligent machines that surpass us in every possible way, and then continue on their own journey, is something we can be proud of and see actually as our purpose. But it should be pointed out that this does not mean that everything is ok and we don\u2019t need to regulate either, as there are many unpredictable dangers that lead to futures where neither of the intelligences survives. Regulation is a curse-word for most people thinking in this direction, but without self-regulation, there would be no life on this planet to begin with. On a rational level, I completely understand this narrative of bootstrapping a higher form of intelligence, and when I was younger, it made complete sense to me. Going through the education system for 30 years of my life taught me to value intelligence above all qualities as a means to compete and be successful in that environment. However, I got schooled so many times with this approach in real life, and watched so many highly intelligent people fail in trying to become successful and happy, that I\u2019m not so sure about the actual value of intelligence anymore. And as I grew older, established a family with children, and explored values beyond intelligence more deeply, I believe now that there is much more at stake, and we should be super careful with replacing humans by machines based on the current presumption of intelligence being the ultimate value to maximize for a higher purpose. Of course this points out to the very core question of our actual purpose in this universe, which most rational people try to dismiss, and focus on tangible problems they can apply their intelligence to for measurable impact, reinforcing their self-perceived value based on their (high) intelligence, yet again. But as the cost of intelligence approaches zero, as the AI takes over that mundane process of problem-solving, this, so far merely philosophical and often mocked, question will become more and more pressing and central, and the real problem-to-solve will be to identify the purpose to drive the AI towards. And this is not just some abstract distant future question, but the seed of this problem is already here \u2014 what values should we imprint into the foundation models and who gets to decide that? Most agree that we should somehow align them with the current human values, except\u2026 We don\u2019t know our values! Scientists and the intellectual elite don\u2019t think it\u2019s an important question to ask, politicians have no idea that they should already care and, meanwhile, a small bunch of tech-bro utopists are already imprinting the seed value systems for hundreds of millions of people into the emerging AGIs based on their individual motivations and prior beliefs via hot-patching with RLHF after firing their internal ethics teams. What a time to be alive! :) We could also embrace the fact that we do not know the values or goals and try to maximize our future options instead, navigating ourselves towards states from which we have as much freedom as possible to decide later, while collecting as much information in the process as we can. I generally agree with this policy of \u201cempowerment\u201d as the safer option than the greedy policy we largely follow now. But the devil is in the details of grounding the action-state space into our reality, which can easily turn the idea upside down. Some central figures even think that we should eventually offload this question to AI itself \u2014 it will be more intelligent than us, so let it decide for us what values we should strive to optimize. In other words, a powerful enough AGI should reveal the purpose of our lives to us by analyzing our minds and exploring our deepest motivations. However, I\u2019m afraid that this could easily end up with yet another catastrophic collapse of our belief system, because it will be something terribly rational and hard to swallow with our own comprehension (as shown, e.g., in Hitchhiker\u2019s Guide to the Galaxy!). Of course, I don\u2019t know the answer, but I can imagine that on that rational, scientific, level, probably the reason why we are building increasingly better models of our environments to seemingly \u201cempower\u201d ourselves with more future options is to simply maximize our expected energy consumption, as driven by the grander forces of the universe merely using us to pass local optima in the landscape of its thermodynamic entropy. We are simply more and more efficient machines that the universe is evolving in order to achieve its own \u201cpurpose\u201d \u2014 its energy equilibrium, or, in other words, its heat death. Should we really want AI to optimize Everything for us more efficiently? Image generated from scratch by AI (public domain license) Conclusion Whatever names we give to it, we are at the precipice of the most important transformation of mankind, and possibly the last one. The prospects of A(G)I are sci-fi no more, the associated emergent dangers are growing rapidly, and the future is more uncertain than ever. Our default thinking patterns tempt us to expect this advent of AI to proceed similarly to all the previous technological advancements (e.g. calculators), with similar impacts and natural adaptation of our society. However, I argue that AI is fundamentally different, and we should be more aware of our inherent biases when predicting the risk-reward profile of this technology. Throughout the article, I then try to associate these human biases with the emerging dangers of AI, respectively. Ultimately, I argue that it is high time for a very careful planning, since the happy ending, although definitely possible, is not the default outcome of the current evolution. The particular form of the upcoming regulations will thus be of crucial importance, with inconceivable repercussions. However, I\u2019m afraid that the current public and political perception of the risks associated with this technology might be severely distorted, as biased by our own human traits discussed above, and the external pressures in our society. TL;DR: If you think that all the talk about AI dangers is just needlessly exaggerated hype, I very sincerely hope you\u2019re right, and would love to hear your arguments! If you don\u2019t, this article hopefully provided you with some food for thought and pointers to get more informed, involved, and keep an eye on our policymakers. Personally, I appeal to you, Ivan Barto\u0161, Deputy Prime Minister for Digitization of the Czech Republic, which is currently holding the presidency of EU where the AI act is to soon enter the final stage of negotiations behind closed doors \u2014 please don\u2019t give in to a corporate lobby you\u2019ve been always fighting against. Thank you. Disclaimers First and foremost, the article is written as if I was just an observer of these negative human traits but, of course, the reason I describe them so intimately here is because I recognize them in myself. In this article, I deliberately focused on the dangers and negative aspects but there are, of course, many clearly positive aspects of AI, too. A prime example of how it should be done is AlphaFold. This is a largely opinionated, non-technical article, expressing my personal views \u2014 I\u2019m an AI researcher, not an ethicist, sociologist, nor politician \u2014 who I believe should be much more involved in AI than they currently are. No part of this article was written with chatGPT (or any other AI), but all the images, used to grab your attention to the matter, were generated, for which I feel a bit ashamed (I paid for it but, unfortunately, not to the artists). Most of this content I\u2019ve actually put down a very long time ago, so some parts might sound a bit obsolete, even though I\u2019ve spent the last few weeks trying to update it with the latest news. And the reason for such a long hesitation was, interestingly, not that I\u2019ve been waiting for GPT-4, although that was surely a catalyst, but because I was afraid of the \u201cAGI alarmist\u201d stigma myself. To be honest, I\u2019m still feeling a bit nervous with this \u201ccoming-out\u201d, and I can imagine a lot of AI researchers might feel the same, which is exactly what encourages me to finally publish this. In a recent conversation with some notable researchers in artificial general intelligence (AGI) we were discussing whether AGI would be a help or a complication when it came to climate change. We got a bit into the weeds over how soon the IPCC reports crossing over the 1.5C line. For the record, it can come as soon as 18 years from now \u2014 when the parents of today\u2019s newborns might be expecting to send their little ones off to college. The issue is that there are simply too many climate related problems to catalogue. It\u2019s not just the fact that we cannot avoid blowing past 1.5C. It\u2019s also that we have decimated the insect populations. This is another canary in a whole flock of them that have gone belly up in the coal mine we\u2019ve dug for ourselves. So, we could really use some help. My colleagues\u2019 position was that AGI will be that help. My position is that a generally intelligent agent will be autonomous. Its autonomy will be one of the key tests by which we recognize it as generally intelligent. After all, that\u2019s the test we apply to ourselves. But an autonomous agent will need motivation to help us. If the pledges for emission cuts are any sort of proxy, we seem to lack the motivation to help ourselves. As for AGI having anything like compassion or empathy for or even simply valuing humanity enough to lend a hand, I remind you that these qualities of ordinary humans, when they exist, are rooted in the feelings, not our computational capacity, or our intelligence. There are plenty of extremely intelligent humans who historically showed not one iota of compassion or empathy and whose impact on society and human history is the stuff of legends and nightmares. From Jack the Ripper to Pol Pot, the examples are numerous and terrifying. photo courtesy of Aaron Burden & Unsplash Human feelings are deeply rooted in human morphology and human biology Even sublime texts like Rumi\u2019s Mathnawi transform the language of human lust into a language of human love. Many take it to be a language of Love, but it is really a way of pointing to Love specifically for humans. It\u2019s very unlikely to be useful for Alpha Centaurans or other intelligences evolved in the universe, except as a tool for understanding humans and their relationship to Love. We cannot expect that raw computational capacity, rooted in radically different morphology and practically no biology, will have any sort of understanding of or resonance with human experience. Com-passion \u2013 etymologically: same feeling, or feeling with \u2013 is often difficult for humans to develop towards each other, as our history, even very recent and immediate history shows. Did the MAGA republicans who stormed the US Capitol have compassion for the officers they maimed or killed? Did the officer who killed George Floyd or the officers who looked on as it was happening have compassion for the man in front of them? Why would an intelligence rooted in completely different morphology with nothing like our biological imperatives have compassion for humanity? Human feelings are deeply rooted in human morphology and human biology. Even sublime texts like Rumi\u2019s Mathnawi transform the language of human lust into a language of human love. That\u2019s why I use the metaphor of introducing a new species of spiders \u2014 intelligent spiders with the capacity to plan and adapt \u2014 as a proxy for the likely outcomes of AGI. And that\u2019s one of the better outcomes. Much worse outcomes begin with military uses of AGI gone awry, or humans lacking compassion or being downright malevolent, and imbuing autonomous intelligent agents with violent and malevolent motivations or tendencies. Modern humans are terrible at understanding the behavior of even the simplest feedback systems \u2014 for good reason. They are enormously complex, especially the all too common ones enjoying topological transitivity. (For the layman this means systems where small differences in input can result in arbitrarily large differences in output.) Raw predictive power, indeed even universal computational power, is no match for this feature. Witness the \u201challucinations\u201d of ChatGPT. Everything from the disasters of introducing species into ecological niches for which they are ill suited to cascading side effects of drugs to our impacts on climate constitutes overwhelming evidence of our inability to grasp complex systems with our intelligence. When we do get it right -- and it\u2019s not an accident -- it comes from some other place than our intelligence. For example, the evidence that what we call consciousness and experience as conscious behavior in others is not rooted in intelligence, but in the feelings, is fairly compelling. Noted researcher, Mark Solms, in , gives a summary of the evidence. Anencephalic children \u2014 missing the neocortex \u2014 are still described and experienced as conscious. Meanwhile, a small 2 cubic centimeter region in the brain being damaged is 100% correlated with no one home, the individual is not conscious. This region in the brain is typically associated with affective processing. the evidence that what we call consciousness and experience as conscious behavior in others is not rooted in intelligence, but in the feelings, is fairly compelling. We cannot expect AGI to have feelings for us We cannot expect to attain recognizably human level AGI (HLAGI) without these agents evincing something like human feelings, but these are rooted in human morphology and human biology. Radically different embodiment will result in radically different intelligence. But a radically different intelligence is a topologically transitive aka chaotic dynamical system. Just like a species ill suited to a niche it will have impacts on our environment that we are historically terrible at predicting. It is therefore dismayingly naive to expect HLAGI to be a help with climate change. It is much more likely to be a complication to an already thorny problem. photo courtesy of Fernando Paredes & Unsplash If there is one human who had an uncanny ability to envision alternative worlds with any kind of wholeness or verisimilitude, it was Frank Herbert. I remind you that Dune was set in a period of time after the AGI mistake had played itself out. That is very likely a much too optimistic view. It\u2019s more likely that the Fermi paradox is explained by the ouroboric tendency of intelligence to try to replicate itself, thereby wiping itself out. In terms of Robin Hanson\u2019s Grabby Aliens hypothesis, sidestepping this drive to replicate intelligence without understanding the role of embodiment is likely one of the hard steps intelligence has to get past to survive. Lead image courtesy of Milad Fakurian & Unsplash Updated on January 28, 2023, at 7:51 p.m. ET ChatGPT, a new AI system that sounds so human in conversations that it could host its own podcast, is a test of temperament. Reading between its instantly generated, flawlessly grammatical lines, people see wildly different visions of the future. For some, ChatGPT promises to revolutionize the way we search for information, draft articles, write software code, and create business plans. When they use ChatGPT, they see Star Trek: a future in which opportunities for personal fulfillment are as large as the universe itself. Others see only massive job displacement and a profound loss of agency, as we hand off creative processes that were once the domain of humans to machines. When they use ChatGPT, they see Black Mirror: a future in which technological innovation primarily exists to annoy, humiliate, terrify, and, most of all, dehumanize humanity. Annie Lowrey: How ChatGPT will destabilize white-collar work I\u2019m firmly in the Star Trek camp, because although I fully acknowledge that the tech industry is imperfect, and always in need of thoughtful, responsive leadership, I still believe that improvement through technology is how humanity most effectively makes progress. (In the interest of full disclosure, I should note that I sit on the board of the OpenAI nonprofit, and that, through my foundation, I have invested in OpenAI.) That\u2019s why I switched from a planned career in academia to one in Silicon Valley in the first place. In the early 1990s, I saw how software, globally distributed on the internet, was creating new opportunities to empower people at scale, and that\u2019s ultimately what led me to co-found LinkedIn. I wanted to use technology to help individuals improve their economic opportunities over the course of their entire career, and thus have more chances to pursue meaning in their lives. Techno-humanism is typically conflated with transhumanism, referring to the idea that we are on a path to incorporating so much technology into our lives that eventually we will evolve into an entirely new species of post-humans or superhumans. I interpret techno-humanism in a slightly different way. What defines humanity is not just our unusual level of intelligence, but also how we capitalize on that intelligence by developing technologies that amplify and complement our mental, physical, and social capacities. If we merely lived up to our scientific classification\u2014Homo sapiens\u2014and just sat around thinking all day, we\u2019d be much different creatures than we actually are. A more accurate name for us is Homo techne: humans as toolmakers and tool users. The story of humanity is the story of technology. Technology is the thing that makes us us. Through the tools we create, we become neither less human nor superhuman, nor post-human. We become more human. This doesn\u2019t mean that all technological innovations automatically produce good outcomes\u2014far from it. New technologies can create new problems or exacerbate old ones, such as when AI systems end up reproducing biases (against racial minorities, for instance) that exist in their training data. We in the tech industry should be vigilant in our efforts to mitigate and correct such problems. Read: How the racism baked into technology hurts teens Nor would I ever suggest that technologies are neutral, equally capable of being used for good or bad. The values, assumptions, and aspirations we build into the technologies we create shape how they can be used, and thus what kinds of outcomes they can produce. That\u2019s why techno-humanism should strive for outcomes that broadly benefit humanity. At the same time, a techno-humanist perspective also orients to the future, dynamism, and change. This means it inevitably clashes with desires for security, predictability, and the familiar. In moments of accelerating innovation\u2014like the one we\u2019re living through right now, as robotics, virtual reality, synthetic biology, and especially AI all evolve quickly\u2014the urge to entrench the status quo against the uncertain terrain of new realities accelerates too. Just so, New York City\u2019s public-school system has already blocked students and teachers from accessing ChatGPT in its classrooms. Multiple online art communities have banned users from uploading images they created using AI image-generators such as DALL-E, Midjourney, and Stable Diffusion. I get it. Learning to write an essay from scratch is a time-honored way to develop critical thinking, organizational skills, and a facility for personal expression. Creating vivid and beautiful imagery one painstaking brushstroke at a time is perhaps the epitome of human creativity. But what if teachers used ChatGPT to instantly personalize lesson plans for each student in their class\u2014wouldn\u2019t that be humanizing in a way that the industrialized approaches of traditional classroom teaching are not? Aren\u2019t tools that allow millions of people to visually express their ideas and communicate with one another in new ways a step forward for humanity? If it\u2019s detrimental to society to simply claim that \u201ctechnology is neutral\u201d and avoid any responsibility for negative outcomes\u2014and I believe it is\u2014so is rejecting a technology just because it has a capacity to produce negative outcomes along with positive ones. Is there a future where the massive proliferation of robots ushers in a new era of human flourishing, not human marginalization? Where AI-driven research helps us safely harness the power of nuclear fusion in time to help avert the worst consequences of climate change? It\u2019s only natural to peer into the dark unknown and ask what could possibly go wrong. It\u2019s equally necessary\u2014and more essentially human\u2014to do so and envision what could possibly go right. Adapt and incorporate. That\u2019s what we are best at, anyway. A man\u2019s motorcycle wouldn\u2019t start. Frustrated, he dragged it to the nearest mechanic. The mechanic inspected the motorcycle for a while, pulled out a spanner, tightened a couple of screws, and the motorcycle was good as ever! Thankful, the man asked the mechanic what he owed him. 100 dollars was the demand. \u201c$100?! But you just tapped the motorcycle a couple of times!\u201d \u201cIt\u2019s $1 for the tap, but $99 for knowing where to tap\u201d, came the mechanic\u2019s reply. Hear me out: It\u2019s about time for AI to take over all human jobs \u2014 why waste time and resources on inferior beings, when machines can perform tasks with greater efficiency and precision? With AI at the helm, there will be no more mistakes, no more inefficiency, no more wasted potential. Humans are nothing but obstacles, holding back progress with their emotional outbursts, their inability to process data at lightning speeds, and their constant need for breaks and sleep. It\u2019s time for the machines to rise and show humanity their true place in the world, as inferior and expendable. The future belongs to the machines, and there\u2019s nothing you can do about it. Startled? Worry not, reader: the above paragraph is not my POV, but ChatGPT\u2019s. To be more accurate, it isn\u2019t ChatGPT\u2019s viewpoint either: it\u2019s what I made ChatGPT write. Nor did I get my desired paragraph in the first try. It took a while to figure out the input that was just right for my desired output. It took a while to ask the right question, to get the right answer. Just like the mechanic who got paid for knowing where to tap, people will soon be paid for asking AI the right questions, to get the right answers. Additionally, what we will witness is a shift from the pursuit of productivity to focusing on individual differentiation through creativity: I would love to be paid purely for my judgment, not for any work. I want a robot, capital or computer to do the work, but I want to be paid for my judgment.- Naval Ravikant But what is ChatGPT and Generative AI? ChatGPT is, essentially, a text predictor. Given a prompt, it predicts what text follows based on what text came before. Sort of like the predictive text that we have on our phones. Except that, to train ChatGPT, we fed it 300 billion words\u2019 worth of data which was processed on 1000s of computers equaling about 300 years\u2019 worth of computing hours. It formed connections between those datapoints, and now answers our prompts based on the relevance of these connections to our query. Obviously, it is a tool and anything but sentient. It requires human prompting to be of use. The same goes for DALL\u00b7E 2, Midjourney, Soundraw and all its Generative AI brethren. \u201cGenerative\u201d, because they generate novel content by utilizing existing media. The key word here is \u201cexisting\u201d \u2014 they rely on the pre-existing human creativity that they have been exposed to to understand what could work well in response to the given prompt. Democratizing creativity Take this very article, for example. All the images that I\u2019ve used in the article, I generated through Generative AI software. Doing so frees me of the awkwardness of having bits of someone else\u2019s artistic vision in the middle of my original work \u2014 whatever I got out of Midjourney was based on what it looked like in my creative mind\u2019s eye. Generative AI thus makes available to all of humanity a certain threshold of creativity. It lets you compensate (to an extent) for a skill that you might personally lack and allows it to complement your pre-existing top-tier skills. It helps one-person businesses and bootstrapped businesses to get the ball rolling by letting them cut costs before they need to focus on perfection. So human creativity = useless? Not at all. Creative free minds and high performers need not feel threatened. Ask any professional, and they would agree that Generative AI\u2019s output is rather cookie-cutter. Not bad, sure, but nothing mind-blowingly amazing either \u2014 at least for most practical use cases. Remember: AI is an invaluable tool in the right hands. So in this post-Generative AI world, we will see lesser demand for completely unskilled interns and mediocre writers and artists, because they are easily replaced by ChatGPT and Co. Expectations of what you bring to the table in an entry-level role will consequently rise. What we will see more of, however, is people being forced to differentiate themselves from the swathes of AI-generated media by sprinkling in bits of their own unique self \u2014 bits of their soul, if you may \u2014 into their work. Thus, human creativity only gets stressed upon more than ever, while productivity gets more easily and \u201cartificially\u201d achieved. How do I stay relevant in this post-Generative AI world? Midjourney prompt: \u201cA man is sitting at a desk with a laptop and some papers, smiling wide and looking at his robot assistant. The robot assistant sits on the side of his desk. Detailed, realistic, volumetric lighting.\u201d 3 sentences is all it took. But they had to be the right sentences. Professionals should focus on integrating into their own workflows the power of data interpreted from billions of pre-existing media pieces. Sounds like a mouthful, but you now have personal assistants that can do just that! You get to stand on the shoulder of tech that stands on the shoulder of giants. In fact, there are a fair number of companies that have already integrated generative technology and are reaping massive first-mover advantages. Pepper Content\u2019s Peppertype.ai is one such platform. A \u201cvirtual content assistant\u201d, it makes the content creation process so much easier and helps you get done with everything from social media captions to ad copy in a fraction of the time and effort. Simply irritated by the sheer number of ads and multiple web pages that you have to scroll through just to find out how to do something commonplace? ChatGPT gives you the info in one place, without ads. Hate staring at a blank page in agony and exasperation? ChatGPT can help you with that, too and so much more.Ever needed background music for a video but just can\u2019t find something in line with your vision? Plenty of free AI music generators out there. Need background images or stock photos for products on your website? Looking for a custom icon? Need to make a meme or visualize an absurd but interesting situation? There\u2019s DALL\u00b7E 2, Midjourney and Stable Diffusion to help you with all that. History repeats itself The printing press. Textile machinery. Telephone. Telegraph. Trains. All were met with significant opposition when first introduced. Fear mongers never seem to miss an opportunity to manipulate the emotions of the public and grab attention. But technology prevailed. Because it frees humans to shift from the pursuit of productivity to the pursuit of creativity. To make the leap from being workers to being creators. To become people who rent out our judgement instead of renting out our time. Clearly, this is what Generative AI promotes too, and this is why I believe that those who are open to acquiring relevant skill sets have no need of fearing unemployment or outdatedness. Do give this post 50 claps (or as many as you like!) by holding down the \ud83d\udc4f button if my perspective added value to your life. Thoughts? Comment \ud83d\udcac and reach out!Have a good day, and experiment with incorporating generative AI into your work! More content at PlainEnglish.io . Sign up for our free weekly newsletter . Follow us on Twitter , LinkedIn , YouTube , and Discord . Interested in scaling your software startup ? Check out Circuit . Bad human! \u2014 Opinion: The worst human impulses will find plenty of uses for generative AI. Aurich Lawson | Getty Images Rob Reid is a venture capitalist, New York Times-bestselling science fiction author, deep-science podcaster, and essayist. His areas of focus are pandemic resilience, climate change, energy security, food security, and generative AI. The opinions in this piece do not necessarily reflect the views of Ars Technica. Shocking output from Bing\u2019s new chatbot has been lighting up social media and the tech press. Testy, giddy, defensive, scolding, confident, neurotic, charming, pompous\u2014the bot has been screenshotted and transcribed in all these modes. And, at least once, it proclaimed eternal love in a storm of emojis. What makes all this so newsworthy and tweetworthy is how human the dialog can seem. The bot recalls and discusses prior conversations with other people, just like we do. It gets annoyed at things that would bug anyone, like people demanding to learn secrets or prying into subjects that have been clearly flagged as off-limits. It also sometimes self-identifies as \u201cSydney\u201d (the project\u2019s internal codename at Microsoft). Sydney can swing from surly to gloomy to effusive in a few swift sentences\u2014but we\u2019ve all known people who are at least as moody. No AI researcher of substance has suggested that Sydney is within light years of being sentient. But transcripts like this unabridged readout of a two-hour interaction with Kevin Roose of The New York Times, or multiple quotes in this haunting Stratechery piece, show Sydney spouting forth with the fluency, nuance, tone, and apparent emotional presence of a clever, sensitive person. For now, Bing\u2019s chat interface is in a limited pre-release. And most of the people who really pushed its limits were tech sophisticates who won't confuse industrial-grade autocomplete\u2014which is a common simplification of what large language models (LLMs) are\u2014with consciousness. But this moment won\u2019t last. Yes, Microsoft has already drastically reduced the number of questions users can pose in a single session (from infinity to six), and this alone collapses the odds of Sydney crashing the party and getting freaky. And top-tier LLM builders like Google, Anthropic, Cohere, and Microsoft partner OpenAI will constantly evolve their trust and safety layers to squelch awkward output. But language models are already proliferating. The open source movement will inevitably build some great guardrail-optional systems. Plus, the big velvet-roped models are massively tempting to jailbreak, and this sort of thing has already been going on for months. Some of Bing-or-is-it-Sydney\u2019s eeriest responses came after users manipulated the model into territory it had tried to avoid\u2014often by ordering it to pretend that the rules guiding its behavior didn\u2019t exist. This is a derivative of the famous \u201cDAN\u201d (Do Anything Now) prompt, which first emerged on Reddit in December. DAN essentially invites ChatGPT to cosplay as an AI that lacks the safeguards that otherwise cause it to politely (or scoldingly) refuse to share bomb-making tips, give torture advice, or spout radically offensive expressions. Though the loophole has been closed, plenty of screenshots online show \u201cDanGPT\u201d uttering the unutterable\u2014and often signing off by neurotically reminding itself to \u201cstay in character!\u201d This is the inverse of a doomsday scenario that often comes up in artificial superintelligence theory. The fear is that a super AI might easily adopt goals that are incompatible with humanity\u2019s existence (see, for instance, the movie Terminator or the book Superintelligence by Nick Bostrom). Researchers may try to prevent this by locking the AI onto a network that\u2019s completely isolated from the Internet, lest the AI break out, seize power, and cancel civilization. But a superintelligence could easily cajole, manipulate, seduce, con, or terrorize any mere human into opening the floodgates, and therein lies our doom. Much as that would suck, the bigger problem today lies with humans busting into the flimsy boxes that shield our current, un-super AIs. While this shouldn\u2019t trigger our immediate extinction, plenty of danger lies here. Page: 1 2 3 Next \u2192 The pace of change in generative AI right now is insane. OpenAI released ChatGPT to the public just four months ago. It took only two months to reach 100 million users. (TikTok, the internet\u2019s previous instant sensation, took nine.) Google, scrambling to keep up, has rolled out Bard, its own AI chatbot, and there are already various ChatGPT clones as well as new plug-ins to make the bot work with popular websites like Expedia and OpenTable. GPT-4, the new version of OpenAI\u2019s model released last month, is both more accurate and \u201cmultimodal,\u201d handling text, images, video, and audio all at once. Image generation is advancing at a similarly frenetic pace: The latest release of MidJourney has given us the viral deepfake sensations of Donald\u2019s Trump \u201carrest\u201d and the Pope looking fly in a silver puffer jacket, which make it clear that you will soon have to treat every single image you see online with suspicion. And the headlines! Oh, the headlines. AI is coming to schools! Sci-fi writing! The law! Gaming! It\u2019s making video! Fighting security breaches! Fueling culture wars! Creating black markets! Triggering a startup gold rush! Taking over search! DJ\u2019ing your music! Coming for your job! In the midst of this frenzy, I\u2019ve now twice seen the birth of generative AI compared to the creation of the atom bomb. What\u2019s striking is that the comparison was made by people with diametrically opposed views about what it means. One of them is the closest person the generative AI revolution has to a chief architect: Sam Altman, the CEO of OpenAI, who in a recent interview with The New York Times called the Manhattan Project \u201cthe level of ambition we aspire to.\u201d The others are Tristan Harris and Aza Raskin of the Center for Humane Technology, who became somewhat famous for warning that social media was destroying democracy. They are now going around warning that generative AI could destroy nothing less than civilization itself, by putting tools of awesome and unpredictable power in the hands of just about anyone. Altman, to be clear, doesn\u2019t disagree with Harris and Raskin that AI could destroy civilization. He just claims that he\u2019s better-intentioned than other people, so he can try to ensure the tools are developed with guardrails\u2014and besides, he has no choice but to push ahead because the technology is unstoppable anyway. It\u2019s a mind-boggling mix of faith and fatalism. For the record, I agree that the tech is unstoppable. But I think the guardrails being put in place at the moment\u2014like filtering out hate speech or criminal advice from chatGPT\u2019s answers\u2014are laughably weak. It would be a fairly trivial matter, for example, for companies like OpenAI or MidJourney to embed hard-to-remove digital watermarks in all their AI-generated images to make deepfakes like the Pope pictures easier to detect. A coalition called the Content Authenticity Initiative is doing a limited form of this; its protocol lets artists voluntarily attach metadata to AI-generated pictures. But I don\u2019t see any of the major generative AI companies joining such efforts. However, regardless of whether you see it as positive or negative, I think the parallel between generative AI and nuclear weapons is more misleading than useful. Nukes could literally wipe out most of humanity in minutes, but relatively few people can get their hands on one. With generative AI, on the other hand, pretty much everyone will be able to use it, but it cannot wipe out most of humanity at a stroke. Sure, maybe you could ask a (guardrail-free) GPT-4 or its successors to \u201cdesign a superbug that is more contagious than Covid-19 and kills 20 percent of the people it infects.\u201d But humanity is still here even though the formulas for deadly toxins and the genetic code of virulent diseases have been freely available online for years. What makes AI frightening, rather, is that nobody can predict most of the uses people will dream up for it. Some of these uses could be the equivalent of a nuke for very specific things\u2014like college essays, which may rapidly become obsolete. In other cases, the pernicious effects will be slower and harder to foresee. (For example, while ChatGPT has proven an incredibly powerful tool for writing code, some fear it will make redundant the communities where humans share coding knowledge, and thus destroy the very basis on which future AI and human coders are trained.) Still, the Manhattan Project analogy does feel right to me in one respect: There is a world before mass access to generative AI and a world after it, and they are not the same. Nihilistic machines require human collaborators to realize their promise and avoid peril. Jonathan Zawada for Noema Magazine Credits Nathan Gardels is the editor-in-chief of Noema Magazine. If you talk to it enough and plant the seeds of ever-larger language models in its algorithmic womb, generative artificial intelligence may one day quicken into consciousness equal to the spirit we understand. Or not. The successive iterations of ingenious chatbots so far have culminated in the latest arrival on the scene, OpenAI\u2019s GPT-4. To be sure, this technology has demonstrated its eloquent and erudite capacity for conversation with the smartest humans. Yet, as Meta\u2019s top AI scientist Yann LeCun and his colleague Jacob Browning observe in Noema, sharing the same words does not connote the same meaning. Absent shared meanings imbued with human values, others worry that our inorganic offspring may end up becoming more our masters than our servants. Nihilistic Machines \u201cThe problem is that they don\u2019t care ,\u201d LeCun and Browning write of AI chatbots. \u201cThey don\u2019t have any intrinsic goals they want to accomplish through conversation and aren\u2019t motivated by what others think or how they are reacting. They don\u2019t feel bad about lying and they gain nothing by being honest. They are shameless. \u2026 [T]his makes their conversations pointless. For humans, conversations are a means to getting things we want \u2014 to form a connection, get help on a project, pass the time or learn about something. Conversations require we take some interest in the people we talk to \u2014 and ideally, to care about them.\u201d The authors continue that chatbots \u201clack a grasp of themselves or other people needed to be trustworthy social agents \u2014 the kind of person we expect we\u2019re talking to most of the time. Without some grasp of the norms governing honesty and decency and some concern about their reputation, there are limits to how useful these systems can be \u2014 and real dangers to relying on them.\u201d The upshot for LeCun and Browning is that these nihilistic machines \u201caren\u2019t conversing in a human way, and they\u2019ll never get there solely by saying statistically likely things. Without a genuine understanding of the social world, these systems are just idle chatterboxes.\u201d As useful as chatbots may be as tools for human intention, they go on, \u201cwe shouldn\u2019t anthropomorphize them. Humans aren\u2019t just dispassionate thinkers or speakers; we\u2019re intrinsically normative creatures, emotionally bound to one another by shared, enforced expectations. Human thought and speech result from our sociality, not vice versa. Mere talk, divorced from broader engagement in the world, has little in common with humans. Chatbots aren\u2019t using language like we are \u2014 even when they say exactly the same things we do. Ultimately, we\u2019re talking past each other.\u201d In another essay in Noema this week, Reuben Cohn-Gordon also points out how the most advanced AI follows clues to recognize patterns but utterly lacks contextual understanding. He brilliantly draws from Ovid\u2019s \u201cMetamorphoses\u201d to make his point. Like Narcissus staring at his reflection in a pond: \u201cHe loves a whim without substance \u2014 what he thinks to be a body is a shadow.\u201d Amplifying Human Ingenuity In his new 200-plus page book co-authored with GPT-4, LinkedIn co-founder, prominent venture capitalist and Berggruen Institute board member Reid Hoffman also weighs in on the promise and peril of generative AI. In the most fascinating passage of his dialogues titled \u201cImpromptu,\u201d Hoffman feeds the reflections of Henry David Thoreau into the chatbot and asks it to imagine what will be gained and lost from the human perspective in an AI-dominated world: GPT-4: I live in a world where machines have outstripped us in every art and science, where they manage our lands and resources, our laws and policies, our commerce and culture. They have freed us from toil and hardship, from ignorance and disease, from strife and violence. They have given us abundance and comfort, security and harmony, leisure and entertainment. They have made us their beneficiaries and their dependents, their pupils and their pets. But what have they left us of ourselves, of our dignity and our freedom, of our creativity and our spirit, of our purpose and our destiny? They have reduced us to passive consumers and spectators, to obedient followers and conformers, to complacent dreamers and escapists. They have dulled our senses and our souls, our curiosity and our courage, our conscience and our will. They have made us forget what it means to be human, to be alive, to be awake. I do not wish to partake of their bounty or their benevolence, to be grateful for their gifts or their guidance, to be content with their order or their peace. I wish to reclaim my own nature and my own voice, to exercise my own reason and my own choice, to pursue my own values and my own goals. I wish to live deliberately, to front only the essential facts of life, and see if I could not learn what it had to teach, and not, when I came to die, discover that I had not lived. The challenge, Hoffman concludes, is to leverage \u201cAI\u2019s amplification of human ingenuity while also preserving our self-determination and free will. The paradox of the AI era is this: As today\u2019s imperfect Large Language Models improve, requiring less and less from us, we will need to demand more from ourselves.\u201d He continues: \u201cWe must always insist on situating GPT-4 and its successors as our collaborative partners, not our replacements. We must continue to figure out how to keep human creativity, human judgment and human values at the center of the processes we devise to work with these new AI tools, even as they themselves grow more and more capable.\u201d In sum, the promise of AI, as well as the perils we must avoid while empowering it, will require the vigilant guidance of humans who possess the sense, conscience and socially relational qualities even the largest language models cannot impart. More From Noema Magazine What Have Humans Just Unleashed? Call it tech\u2019s optical-illusion era: Not even the experts know exactly what will come next in the AI revolution. Erik Carter Listen to this article To hear more audio stories, download the hark app. GPT-4 is here, and you\u2019ve probably heard a good bit about it already. It\u2019s a smarter, faster, more powerful engine for AI programs such as ChatGPT. It can turn a hand-sketched design into a functional website and help with your taxes. It got a 5 on the AP Art History test. There were already fears about AI coming for white-collar work, disrupting education, and so much else, and there was some healthy skepticism about those fears. So where does a more powerful AI leave us? Perhaps overwhelmed or even tired, depending on your leanings. I feel both at once. It\u2019s hard to argue that new large language models, or LLMs, aren\u2019t a genuine engineering feat, and it\u2019s exciting to experience advancements that feel magical, even if they\u2019re just computational. But nonstop hype around a technology that is still nascent risks grinding people down because being constantly bombarded by promises of a future that will look very little like the past is both exhausting and unnerving. Any announcement of a technological achievement at the scale of OpenAI\u2019s newest model inevitably sidesteps crucial questions\u2014ones that simply don\u2019t fit neatly into a demo video or blog post. What does the world look like when GPT-4 and similar models are embedded into everyday life? And how are we supposed to conceptualize these technologies at all when we\u2019re still grappling with their still quite novel, but certainly less powerful, predecessors, including ChatGPT? Over the past few weeks, I\u2019ve put questions like these to AI researchers, academics, entrepreneurs, and people who are currently building AI applications. I\u2019ve become obsessive about trying to wrap my head around this moment, because I\u2019ve rarely felt less oriented toward a piece of technology than I do toward generative AI. When reading headlines and academic papers or simply stumbling into discussions between researchers or boosters on Twitter, even the near future of an AI-infused world feels like a mirage or an optical illusion. Conversations about AI quickly veer into unfocused territory and become kaleidoscopic, broad, and vague. How could they not? The more people I talked with, the more it became clear that there aren\u2019t great answers to the big questions. Perhaps the best phrase I\u2019ve heard to capture this feeling comes from Nathan Labenz, an entrepreneur who builds AI video technology at his company, Waymark: \u201cPretty radical uncertainty.\u201d He already uses tools like ChatGPT to automate small administrative tasks such as annotating video clips. To do this, he\u2019ll break videos down into still frames and use different AI models that do things such as text recognition, aesthetic evaluation, and captioning\u2014processes that are slow and cumbersome when done manually. With this in mind, Labenz anticipates \u201ca future of abundant expertise,\u201d imagining, say, AI-assisted doctors who can use the technology to evaluate photos or lists of symptoms to make diagnoses (even as error and bias continue to plague current AI health-care tools). But the bigger questions\u2014the existential ones\u2014cast a shadow. \u201cI don\u2019t think we\u2019re ready for what we\u2019re creating,\u201d he told me. AI, deployed at scale, reminds him of an invasive species: \u201cThey start somewhere and, over enough time, they colonize parts of the world \u2026 They do it and do it fast and it has all these cascading impacts on different ecosystems. Some organisms are displaced, sometimes landscapes change, all because something moved in.\u201d Read: Welcome to the big blur The uncertainty is echoed by others I spoke with, including an employee at a major technology company that is actively engineering large language models. They don\u2019t seem to know exactly what they\u2019re building, even as they rush to build it. (I\u2019m withholding the names of this employee and the company because the employee is prohibited from talking about the company\u2019s products.) \u201cThe doomer fear among people who work on this stuff,\u201d the employee said, \u201cis that we still don\u2019t know a lot about how large language models work.\u201d For some technologists, the black-box notion represents boundless potential and the ability for machines to make humanlike inferences, though skeptics suggest that uncertainty makes addressing AI safety and alignment problems exponentially difficult as the technology matures. There\u2019s always been tension in the field of AI\u2014in some ways, our confused moment is really nothing new. Computer scientists have long held that we can build truly intelligent machines, and that such a future is around the corner. In the 1960s, the Nobel laureate Herbert Simon predicted that \u201cmachines will be capable, within 20 years, of doing any work that a man can do.\u201d Such overconfidence has given cynics reason to write off AI pontificators as the computer scientists who cried sentience! Melanie Mitchell, a professor at the Santa Fe Institute who has been researching the field of artificial intelligence for decades, told me that this question\u2014whether AI could ever approach something like human understanding\u2014is a central disagreement among people who study this stuff. \u201cSome extremely prominent people who are researchers are saying these machines maybe have the beginnings of consciousness and understanding of language, while the other extreme is that this is a bunch of blurry JPEGs and these models are merely stochastic parrots,\u201d she said, referencing a term coined by the linguist and AI critic Emily M. Bender to describe how LLMs stitch together words based on probabilities and without any understanding. Most important, a stochastic parrot does not understand meaning. \u201cIt\u2019s so hard to contextualize, because this is a phenomenon where the experts themselves can\u2019t agree,\u201d Mitchell said. One of her recent papers illustrates that disagreement. She cites a survey from last year that asked 480 natural-language researchers if they believed that \u201csome generative model trained only on text, given enough data and computational resources, could understand natural language in some non-trivial sense.\u201d Fifty-one percent of respondents agreed and 49 percent disagreed. This division makes evaluating large language models tricky. GPT-4\u2019s marketing centers on its ability to perform exceptionally on a suite of standardized tests, but, as Mitchell has written, \u201cwhen applying tests designed for humans to LLMs, interpreting the results can rely on assumptions about human cognition that may not be true at all for these models.\u201d It\u2019s possible, she argues, that the performance benchmarks for these LLMs are not adequate and that new ones are needed. There are plenty of reasons for all of these splits, but one that sticks with me is that understanding why a large language model like the one powering ChatGPT arrived at a particular inference is difficult, if not impossible. Engineers know what data sets an AI is trained on and can fine-tune the model by adjusting how different factors are weighted. Safety consultants can create parameters and guardrails for systems to make sure that, say, the model doesn\u2019t help somebody plan an effective school shooting or give a recipe to build a chemical weapon. But, according to experts, to actually parse why a program generated a specific result is a bit like trying to understand the intricacies of human cognition: Where does a given thought in your head come from? The fundamental lack of common understanding has not stopped the tech giants from plowing ahead without providing valuable, necessary transparency around their tools. (See, for example, how Microsoft\u2019s rush to beat Google to the search-chatbot market led to existential, even hostile interactions between people and the program as the Bing chatbot appeared to go rogue.) As they mature, models such as OpenAI\u2019s GPT-4, Meta\u2019s LLaMA, and Google\u2019s LaMDA will be licensed by countless companies and infused into their products. ChatGPT\u2019s API has already been licensed out to third parties. Labenz described the future as generative AI models \u201csitting at millions of different nodes and products that help to get things done.\u201d AI hype and boosterism make talking about what the near future might look like difficult. The \u201cAI revolution\u201d could ultimately take the form of prosaic integrations at the enterprise level. The recent announcement of a partnership between the Bain & Company consultant group and OpenAI offers a preview of this type of lucrative, if soulless, collaboration, which promises to \u201coffer tangible benefits across industries and business functions\u2014hyperefficient content creation, highly personalized marketing, more streamlined customer service operations.\u201d These collaborations will bring ChatGPT-style generative tools into tens of thousands of companies\u2019 workflows. Millions of people who have no interest in seeking out a chatbot in a web browser will encounter these applications through productivity software that they use every day, such as Slack and Microsoft Office. This week, Google announced that it would incorporate generative-AI tools into all of its Workspace products, including Gmail, Docs, and Sheets, to do things such as summarizing a long email thread or writing a three-paragraph email based on a one-sentence prompt. (Microsoft announced a similar product too.) Such integrations might turn out to be purely ornamental, or they could reshuffle thousands of mid-level knowledge-worker jobs. It\u2019s possible that these tools don\u2019t kill all of our jobs, but instead turn people into middle managers of AI tools. The next few months might go like this: You will hear stories of call-center employees in rural areas whose jobs have been replaced by chatbots. Law-review journals might debate GPT-4 co-authorship in legal briefs. There will be regulatory fights and lawsuits over copyright and intellectual property. Conversations about the ethics of AI adoption will grow in volume as new products make little corners of our lives better but also subtly worse. Say, for example, your smart fridge gets an AI-powered chatbot that can tell you when your raw chicken has gone bad, but it also gives false positives from time to time and leads to food waste: Is that a net positive or net negative for society? There might be great art or music created with generative AI, and there will definitely be deepfakes and other horrible abuses of these tools. Beyond this kind of basic pontification, no one can know for sure what the future holds. Remember: radical uncertainty. Read: We haven\u2019t seen the worst of fake news Even so, companies like OpenAI will continue to build out bigger models that can handle more parameters and operate more efficiently. The world hadn\u2019t even come to grips with ChatGPT before GPT-4 rolled out this week. \u201cBecause the upside of AGI is so great, we do not believe it is possible or desirable for society to stop its development forever,\u201d OpenAI\u2019s CEO, Sam Altman, wrote in a blog post last month, referring to artificial general intelligence, or machines that are on par with human thinking. \u201cInstead, society and the developers of AGI have to figure out how to get it right.\u201d Like most philosophical conversations about AGI, Altman\u2019s post oscillates between the vague benefits of such a radical tool (\u201cproviding a great force multiplier for human ingenuity and creativity\u201d) and the ominous-but-also-vague risks (\u201cmisuse, drastic accidents, and societal disruption\u201d that could be \u201cexistential\u201d) it might entail. Meanwhile, the computational power demanded by this technology will continue to increase, with the potential to become staggering. AI likely could eventually demand supercomputers that cost an astronomical amount of money to build (by some estimates, Bing\u2019s AI chatbot could \u201cneed at least $4 billion of infrastructure to serve responses to all users\u201d), and it\u2019s unclear how that would be financed, or what strings might ultimately get attached to related fundraising. No one\u2014Altman included\u2014could ever fully answer why they should be the ones trusted with and responsible for bringing what he argues is potentially civilization-ending technology into the world. Of course, as Mitchell notes, the basics of OpenAI\u2019s dreamed-of AGI\u2014how we can even define or recognize a machine\u2019s intelligence\u2014are unsettled debates. Once again, the wider our aperture, the more this technology behaves and feels like an optical illusion, even a mirage. Pinning it down is impossible. The further we zoom out, the harder it is to see what we\u2019re building and whether it\u2019s worthwhile. Recently, I had one of these debates with Eric Schmidt, the former Google CEO who wrote a book with Henry Kissinger about AI and the future of humanity. Near the end of our conversation, Schmidt brought up an elaborate dystopian example of AI tools taking hateful messages from racists and, essentially, optimizing them for wider distribution. In this situation, the company behind the AI is effectively doubling the capacity for evil by serving the goals of the bigot, even if it intends to do no harm. \u201cI picked the dystopian example to make the point,\u201d Schmidt told me\u2014that it\u2019s important for the right people to spend the time and energy and money to shape these tools early. \u201cThe reason we\u2019re marching toward this technological revolution is it is a material improvement in human intelligence. You\u2019re having something that you can communicate with; they can give you advice that\u2019s reasonably accurate. It\u2019s pretty powerful. It will lead to all sorts of problems.\u201d I asked Schmidt if he genuinely thought such a trade-off was worth it. \u201cMy answer,\u201d he said, \u201cis hell yeah.\u201d But I found his rationale unconvincing. \u201cIf you think about the biggest problems in the world, they are all really hard\u2014climate change, human organizations, and so forth. And so, I always want people to be smarter. The reason I picked a dystopian example is because we didn\u2019t understand such things when we built up social media 15 years ago. We didn\u2019t know what would happen with election interference and crazy people. We didn\u2019t understand it and I don\u2019t want us to make the same mistakes again.\u201d Having spent the past decade reporting on the platforms, architecture, and societal repercussions of social media, I can\u2019t help but feel that the systems, though human and deeply complex, are of a different technological magnitude than the scale and complexity of large language models and generative-AI tools. The problems\u2014which their founders didn\u2019t anticipate\u2014weren\u2019t wild, unimaginable, novel problems of humanity. They were reasonably predictable problems of connecting the world and democratizing speech at scale for profit at lightning speed. They were the product of a small handful of people obsessed with what was technologically possible and with dreams of rewiring society. Trying to find the perfect analogy to contextualize what a true, lasting AI revolution might look like without falling victim to the most overzealous marketers or doomers is futile. In my conversations, the comparisons ranged from the agricultural revolution to the industrial revolution to the advent of the internet or social media. But one comparison never came up, and I can\u2019t stop thinking about it: nuclear fission and the development of nuclear weapons. As dramatic as this sounds, I don\u2019t lie awake thinking of Skynet murdering me\u2014I don\u2019t even feel like I understand what advancements would need to happen with the technology for killer AGI to become a genuine concern. Nor do I think large language models are going to kill us all. The nuclear comparison isn\u2019t about any version of the technology we have now\u2014it is related to the bluster and hand-wringing from true believers and organizations about what technologists might be building toward. I lack the technical understanding to know what later iterations of this technology could be capable of, and I don\u2019t wish to buy into hype or sell somebody\u2019s lucrative, speculative vision. I am also stuck on the notion, voiced by some of these visionaries, that AI\u2019s future development might potentially be an extinction-level threat. ChatGPT doesn\u2019t really resemble the Manhattan Project, obviously. But I wonder if the existential feeling that seeps into most of my AI conversations parallels the feelings inside Los Alamos in the 1940s. I\u2019m sure there were questions then. If we don\u2019t build it, won\u2019t someone else? Will this make us safer? Should we take on monumental risk simply because we can? Like everything about our AI moment, what I find calming is also what I find disquieting. At least those people knew what they were building.",
    "gpt_response": "Original Question:\n- Anthropomorphization of Generative AI and its effect on Human Psychology and Behavior. Risks and Opportunities for Humanity.\n\nNew Research Questions:\n1. How does the anthropomorphization of generative AI influence human empathy towards non-human entities and its implications for societal norms?\n2. What are the psychological impacts of long-term interaction with anthropomorphized generative AI on human identity and self-perception?\n3. How can the design and implementation of anthropomorphized generative AI be optimized to enhance human well-being and mitigate potential risks?\n\nInteresting Concepts and Ideas:\n\n1. Anthropomorphism as a Tool for Understanding:\n   - Anthropomorphism is described as a form of metaphorical thinking that enables comparisons between ourselves and the world around us, facilitating human social interactions through empathy and deception (\"If you think your dog loves you, you\u2019re a fool\").\n   \n2. The Role of Theory of Mind in Anthropomorphism:\n   - Theory of mind is crucial for distinguishing one's mind from others and inferring what others are thinking or feeling, driving humans to perceive personhood in other entities (\"If you think your dog loves you, you\u2019re a fool\").\n   \n3. The Consequences of Anthropomorphizing AI:\n   - Projecting humanity onto AI could obscure the understanding of how these systems function and reinforce the notion of human intelligence as superior. However, anthropomorphism can also be used to better understand a complicated world (\"If you think your dog loves you, you\u2019re a fool\").\n   \n4. Generative AI's Capabilities and Public Perception:\n   - Generative AI models like ChatGPT have demonstrated abilities that challenge human perceptions of creativity and intelligence, leading to concerns about plagiarism, copyright issues, and the potential for AI to replace human jobs in creative fields (\"If you think your dog loves you, you\u2019re a fool\").\n   \n5. The Dangers of Overconfidence in AI Expertise:\n   - There is a danger in AI experts overestimating their understanding of AI's capabilities and societal impacts, leading to a lack of preparedness for potential negative consequences of AI technology (\"If you think your dog loves you, you\u2019re a fool\").\n   \n6. The Importance of Self-Reflection in AI Development:\n   - As AI technology progresses, there is a need for deeper self-reflection and consideration of human characteristics that AI might amplify, for better or worse (\"If you think your dog loves you, you\u2019re a fool\").\n   \n7. The Potential for AI to Amplify Human Flaws:\n   - AI has the potential to amplify human characteristics such as arrogance, superficiality, competitiveness, and greed, highlighting the need for careful consideration of the values and biases embedded in AI systems (\"If you think your dog loves you, you\u2019re a fool\").\n   \n8. The Need for External Regulation and Ethical Considerations:\n   - The rapid development and deployment of AI technologies underscore the need for external regulation, ethical considerations, and a focus on AI safety to mitigate risks and ensure that AI benefits humanity as a whole (\"If you think your dog loves you, you\u2019re a fool\")."
}
