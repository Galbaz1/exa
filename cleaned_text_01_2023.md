
## Timeframe: 01/2023
### URL: https://techcrunch.com/2023/01/05/whoops-is-generative-ai-already-becoming-a-bubble/
### Relevance Score: 0.16808533668518066
### Highlights: ['One of our questions sought their prediction of where the next bubble would be, and almost half of those surveyed mentioned generative AI. While several investors said they were bullish on the new tech overall, they also admitted that the sector was likely to get lost in its own hype. Don Butler, managing director at Thomvest Ventures, feels the bubble is already here.', 'Don Butler, managing director at Thomvest Ventures, feels the bubble is already here. “We believe that the applicability of AI to so many use cases will lead to a very large number of startups being funded in the space, including at some eye-watering valuations, and so we think that the next big bubble is already expanding in this area,” he said. This isn’t surprising, given how the sector has burgeoned recently as consumer tools like ChatGPT, DALL-E and Lensa AI soared in popularity both in and outside the tech community.\t ']

Venture capitalists are in the business of predicting the next big thing, even if they get burned in the process. While everyone piled onto crypto in 2021 — and many remain bullish about its future despite multiple failures this year — 2022 saw the rise of generative AI. But as is the case with any transformative new tech, hype is sure to accompany growing adoption, and generative AI has garnered so much attention and money that many VCs already feel the budding sector will be the next bubble. TechCrunch recently surveyed more than 35 investors working in different geographies, investment stages and sectors about how they were feeling about next year. One of our questions sought their prediction of where the next bubble would be, and almost half of those surveyed mentioned generative AI. While several investors said they were bullish on the new tech overall, they also admitted that the sector was likely to get lost in its own hype. Don Butler, managing director at Thomvest Ventures, feels the bubble is already here. “We believe that the applicability of AI to so many use cases will lead to a very large number of startups being funded in the space, including at some eye-watering valuations, and so we think that the next big bubble is already expanding in this area,” he said. This isn’t surprising, given how the sector has burgeoned recently as consumer tools like ChatGPT, DALL-E and Lensa AI soared in popularity both in and outside the tech community.

---

## Timeframe: 01/2023
### URL: https://techcrunch.com/2023/01/27/the-current-legal-cases-against-generative-ai-are-just-the-beginning/?utm_source=substack&utm_medium=email
### Relevance Score: 0.15554985404014587
### Highlights: ['“a sketch of a bird perched on a windowsill”) as they work their way through massive training datasets. The models are trained to “re-create” images as opposed to drawing them from scratch, starting with pure noise and refining the image over time to make it incrementally closer to the text prompt. Perfect recreations don’t occur often, to Torres’ point.', 'The models are trained to “re-create” images as opposed to drawing them from scratch, starting with pure noise and refining the image over time to make it incrementally closer to the text prompt. Perfect recreations don’t occur often, to Torres’ point. As for images in the style of a particular artist, style has proven nearly impossible to shield with copyright.']

As generative AI enters the mainstream, each new day brings a new lawsuit. Microsoft, GitHub and OpenAI are currently being sued in a class action motion that accuses them of violating copyright law by allowing Copilot, a code-generating AI system trained on billions of lines of public code, to regurgitate licensed code snippets without providing credit. Two companies behind popular AI art tools, Midjourney and Stability AI, are in the crosshairs of a legal case that alleges they infringed on the rights of millions of artists by training their tools on web-scraped images. And just last week, stock image supplier Getty Images took Stability AI to court for reportedly using millions of images from its site without permission to train Stable Diffusion, an art-generating AI. At issue, mainly, is generative AI’s tendency to replicate images, text and more — including copyrighted content — from the data that was used to train it. In a recent example, an AI tool used by CNET to write explanatory articles was found to have plagiarized articles written by humans — articles presumably swept up in its training dataset. Meanwhile, an academic study published in December found that image-generating AI models like DALL-E 2 and Stable Diffusion can and do replicate aspects of images from their training data. The generative AI space remains healthy — it raised $1.3 billion in venture funding through November 2022, according to PitchBook, up 15% from the year prior. But the legal questions are beginning to affect business. Some image-hosting platforms have banned AI-generated content for fear of legal blowback. And several legal experts have cautioned generative AI tools could put companies at risk if they were to unwittingly incorporate copyrighted content generated by the tools into any of products they sell. “Unfortunately, I expect a flood of litigation for almost all generative AI products,” Heather Meeker, a legal expert on open source software licensing and a general partner at OSS Capital, told TechCrunch via email. “The copyright law needs to be clarified.” Content creators such as Polish artist Greg Rutkowski, known for creating fantasy landscapes, have become the face of campaigns protesting the treatment of artists by generative AI startups. Rutkowski has complained about the fact that typing text like “Wizard with sword and a glowing orb of magic fire fights a fierce dragon Greg Rutkowski” will create an image that looks very similar to his original work — threatening his income. Given generative AI isn’t going anywhere, what comes next? Which legal cases have merit and what court battles lie on the horizon? Eliana Torres, an intellectual property attorney with Nixon Peabody, says that the allegations of the class action suit against Stability AI, Midjourney, and DeviantArt will be challenging to prove in court. In particular, she thinks it’ll be difficult to ascertain which images were used to train the AI systems because the art the systems generate won’t necessarily look exactly like any of the training images. State-of-the-art image-generating systems like Stable Diffusion are what’s known as “diffusion” models. Diffusion models learn to create images from text prompts (e.g. “a sketch of a bird perched on a windowsill”) as they work their way through massive training datasets. The models are trained to “re-create” images as opposed to drawing them from scratch, starting with pure noise and refining the image over time to make it incrementally closer to the text prompt. Perfect recreations don’t occur often, to Torres’ point. As for images in the style of a particular artist, style has proven nearly impossible to shield with copyright. “It will … be challenging to get a general acceptance of the definition of ‘in style of’ as ‘a work that others would accept as a work created by that artist whose style was called upon,’ which is mentioned in the complaint [i.e. against Stability AI et al.],” Torres told TechCrunch in an email interview. Torres also believes the suit should be directed not at the creators of these AI systems, but at the party responsible for compiling the images used to train them: Large-scale Artificial Intelligence Open Network (LAION), a nonprofit organization. Midjourney, DeviantArt and Stability AI use training data from LAION’s datasets, which span billions of images from around the web. “If LAION created the dataset, then the alleged infringement occurred at that point, not once the dataset was used to train the models,” Torres said. “It’s the same way a human can walk into a gallery and look at paintings but is not allowed to take photos.” Companies like Stability AI and OpenAI, the company behind ChatGPT, have long claimed that “fair use” protects them in the event that their systems were trained on licensed content. This doctrine enshrined in U.S. law permits limited use of copyrighted material without first having to obtain permission from the rightsholder. Supporters point to cases like Authors Guild v. Google, in which the New York-based U.S. Court of Appeals for the Second Circuit ruled that Google manually scanning millions of copyrighted books without a license to create its book search project was fair use. What constitutes fair use is constantly being challenged and revised, but in the generative AI realm, it’s an especially untested theory. A recent article in Bloomberg Law asserts that the success of a fair use defense will depend on whether the works generated by the AI are considered transformative — in other words, whether they use the copyrighted works in a way that significantly varies from the originals. Previous case law, particularly the Supreme Court’s 2021 Google v. Oracle decision, suggests that using collected data to create new works can be transformative. In that case, Google’s use of portions of Java SE code to create its Android operating system was found to be fair use. Interestingly, other countries have signaled a move toward more permissive use of publicly available content — copyrighted or not. For example, the U.K. is planning to tweak an existing law to allow text and data mining “for any purpose,” moving the balance of power away from rightsholders and heavily toward businesses and other commercial entities. There’s been no appetite to embrace such a shift in the U.S., however, and Torres doesn’t expect that to change anytime soon — if ever. The Getty case is slightly more nuanced. Getty — which Torres notes hasn’t yet filed a formal complaint — must show damages and connect any infringement it alleges to specific images. But Getty’s statement mentions that it has no interest in financial damages and is merely looking for a “new legal status quo.” Andrew Burt, one of the founders of AI-focused law firm BNH.ai, disagrees with Torres to the extent that he believes generative AI lawsuits focused on intellectual property issues will be “relatively straightforward.” In his view, if copyrighted data was used to train AI systems — whether because of intellectual property or privacy restrictions — those systems should and will be subject to fines or other penalties. Burt noted that the Federal Trade Commission (FTC) is already pursuing this path with what it calls “algorithmic disgorgement,” where it forces tech firms to kill problematic algorithms along with any ill-gotten data that they used to train them. In a recent example, the FTC used the remedy of algorithmic disgorgement to force Everalbum, the maker of a now-defunct mobile app called Ever, to delete facial recognition algorithms the company developed using content uploaded by people who used its app. (Everalbum didn’t make it clear that the users’ data was being used for this purpose.) “I would expect generative AI systems to be no different from traditional AI systems in this way,” Burt said. What are companies to do, then, in the absence of precedent and guidance? Torres and Burt concur that there’s no obvious answer. For her part, Torres recommends looking closely at the terms of use for each commercial generative AI system. She notes that Midjourney has different rights for paid versus unpaid users, while OpenAI’s DALL-E assigns rights around generated art to users while also warning them of “similar content” and encouraging due diligence to avoid infringement. “Businesses should be aware of the terms of use and do their due diligence, such as using reverse image searches of the generated work intended to be used commercially,” she added. Burt recommends that companies adopt risk management frameworks such as the AI Risk Management Framework released by National Institute of Standards and Technology, which gives guidance on how to address and mitigate risks in the design and use of AI systems. He also suggests that companies continuously test and monitor their systems for potential legal liabilities. “While generative AI systems make AI risk management harder — it is, to be fair, much more straightforward to monitor an AI system that makes binary predictions for risks — there are concrete actions that can be taken,” Burt said. Some firms, under pressure from activists and content creators, have taken steps in the right direction. Stability AI plans to allow artists to opt out of the dataset used to train the next-generation Stable Diffusion model. Through the website HaveIBeenTrained.com, rightsholders will be able to request opt-outs before training begins in a few weeks’ time. Rival OpenAI offers no such opt-out mechanism, but the firm has partnered with organizations like Shutterstock to license portions of their image galleries. For Copilot, GitHub introduced a filter that checks code suggestions with their surrounding code of about 150 characters against public GitHub code and hides suggestions if there’s a match or “near match.” It’s an imperfect measure — enabling the filter can cause Copilot to omit key pieces of attribution and license text — but GitHub has said that it plans to introduce additional features in 2023 aimed at helping developers make informed decisions about whether to use Copilot’s suggestions. Taking the ten-thousand-foot view, Burt believes that generative AI is being deployed more and more without an understanding of how to address its dangers. He praises efforts to combat the obvious problems, like copyrighted works being used to train content generators. But he cautions that the opacity of the systems will put pressure on businesses to prevent the systems from wreaking havoc — and having a plan to address the systems’ risks before they’re put into the wild. “Generative AI models are among the most exciting and novel uses of AI — with the clear potential to transform the ‘knowledge economy’,” he said. “Just as with AI in many other areas, the technology is largely there and ready for use. What isn’t yet mature are the ways to manage all of its risks. Without thoughtful, mature evaluation and management of these systems’ harms, we risk deploying a technology before we understand how to stop it from causing damage.” Meeker is more pessimistic, arguing that not all businesses — regardless of the mitigations they undertake — will be able to shoulder the legal costs associated with generative AI. This points to the urgent need for clarification or changes in copyright law, she says. “If AI developers don’t know what data they can use to train models, the technology could be set back by years,” Meeker said. “In a sense, there is nothing they can do, because if businesses are unable to lawfully train models on freely available materials, they won’t have enough data to train the models. There are only various long-term solutions like opt-in or opt-out models, or systems that aggregate royalties for payment to all authors … The suits against AI businesses for ingesting copyrightable material to train models are potentially crippling to the industry, [and] could cause consolidation that would limit innovation.”

---

## Timeframe: 01/2023
### URL: https://www.nytimes.com/2023/01/07/technology/generative-ai-chatgpt-investments.html
### Relevance Score: 0.1640152931213379
### Highlights: ['Taxes may apply. Offer terms are subject to change.   Subscribe to The Times to read (and print) as many articles as you’d like.', 'Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change.']

The New York Times: Digital and Home Delivery Subscriptions Offer for a New York Times News subscription; current subscribers not eligible. Subscription excludes print edition. Subscription also excludes digital access to New York Times Games, Cooking, Wirecutter or The Athletic. Your payment method will automatically be charged in advance the introductory rate of $4 every 4 weeks for 1 year, and after 1 year the standard rate of $17 every 4 weeks. Your subscription will continue until you cancel. Cancellation takes effect at the end of your current billing period. Taxes may apply. Offer terms are subject to change. plus-icon check Subscribe to The Times to read (and print) as many articles as you’d like. nytimes.com/subscription

---

## Timeframe: 01/2023
### URL: https://www.nytimes.com/2023/01/27/technology/anthropic-ai-funding.html
### Relevance Score: 0.15922337770462036
### Highlights: ['The start-up has held discussions about a large round of funding, according to three people with knowledge of the situation. Replika, another chatbot company, and You.com, which is rolling out similar technology into a new kind of search engine, said they, too, had received unsolicited interest from investors. All specialize in generative A.I.', 'Replika, another chatbot company, and You.com, which is rolling out similar technology into a new kind of search engine, said they, too, had received unsolicited interest from investors. All specialize in generative A.I. The result of more than a decade of research inside companies like OpenAI, these technologies are poised to remake everything from online search engines like Google Search and Microsoft Bing to photo and graphics editors like Photoshop.']

Anthropic, an A.I. Start-Up, Is Said to Be Close to Adding $300 Million Anthropic specializes in generative artificial intelligence, a hot investment in Silicon Valley. The new funding could value the company at roughly $5 billion. Credit...Carol Yepes/Getty Images Erin Griffith and Cade Metz Erin Griffith reports on start-ups and venture capital and Cade Metz on artificial intelligence and other emerging technologies. Anthropic, a San Francisco artificial intelligence start-up, is close to raising roughly $300 million in new funding, two people with knowledge of the situation said, in the latest sign of feverish excitement for a new class of A.I. start-ups. The deal could value Anthropic at roughly $5 billion, though the terms were still being worked out and the valuation could change, one of the people said. The start-up, which was founded in 2021, previously raised $704 million, valuing it at $4 billion, according to PitchBook, which tracks private investment data. Silicon Valley has been gripped by a frenzy over start-ups working on “generative” A.I., technologies that can generate text, images and other media in response to short prompts. This week, Microsoft invested $10 billion in OpenAI, the San Francisco start-up that kicked off the furor in November with a chatbot, ChatGPT. ChatGPT has wowed more than a million people with its knack for answering questions in clear, concise prose. Even as funding for other start-ups has dried up, investors have chased deals in similar A.I. companies, signaling that the otherwise gloomy market for tech investing has at least one bright spot. Other funding deals in the works include Character.AI, which lets people talk to chatbots that impersonate celebrities. The start-up has held discussions about a large round of funding, according to three people with knowledge of the situation. Replika, another chatbot company, and You.com, which is rolling out similar technology into a new kind of search engine, said they, too, had received unsolicited interest from investors. All specialize in generative A.I. The result of more than a decade of research inside companies like OpenAI, these technologies are poised to remake everything from online search engines like Google Search and Microsoft Bing to photo and graphics editors like Photoshop. The explosion of interest in generative A.I. has investors and start-ups racing to choose their teams. Start-ups want to take money from the most powerful investors with the deepest pockets, and investors are trying to pick winners from a growing list of ambitious companies. The stakes are high. Venture capital investors typically do not back multiple companies in one category for competitive reasons. So a bad bet now could lead to a missed opportunity to make money on other deals down the line. Despite the excitement, few of these start-ups have a clear plan to make money. That has rarely been a problem in Silicon Valley; past generations of investors poured money into social media sites or mobile apps on the assumption that they would figure out how to turn a profit later. But that strategy has been less of a sure bet in recent years as start-ups have expanded beyond the tech industry’s bread and butter of selling software or selling ads. Certain businesses, like on-demand delivery, ride-hailing apps and subscription meal kits, took longer to make money than investors hoped or did not make money at all. Anthropic was founded by a group of people that included several researchers who left OpenAI. Its funding talks stand out because of its earlier backers. The vast majority of its funding came from the disgraced cryptocurrency entrepreneur Sam Bankman-Fried and his colleagues at FTX, the cryptocurrency platform that went bankrupt amid fraud charges last year. That money could be clawed back by a bankruptcy court, leaving Anthropic in limbo.

---

## Timeframe: 01/2023
### URL: https://www.nytimes.com/2023/01/23/business/microsoft-chatgpt-artificial-intelligence.html
### Relevance Score: 0.1497313529253006
### Highlights: ['system, GPT-3, which could generate text on its own, including tweets, blog posts, news articles and even computer code. Last year, it unveiled DALL-E, which lets anyone generate photorealistic images simply by describing what he or she wants to see. Based on the same technology as GPT-3, ChatGPT showed the general public just how powerful this kind of technology could be.', 'More than a million people tested the chatbot during its first few days online, using it to answer trivia questions, explain ideas and generate everything from poetry to term papers. Microsoft has already incorporated GPT-3, DALL-E and other OpenAI technologies into its products. Most notably, GitHub, a popular online service for programmers owned by Microsoft, offers Copilot, a tool that can automatically generate snippets of computer code.']

The tech giant aims to remain at the forefront of generative artificial intelligence with its partnership with OpenAI. Microsoft has extended its partnership with OpenAI, the artificial-intelligence lab behind the chatbot ChatGPT.Credit...Jovelle Tamayo for The New York Times Cade Metz and Karen Weise Cade Metz and Karen Weise reported this article from San Francisco and Seattle. Microsoft said on Monday that it was making a “multiyear, multibillion-dollar” investment in OpenAI, the San Francisco artificial intelligence lab behind the experimental online chatbot ChatGPT. The companies did not disclose the specific financial terms of the deal, but a person familiar with the matter said Microsoft would invest $10 billion in OpenAI. Microsoft had already invested more than $3 billion in OpenAI, and the new deal is a clear indication of the importance of OpenAI’s technology to the future of Microsoft and its competition with other big tech companies like Google, Meta and Apple. With Microsoft’s deep pockets and OpenAI’s cutting-edge artificial intelligence, the companies hope to remain at the forefront of generative artificial intelligence — technologies that can generate text, images and other media in response to short prompts. After its surprise release at the end of November, ChatGPT — a chatbot that answers questions in clear, well-punctuated prose — became the symbol of a new and more powerful wave of A.I. The fruit of more than a decade of research inside companies like OpenAI, Google and Meta, these technologies are poised to remake everything from online search engines like Google Search and Microsoft Bing to photo and graphics editors like Photoshop. The deal follows Microsoft’s announcement last week that it had begun laying off employees as part of an effort to cull 10,000 positions. The changes, including severance, ending leases and what it called “changes to our hardware portfolio” would cost $1.2 billion, it said. Satya Nadella, the company’s chief executive, said last week that the cuts would let the company refocus on priorities such as artificial intelligence, which he called “the next major wave of computing.” Mr. Nadella made clear in his company’s announcement on Monday that the next phase of the partnership with OpenAI would focus on bringing tools to the market, saying that “developers and organizations across industries will have access to the best A.I. infrastructure, models and tool chain.” OpenAI was created in 2015 by small group of entrepreneurs and artificial intelligence researchers, including Sam Altman, head of the start-up builder Y Combinator; Elon Musk, the billionaire chief executive of the electric carmaker Tesla; and Ilya Sutskever, one of the most important researchers of the past decade. They founded the lab as a nonprofit organization. But after Mr. Musk left the venture in 2018, Mr. Altman remade OpenAI as a for-profit company so it could raise the money needed for its research. Image Satya Nadella, chief executive of Microsoft, said the next phase of the partnership with OpenAI would focus on bringing tools to the market.Credit...Yonhap/EPA, via Shutterstock A year later, Microsoft invested a billion dollars in the company; over the next few years, it quietly invested another $2 billion. These funds paid for the enormous amounts of computing power needed to build the kind of generative A.I. technologies OpenAI is known for. OpenAI is also in talks to complete a deal in which it would sell existing shares in a so-called tender offer. This could total $300 million, depending on how many employees agree to sell their stock, according to two people with knowledge of the discussions, and would value the company at around $29 billion. In 2020, OpenAI built a milestone A.I. system, GPT-3, which could generate text on its own, including tweets, blog posts, news articles and even computer code. Last year, it unveiled DALL-E, which lets anyone generate photorealistic images simply by describing what he or she wants to see. Based on the same technology as GPT-3, ChatGPT showed the general public just how powerful this kind of technology could be. More than a million people tested the chatbot during its first few days online, using it to answer trivia questions, explain ideas and generate everything from poetry to term papers. Microsoft has already incorporated GPT-3, DALL-E and other OpenAI technologies into its products. Most notably, GitHub, a popular online service for programmers owned by Microsoft, offers Copilot, a tool that can automatically generate snippets of computer code. Last week, it expanded availability of several OpenAI services to customers of Microsoft’s Azure cloud computing offering, and said ChatGPT would be “coming soon.” The company said it planned to report its latest quarterly results on Tuesday, and investors expect the difficult economy, including declining personal computer sales and more cautious business spending, to further hit revenues. Microsoft has faced slowing growth since late summer, and Wall Street analysts expect the new financial results to show its slowest growth since 2016. But the business still produces substantial profits and cash. It has continued to return money to investors through quarterly dividends and a $60 billion share buyback program authorized by its board in 2021. Both Microsoft and OpenAI say their goals are even higher than a better chatbot or programming assistant. OpenAI’s stated mission was to build artificial general intelligence, or A.G.I., a machine that can do anything the human brain can do. When OpenAI announced its initial deal with Microsoft in 2019, Mr. Nadella described it as the kind of lofty goal that a company like Microsoft should pursue, comparing A.G.I. to the company’s efforts to build a quantum computer, a machine that would be exponentially faster than today’s machines. “Whether it’s our pursuit of quantum computing or it’s a pursuit of A.G.I., I think you need these high-ambition North Stars,” he said. That is not something that researchers necessarily know how to build. But many believe that systems like ChatGPT are a path to this lofty goal. In the near term, these technologies are a way for Microsoft to expand its business, bolster revenue and compete with the likes of Google and Meta, which are also addressing A.I. advancements with a sense of urgency. Sundar Pichai, the chief executive of Google’s parent company, Alphabet, recently declared a “code red,” upending plans and jump-starting A.I. development. Google intends to unveil more than 20 products and demonstrate a version of its search engine with chatbot features this year, according to a slide presentation reviewed by The New York Times and two people with knowledge of the plans, who were not authorized to discuss them. But the new A.I. technologies come with a long list of flaws. They often produce toxic content, including misinformation, hate speech and images that are biased against women and people of color. Microsoft, Google, Meta and other companies have been reluctant to release many of these technologies because they could damage their established brands. Five years ago, Microsoft released a chatbot called Tay, which generated racist and xenophobic language, and quickly removed it from the internet after complaints from users. Nico Grant contributed reporting.

---

## Timeframe: 01/2023
### URL: https://www.theverge.com/2023/1/28/23574573/google-musiclm-text-to-music-ai
### Relevance Score: 0.1568908840417862
### Highlights: ['That, by the way, is the result of asking it to make music that would play at a gym. You may also have noticed that the lyrics are nonsense, but in a way that you may not necessarily catch if you’re not paying attention — kind of like if you were listening to someone singing in Simlish or that one song that’s meant to sound like English but isn’t. I won’t pretend to know how Google achieved these results, but it’s released a research paper explaining it in detail if you’re the type of person who would understand this figure:            A figure explaining the “hierarchical sequence- to-sequence modeling task” that the researchers use along with   AudioLM, another Google project  .', 'The site lets you play the input audio, where someone hums or whistles a tune, then lets you hear how the model reproduces it as an electronic synth lead, string quartet, guitar solo, etc. From the examples I listened to, it manages the task very well. Like with other forays into this type of AI, Google is being significantly more cautious with MusicLM than some of its peers may be with similar tech.']

Google researchers have made an AI that can generate minutes-long musical pieces from text prompts, and can even transform a whistled or hummed melody into other instruments, similar to how systems like DALL-E generate images from written prompts (via TechCrunch ). The model is called MusicLM, and while you can’t play around with it for yourself, the company has uploaded a bunch of samples that it produced using the model. The examples are impressive. There are 30-second snippets of what sound like actual songs created from paragraph-long descriptions that prescribe a genre, vibe, and even specific instruments, as well as five-minute-long pieces generated from one or two words like “melodic techno.” Perhaps my favorite is a demo of “story mode,” where the model is basically given a script to morph between prompts. For example, this prompt: electronic song played in a videogame (0:00-0:15) meditation song played next to a river (0:15-0:30) fire (0:30-0:45) fireworks (0:45-0:60) It may not be for everyone, but I could totally see this being composed by a human (I also listened to it on loop dozens of times while writing this article). Also featured on the demo site are examples of what the model produces when asked to generate 10-second clips of instruments like the cello or maracas (the later example is one where the system does a relatively poor job), eight-second clips of a certain genre, music that would fit a prison escape, and even what a beginner piano player would sound like versus an advanced one. It also includes interpretations of phrases like “futuristic club” and “accordion death metal.” MusicLM can even simulate human vocals, and while it seems to get the tone and overall sound of voices right, there’s a quality to them that’s definitely off. The best way I can describe it is that they sound grainy or staticky. That quality isn’t as clear in the example above, but I think this one illustrates it pretty well. That, by the way, is the result of asking it to make music that would play at a gym. You may also have noticed that the lyrics are nonsense, but in a way that you may not necessarily catch if you’re not paying attention — kind of like if you were listening to someone singing in Simlish or that one song that’s meant to sound like English but isn’t. I won’t pretend to know how Google achieved these results, but it’s released a research paper explaining it in detail if you’re the type of person who would understand this figure: A figure explaining the “hierarchical sequence- to-sequence modeling task” that the researchers use along with AudioLM, another Google project . Chart: Google AI-generated music has a long history dating back decades; there are systems that have been credited with composing pop songs, copying Bach better than a human could in the 90s, and accompanying live performances. One recent version uses AI image generation engine StableDiffusion to turn text prompts into spectrograms that are then turned into music. The paper says that MusicLM can outperform other systems in terms of its “quality and adherence to the caption,” as well as the fact that it can take in audio and copy the melody. That last part is perhaps one of the coolest demos the researchers put out. The site lets you play the input audio, where someone hums or whistles a tune, then lets you hear how the model reproduces it as an electronic synth lead, string quartet, guitar solo, etc. From the examples I listened to, it manages the task very well. Like with other forays into this type of AI, Google is being significantly more cautious with MusicLM than some of its peers may be with similar tech. “We have no plans to release models at this point,” concludes the paper, citing risks of “potential misappropriation of creative content” (read: plagiarism) and potential cultural appropriation or misrepresentation. It’s always possible the tech could show up in one of Google’s fun musical experiments at some point, but for now, the only people who will be able to make use of the research are other people building musical AI systems. Google says it’s publicly releasing a dataset with around 5,500 music-text pairs, which could help when training and evaluating other musical AIs.

---

## Timeframe: 01/2023
### URL: https://www.theverge.com/2023/1/26/23572834/buzzfeed-using-ai-tools-personalize-generate-content-openai
### Relevance Score: 0.1443822979927063
### Highlights: ['The problem is that the more AI is used in a story’s creation, the more chance there is for mistakes to be introduced. This is especially true of tools like ChatGPT, which have a tendency to produce “fluent bullshit” — that is, plausible but incorrect information. Recently, CNET was criticized for failing to clearly disclose its use of AI in writing a number of articles offering financial advice in its money section.', 'Automated tools are used to input figures and results into prewritten templates. As AI systems have improved, though, so has their ability to write more of a story. The problem is that the more AI is used in a story’s creation, the more chance there is for mistakes to be introduced.']

BuzzFeed says it’s going to use AI tools provided by ChatGPT creator OpenAI to “enhance” and “personalize” its content, according to a memo sent this morning to staff by CEO Jonah Peretti and statements given to The Verge. In the memo, Peretti says AI will be one of the two major trends defining the future of digital media (the other being “creators”). Peretti says that in 2023, BuzzFeed’s “AI inspired content” will launch on the site, “enhancing the quiz experience, informing our brainstorming, and personalizing our content for our audience.” News of the memo was first reported by The Wall Street Journal . “Our industry will expand beyond AI-powered curation (feeds), to AI-powered creation (content),” says Peretti. “AI opens up a new era of creativity, where creative humans like us play a key role providing the ideas, cultural currency, inspired prompts, IP, and formats that come to life using the newest technologies.” In an example cited by the WSJ but not included in the memo, AI could be used to generate personalized rom-com pitches for readers. They would be asked a series of questions, including personal information (like “name an endearing flaw” and “pick a favorite rom-com trope”), which would be used to generate a shareable output. Peretti notes in the memo that he’ll be sharing “a preview of the content we’ll be launching in February” at an all hands meeting later today. (You can read the memo in full below.) When asked by The Verge if BuzzFeed was considering using AI in its newsroom, the company’s VP of communications, Matt Mittenthal, replied, “No.” Mitthenthal also said: “I can confirm that we’ll be using OpenAI technology.” The increasing capability of AI tools like ChatGPT to write prose has made the technology a tempting prospect for media companies hurt by falling advertising rates. BuzzFeed, in particular, has had a rocky ride on the markets since going public in December 2021. By June 2022, the company’s share price had already fallen 40 percent and has continued trending downwards since. Following news this morning that BuzzFeed will be using AI to produce content, its share price had risen more than 100 percent at the time of writing. Although publishers like The Associated Press have used automated tools to produce stories for nearly a decade, these have tended to be formulaic articles covering news like earnings reports and sporting results. Automated tools are used to input figures and results into prewritten templates. As AI systems have improved, though, so has their ability to write more of a story. The problem is that the more AI is used in a story’s creation, the more chance there is for mistakes to be introduced. This is especially true of tools like ChatGPT, which have a tendency to produce “fluent bullshit” — that is, plausible but incorrect information. Recently, CNET was criticized for failing to clearly disclose its use of AI in writing a number of articles offering financial advice in its money section. The company, which is owned by private equity firm Red Ventures, has since paused its use of AI tools and says it found errors in more than half of the articles written with the technology’s help. Hi all, I want to share thoughts today on our strategy. As we continue to navigate through challenging times, our mission is more important than ever – to spread truth, joy, and creativity to the biggest, most diverse, most online audiences. We are fortunate to have a talented team, iconic brands, and a differentiated technology platform that allows us to stand out in a crowded market and have an unparalleled impact on culture. And, of course, the best of the Internet: lists, quizzes, and video from BuzzFeed; premium, award-winning series from Complex Networks; two of the largest food brands on the Internet in Tasty and First We Feast; and free, award-winning journalism from HuffPost and BuzzFeed News. In the short term, we will fiercely focus on delivering strong value for our partners so that they continue to spend with us during the recession, and manage our costs and run our company like a lean, scrappy startup. In tough economic times, we need to fight for every penny of revenue, and try to save every penny of costs. But the truth is that working hard for our partners and operating cost efficiently, while essential, will only get us so far. To fulfill the promise of our mission, we need to build a stronger business foundation by executing a forward-looking strategy. We must look ahead and shift our business towards longer term trends in order to seize the opportunities that will come in the eventual recovery. Over the next three years, the future of digital media will be defined by two major trends: creators, and AI. We will help shape these trends to create massive value for our audience, our employees, and our shareholders. Working together, we will: Build the premier platform for creative people to do their most inspired work, reach the most people, and live their best lives. Content will increasingly be produced by talent around the world who work for themselves and multiple companies. Broadly defined, I’ll refer to these people as “creators,” but they include people who you might also refer to as celebrities, actors, athletes, entertainers, freelancers, influencers, contributors, producers, community members, or talent. The best media companies will provide a differentiated creator platform to help creators succeed in a crowded market. The creator platforms of the future will bring together widely known and loved brands, distribution channels, creative partnerships, business opportunities, and purpose-built creator tools. Lead the future of AI-powered content and maximize the creativity of our writers, producers, and creators and our business. The creative process will increasingly become AI-assisted and technology-enabled. If the past 15 years of the internet have been defined by algorithmic feeds that curate and recommend content, the next 15 years will be defined by AI and data helping create, personalize, and animate the content itself. Our industry will expand beyond AI-powered curation (feeds), to AI-powered creation (content). AI opens up a new era of creativity, where creative humans like us play a key role providing the ideas, cultural currency, inspired prompts, IP, and formats that come to life using the newest technologies. Here is how we are going to do it: Our teams at BuzzFeed, Complex Networks, Tasty, HuffPost and BuzzFeed News will serve as the braintrust, innovating new models, creating new formats, and developing new production models to help creative people reach more people and have a bigger impact. Our business team will serve as the monetization experts, helping creators make the most money from their work, bringing new tech-powered products to market, and tapping into studio, commerce, and advertising opportunities. And our tech, product, and select content teams will become the experts on the latest technologies, the best AI-models, and the most interactive formats in order to build industry-leading tools and bring creator ideas to life. We already have significant scale and growth opportunities from creator initiatives across the company; and with a new mindset and added focus, we are set up to win. Across BuzzFeed, Tasty, Complex Networks and more, we’re connecting brands with the best emerging talent and publishers across the Internet. That includes a wide variety of amazing talent in our Catalyst Creator’s Program: foodies backed by iconic brands like Tasty; identity-focused creators from brands like Cocoa Butter and Pero Like and beloved creators in the shopping space, who drive viral recommendations and brand purchases. BuzzFeed, Inc.’s Catalyst creator network overall encompasses many more creators, with more than 280+ million total followers. Over 20% of the BuzzFeed.com web audience is consuming creator made content. On Instagram, Tasty’s Creator Residents and Creator collaborations generated more than one billion views in 2022. HuffPost Voices and HuffPost Personal will attract 250 new contributors this year. Last year for the first time, BuzzFeed News and HuffPost launched a 10-month residency for ten external creators who are focused on short-form video content that complements the incredible reporting from our newsrooms. The creators meet regularly with newsroom leaders to discuss ideas for content that riffs off, or is inspired by, our original reporting. Next month we’re beginning to work with the first class of Complex Creators with over 15 new faces joining our roster of talent. Our Creator, Webscouts, and Video Acquisition teams will partner with 600+ creators this year and are growing off of a base of over ten billion views in 2022. The highest end creators – celebrities – are both hosting and appearing on our shows regularly, with talent like Sean Evans, our host, interviewing Viola Davis and Lizzo on Hot Ones; Joe La Puma hosting Mark Wahlberg and Idris Elba on Sneaker Shopping; and stars like Tom Holland and Chris Evans (finally!) playing with puppies. In total, today our wide variety of creator-powered businesses generate tens of millions of dollars in revenue and an audience of well over 200 million people. Research firm Statista recently reported that the global media spend in the influencer advertising segment is projected to reach $51 billion by 2027, increasing at an annual growth rate of 13.21% from 2022 to 2027. Many creator-focused startups are struggling because they don’t have the brands and compelling offerings we are all to provide. We are well-positioned to take share in this lucrative and growing new market segment in the coming year. Our work in AI-powered creativity is also off to a good start, and in 2023, you’ll see AI inspired content move from an R&D stage to part of our core business, enhancing the quiz experience, informing our brainstorming, and personalizing our content for our audience. I’ll share more soon about the encouraging developments and new applications we are working on as AI models continue to advance. To be clear, we see the breakthroughs in AI opening up a new era of creativity that will allow humans to harness creativity in new ways with endless opportunities and applications for good. In publishing, AI can benefit both content creators and audiences, inspiring new ideas and inviting audience members to co-create personalized content. When you see this work in action it is pretty amazing. I’ll share a preview of the content we’ll be launching in February at the All Hands later today. That’s our plan. We’ll fight for revenue and manage costs through this recession. We’ll build the best platform for creators. We’ll lead the future of AI-powered content. Throughout it all, we’ll stay true to our mission. This is all possible because of our strong brands, huge audience, and talented teams. Together we can build the future of media. It will be a worthy challenge and an exciting adventure. It is an honor to work alongside you. Looking forward to fighting through a tough market and creating the future of media together. See you at the All Hands today at 1pm ET/10am PT, where we’ll talk more about all of this. Jonah Update Thursday January 26th, 13:21AM ET: The story has been updated with the memo from Peretti. Update Thursday January 26th, 13:54AM ET: Updated to include independent confirmation that BuzzFeed will be using AI tools from OpenAI and not using AI in its newsroom.

---

## Timeframe: 01/2023
### URL: https://www.wired.com/gallery/where-the-ai-art-boom-came-from-and-where-its-going/
### Relevance Score: 0.15282955765724182
### Highlights: ["They claim it is significantly more efficient than previous image generators, creating images in a third of the time Stable Diffusion needs, and with higher quality results. Google's new technique can also be used to edit images using text instructions—something that could prove useful to creative professionals. One thing holding back wider use of image generators is that they do not have a meaningful understanding of how text relates to elements in an image.", 'One thing holding back wider use of image generators is that they do not have a meaningful understanding of how text relates to elements in an image. In October, two students at MIT, Nan Liu and Shuang Li,\xa0demonstrated a way to ask an image generator to include or exclude specific elements in an image, and specify details like placing one object in front of another. That could help people get image generators to do what they ask more often, but Josh Tenenbaum, a professor at MIT involved in the project, says the fact remains that existing AI tools simply do not understand the world in the way humans do.']

The wide availability of image generators has caused not only an explosion of experimentation but also discussion around the implications of the technology. One knotty problem is that the images created can inherit biases from the data they are fed; another that they could be used to generate harmful content. The copyright and trademark implications of AI art are also unclear, and some artists worry that such tools may make work harder to find. Those debates will continue in 2023—and the technology looks likely to keep improving quickly. In December, researchers at Google announced an image-generation tool called Muse built around a new technique. They claim it is significantly more efficient than previous image generators, creating images in a third of the time Stable Diffusion needs, and with higher quality results. Google's new technique can also be used to edit images using text instructions—something that could prove useful to creative professionals. One thing holding back wider use of image generators is that they do not have a meaningful understanding of how text relates to elements in an image. In October, two students at MIT, Nan Liu and Shuang Li, demonstrated a way to ask an image generator to include or exclude specific elements in an image, and specify details like placing one object in front of another. That could help people get image generators to do what they ask more often, but Josh Tenenbaum, a professor at MIT involved in the project, says the fact remains that existing AI tools simply do not understand the world in the way humans do. “It's amazing what they can do, but their ability to imagine what the world might be like from simple descriptions is often very limited and counterintuitive,” he says. As excitement—and funding—for AI art tools grows, 2023 will probably bring higher quality AI-made images and perhaps the emergence of AI video generators. Researchers have demonstrated prototypes, although their output is so far relatively simple. Yet Stable Diffusion, Midjourney, Google, Meta, and Nvidia are all working on the technology. For a taste of what’s to come, WIRED asked Meta to generate a few videos of New Year’s celebrations. The results are crude, but if the recent history of AI imager generators is anything to go by, then they will improve fast. A whole new set of debates about AI's creative power and ethical and economic consequences may be about to begin.

---

## Timeframe: 01/2023
### URL: https://www.wired.com/story/generative-ai-video-game-development/
### Relevance Score: 0.1455288529396057
### Highlights: ['“And it works because this content doesn’t really need to function: It doesn’t have functionality constraints. Maybe you can replace them with deep-learning-based stuff. But I don’t think it’s going to make a big difference.', 'With computers doing the boring stuff, a small team could whip up a map the size of San Andreas. Crunch becomes a thing of the past; games release in a finished state. A new age beckons.']

Creating a video game demands hard, repetitive work. How could it not? Developers are in the business of building world, so it’s easy to understand why the games industry would be excited about generative AI. With computers doing the boring stuff, a small team could whip up a map the size of San Andreas. Crunch becomes a thing of the past; games release in a finished state. A new age beckons. There are, at the very least, two interrelated problems with this narrative. First, there’s the logic of the hype itself—reminiscent of the frenzied gold rush over crypto/Web3/the metaverse—that, consciously or not, seems to consider automating artists’ jobs a form of progress. Second, there’s the gap between these pronouncements and reality. Back in November, when DALL-E was seemingly everywhere, venture capital firm Andreessen Horowitz posted a a long analysis on their website touting a “generative AI revolution in games” that would do everything from shorten development time to change the kinds of titles being made. The following month, Andreessen partner Jonathan Lai posted a Twitter thread expounding on a “Cyberpunk where much of the world/text was generated, enabling devs to shift from asset production to higher-order tasks like storytelling and innovation” and theorizing that AI could enable “good + fast + affordable” game-making. Eventually, Lai’s mentions filled with so many irritated replies that he posted a second thread acknowledging “there are definitely lots of challenges to be solved.” “I have seen some, frankly, ludicrous claims about stuff that’s supposedly just around the corner,” says Patrick Mills, the acting franchise content strategy lead at CD Projekt Red, the developer of Cyberpunk 2077. “I saw people suggesting that AI would be able to build out Night City, for example. I think we’re a ways off from that.” Even those advocating for generative AI in video games think a lot of the excited talk about machine learning in the industry is getting out of hand. It’s “ridiculous,” says Julian Togelius, codirector of the NYU Game Innovation Lab, who has authored dozens of papers on the topic. “Sometimes it feels like the worst kind of crypto bros left the crypto ship as it was sinking, and then they came over here and were like, ‘Generative AI: Start the hype machine.’” It’s not that generative AI can’t or shouldn’t be used in game development, Togelius explains. It’s that people aren’t being realistic about what it could do. Sure, AI could design some generic weapons or write some dialog, but compared to text or image generation, level design is fiendish. You can forgive generators that produce a face with wonky ears or some lines of gibberish text. But a broken game level, no matter how magical it looks, is useless. “It is bullshit,” he says, “You need to throw it out or fix it manually.” Basically—and Togelius has had this conversation with multiple developers—no one wants level generators that work less than 100 percent of the time. They render games unplayable, destroying whole titles. “That’s why it’s so hard to take generative AI that is so hard to control and just put it in there,” he says. A technique analogous to generative AI will be familiar to a lot of gamers: procedural generation. Togelius says, for argument’s sake, that he would be happy to say procedural generation is the same as generative AI (he describes their connection as “kind of a sliding scale”). But procedural generation typically doesn’t use machine learning. Rather than an AI model, it runs on predetermined equations, generating, for example, the gargantuan cosmos of No Man’s Sky. Developers also use software like SpeedTree, which, as its name suggests, conjures forests. The point is that procedural generation systems still require massive human supervision; developers must keep vigilant for unscalable crevasses or monstrous trees. And it’s not even clear that replacing procedural generation with generative AI right now would make a noticeable difference. “These things already exist,” says Togelius. “And it works because this content doesn’t really need to function: It doesn’t have functionality constraints. Maybe you can replace them with deep-learning-based stuff. But I don’t think it’s going to make a big difference. Perhaps it will make some difference in the long run.” There is a general misunderstanding of where the tech is at, explains Mills. “A fundamental reason why these generative AIs can’t make something like Night City is because these tools are designed to produce specific outcomes,” says Mills. “A lot of people seem to be under the impression that these are somehow close to general intelligences. But that’s not how it works. You’d need to custom-build an AI that could build Night City, or open world cities in general.” There’s also a failure to take into account the corporate landscape. Games still employ systems that grew from early technological limitations, like dialog or behavior trees. You can’t just drop fancy machine learning into game franchises that have developed without generative AI in mind. Games—in an industry with huge budgets and tight margins—would need total redesigns to accommodate and take advantage of this technology. Take, for example, non-player characters. Text-based generative AI tools seem like a great way to deepen conversation, and Togelius has been advising developers intrigued by this very idea. But it’s not that simple. Characters based on these language models are liable to go off on tangents, discussing topics outside of the game’s world. “This is super interesting, but it’s also super hard,” says Togelius. “You can’t just drop it in there. It’s not going to work. You can’t expect the NPCs to behave in Skyrim or Elden Ring or Grand Theft Auto or your typical RPG. You have to design around the fact that they are, in some sense, uncontrollable.” Nevertheless, there are some peripheral uses for generative AI right now. A good rule of thumb—one that applies to procedural generation too—is that the less crucial the content is, the more likely deep learning methods could be helpful. “For things like text generation, I could use this today to help generate filler for assets that aren’t really meant to be the focus of the player’s attention, like prop newspapers and such,” says Mills. Another appeal is these tools’ low barrier of entry, says Adrian Hon, the CEO and founder of independent games developer Six to Start and the co-creator of Zombies, Run! Procedural generation, at least as the term is typically understood, requires a coder; anyone can use tools like Midjourney and Stable Diffusion. He can see how they could help with prototyping or mood-boarding during a game’s early concept phase. But, Hon notes, many artists are skeptical of AI. Part of the backlash to the generative AI hype has been that these tools are modeling their output on the work of human creators. Some are even suing Stable Diffusion and Midjourney, claiming that Stable Diffusion, which powers Midjourney, was trained on images used without permission. “Obviously, there’s a whole copyright question. We know about all these suits going on,” he says. “But even if they get resolved, I think that there’ll be some real upset among artists, which is understandable.” As with so many discussions about automation, the hype here is detached from current reality (debates over automation usually arise during times of “deep anxiety about the functioning of the labor market,” writes sociologist Aaron Benanav). But, leaving reality for a second, it’s notable that much of the conversation around generative AI seems almost to revel at the prospect of replacing humans. Even an innocuous statement promising a boon for indie developers—“A small team can make a world the size of Red Dead’s,” for example—contains a kernel of this logic, explains Raphael van Lierop, the founder and creative director of independent studio Hinterland. It’s reductive, suggesting the work of a large part of that large team is mindlessly robotic. “The focus on generative AI is another facet of what feels like an attack on creators and the act of creation, one that is expressed in a lot of different ways in our society right now,” he says. Reflecting a prevailing mood among artists across mediums, he sees nothing interesting about art made by an AI. “It’s a dead end,” he says. There’s definitely an unsettlingly inhuman element to all of this, one that you could imagine manifesting as a torrent of AI-generated shovelware run on predatory monetary systems. But at the higher echelons of game development, games created entirely by machines—ones worth playing, at least—are some way off. “The way some people say it’s going to be used, to just suddenly replace people and do the whole job by itself, is bullshit,” says Togelius. “You need humans.”

---

## Timeframe: 01/2023
### URL: https://www.breitbart.com/tech/2023/01/30/gmail-creator-chatgpt-ai-could-destroy-googles-business-within-two-years/
### Relevance Score: 0.09191793203353882
### Highlights: ["Google may be only a year or two away from total disruption. AI will eliminate the Search Engine Result Page, which is where they make most of their money. Even if they catch up on AI, they can't fully deploy it without destroying the most valuable part of their business!", "AI will eliminate the Search Engine Result Page, which is where they make most of their money. Even if they catch up on AI, they can't fully deploy it without destroying the most valuable part of their business! https://t.co/jtq25LXdkj "]

The creator of Google’s popular email service, Gmail, recently tweeted that he thinks Google could be put out of business by OpenAI’s ChatGPT chatbot within two years. Interesting Engineering reports that Gmail’s creator, Paul Buchheit, recently tweeted that he thinks Google will only be in business for two more years due to the rise in popularity of advanced AI tools. A discussion about the future of search engines like Google has erupted in response to OpenAI’s ChatGPT and the chatbot’s explosive growth. Google may be only a year or two away from total disruption. AI will eliminate the Search Engine Result Page, which is where they make most of their money. Even if they catch up on AI, they can't fully deploy it without destroying the most valuable part of their business! https://t.co/jtq25LXdkj — Paul Buchheit (@paultoo) December 1, 2022 Launched in November 2022, ChatGPT has gained popularity among millions of users as a place to ask questions. ChatGPT offers responses in a conversational style, as opposed to search engines that return pages of results, making it more straightforward for users to ask follow-up questions. The sudden popularity of ChatGPT has caused Google’s top executives to reassess their priorities and shift their attention to AI products. Google’s lucrative search engine, which returns search results, is critical to the company’s business strategy. Most of the company’s income comes from advertising charges for listing goods and services next to search results. Google’s revenue in 2021 was over $250 billion, its highest level in the company’s nearly 25-year history. With the launch of ChatGPT, however, the Gmail maestro thinks Google’s search engine may experience a crisis as users turn to simple solutions rather than indexed pages. Even if Google were to launch internal AI products right away, according to Paul Buchheit, it would be impossible to do so without jeopardizing its primary source of income. Microsoft has signed a massive deal with ChatGPT’s creator in an effort to integrate the chatbot’s capabilities into its own search engine and overtake Google as the market leader. Although ChatGPT itself does not think its existence will have an impact on Google’s search business, Microsoft would profit from Google’s decline. The rapid development of ChatGPT and Microsoft’s backing raises the possibility that AI will have a significant influence on the direction of search engines in the future. Read more at Interesting Engineering here. Lucas Nolan is a reporter for Breitbart News covering issues of free speech and online censorship. Follow him on Twitter @LucasNolan

---

## Timeframe: 01/2023
### URL: https://www.breitbart.com/tech/2023/01/09/wef-creep-yuval-noah-harari-humans-may-need-to-relearn-how-to-see-and-walk-in-virtual-reality-future/
### Relevance Score: 0.05213436111807823
### Highlights: ['“Will it be interesting for an entirely virtual species to inhabit [virtual reality]? Maybe that could be cool, but that doesn’t help us. So even people thinking about uploading their consciousness — I’ve thought through that one a lot — it would be a copy of me, but it wouldn’t be me.”', '… I’ll give an example of how deep it goes: it’s not just what you learned in college or what you learned in kindergarten;\xa0it’s even what you learned as a baby, as a toddler, like learning how to see, or learning how to walk. But what does it mean that I have to relearn how to see and walk? As virtual reality improves — and with all the talk of the metaverse and so forth, which we will\xa0 discuss later on — increasingly, it’s likely that [there] will be many more activities shifting from the physical biological world that we know into a new reality — a virtual reality — which has different physical and biological laws. ']

Historian, futurist, and noted WEF creep Yuval Noah Harari speculated that human beings may need to “relearn how to see and walk” in a virtual reality future divorcing people from the “physical and biological” world during a discussion with podcaster Tom Bilyeu published on Tuesday. Harari, an author and World Economic Forum (WEF) adviser, emphasized humanity’s capacity for change and abstract thinking in his contemplation of a technologically-driven paradigm shift in the human condition with the framework of virtual reality. Humanity may need to “let go” of timeless and ubiquitous components of the human experience, including sight and the ability to walk, he considered. He remarked: Maybe the most important quality to survive and flourish in 21st century is to have mental flexibility — not just to keep learning and changing again and again — [but] also to keep letting go. Part of what makes it difficult to learn new things [is] that we hold on. … I’ll give an example of how deep it goes: it’s not just what you learned in college or what you learned in kindergarten; it’s even what you learned as a baby, as a toddler, like learning how to see, or learning how to walk. But what does it mean that I have to relearn how to see and walk? As virtual reality improves — and with all the talk of the metaverse and so forth, which we will discuss later on — increasingly, it’s likely that [there] will be many more activities shifting from the physical biological world that we know into a new reality — a virtual reality — which has different physical and biological laws. Harari asked, “Can we as humans just shift to the immaterial realm of the Metaverse and leave our biological bodies behind? Or is it impossible or even dangerous to try and separate our kind of mental existence from our bodily and physical existence?” Bilyeu alluded to the centrality of physicality to the human condition while considering a technologically-driven attempt of divorcing humanity from physical existence. “I am very grounded in biology,” he stated. “Will it be interesting for an entirely virtual species to inhabit [virtual reality]? Maybe that could be cool, but that doesn’t help us. So even people thinking about uploading their consciousness — I’ve thought through that one a lot — it would be a copy of me, but it wouldn’t be me.” He continued, “So all of this sadness of death that I would be hoping to avoid by doing that doesn’t help. Maybe it kind of gives the same sense of having a kid, but it wouldn’t by any means save me from having to deal with death.” Follow Robert Kraychik on Twitter @rkraychik .

---

## Timeframe: 01/2023
### URL: https://www.breitbart.com/tech/2023/01/18/tesla-that-caused-in-8-car-san-francisco-pileup-had-full-self-driving-activated/
### Relevance Score: 0.05040936544537544
### Highlights: ['The feature is designed to keep up with traffic, steer within a lane, and follow traffic signals, and despite the “full-self driving” name, it is required that human drivers be prepared to take control at any moment. Drivers are reportedly warned on an in-car screen by Tesla when installing the feature that it “may do the wrong thing at the worst time.” Tesla generally does not respond to request for comments, but said in an update to its vehicle safety data this month: “We are proud of Autopilot’s performance and its impact on reducing traffic collisions. The benefit and promise of Autopilot is clear from the Vehicle Safety Report data that we have been sharing for 4 years.”', 'Drivers are reportedly warned on an in-car screen by Tesla when installing the feature that it “may do the wrong thing at the worst time.” Tesla generally does not respond to request for comments, but said in an update to its vehicle safety data this month: “We are proud of Autopilot’s performance and its impact on reducing traffic collisions. The benefit and promise of Autopilot is clear from the Vehicle Safety Report data that we have been sharing for 4 years.”   Lucas Nolan is a reporter for Breitbart News covering issues of free speech and online censorship.']

The Tesla Model S that braked sharply, triggering an eight-car pileup in San Francisco on Thanksgiving Day, reportedly had Elon Musk’s Autopilot “full self-driving” software engaged at the time of the crash. Breitbart News previously reported that nine people, including a 2-year-old child, were hurt in an eight-vehicle crash caused by a Tesla Model S on Thanksgiving Day on the San Francisco Bay Bridge. The collision happened as the car abruptly changed lanes and stopped in the bridge’s far-left lane. I obtained surveillance footage of the self-driving Tesla that abruptly stopped on the Bay Bridge, resulting in an eight-vehicle crash that injured 9 people including a 2 yr old child just hours after Musk announced the self-driving feature. Full story: https://t.co/LaEvX9TzxW pic.twitter.com/i75jSh2UpN — Ken Klippenstein (@kenklippenstein) January 10, 2023 The East Bay Times now reports that the vehicle had Tesla’s “full self-driving” software enabled at the time of the crash, according to recently released data from the federal government. The Tesla Model S slowed to 7 mph on the highway at the time of the crash according to the data. The Tesla’s driver told authorities that the vehicle’s “full self-driving” software braked unexpectedly triggering the pileup. The National Highway Traffic Safety Administration (NHTSA) announced at the time that it was sending a special crash investigation team to examine the accident. The agency usually conducts such investigations into approximately 100 crashes per year. The crash happened just hours after Tesla CEO Elon Musk announced that the full self-driving system was available to anyone in North America who requested it and paid for the feature. The feature is designed to keep up with traffic, steer within a lane, and follow traffic signals, and despite the “full-self driving” name, it is required that human drivers be prepared to take control at any moment. Drivers are reportedly warned on an in-car screen by Tesla when installing the feature that it “may do the wrong thing at the worst time.” Tesla generally does not respond to request for comments, but said in an update to its vehicle safety data this month: “We are proud of Autopilot’s performance and its impact on reducing traffic collisions. The benefit and promise of Autopilot is clear from the Vehicle Safety Report data that we have been sharing for 4 years.” Read more at the East Bay Times here. Lucas Nolan is a reporter for Breitbart News covering issues of free speech and online censorship. Follow him on Twitter @LucasNolan

---
