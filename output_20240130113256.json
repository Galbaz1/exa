{
    "question": "https://www.wired.com/story/artificial-intelligence-animals-anthropomorphism/",
    "full_extract": "https://www.scientificamerican.com/article/artificial-intelligence-could-finally-let-us-talk-with-animals/ Artificial Intelligence Could Finally Let Us Talk with Animals 2023-10-01 Lois Parshley nderneath the thick forest canopy on a remote island in the South Pacific, a New Caledonian Crow peers from its perch, dark eyes glittering. The bird carefully removes a branch, strips off unwanted leaves with its bill and fashions a hook from the wood. The crow is a perfectionist: if it makes an error, it will scrap the whole thing and start over. When it's satisfied, the bird pokes the finished utensil into a crevice in the tree and fishes out a wriggling grub.The New Caledonian Crow is one of the only birds known to manufacture tools, a skill once thought to be unique to humans. Christian Rutz, a behavioral ecologist at the University of St Andrews in Scotland, has spent much of his career studying the crow's capabilities. The remarkable ingenuity Rutz observed changed his understanding of what birds can do. He started wondering if there might be other overlooked animal capacities. The crows live in complex social groups and may pass toolmaking techniques on to their offspring. Experiments have also shown that different crow groups around the island have distinct vocalizations. Rutz wanted to know whether these dialects could help explain cultural differences in toolmaking among the groups.New technology powered by artificial intelligence is poised to provide exactly these kinds of insights. Whether animals communicate with one another in terms we might be able to understand is a question of enduring fascination. Although people in many Indigenous cultures have long believed that animals can intentionally communicate, Western scientists traditionally have shied away from research that blurs the lines between humans and other animals for fear of being accused of anthropomorphism. But with recent breakthroughs in AI, \u201cpeople realize that we are on the brink of fairly major advances in regard to understanding animals' communicative behavior,\u201d Rutz says.Beyond creating chatbots that woo people and producing art that wins fine-arts competitions, machine learning may soon make it possible to decipher things like crow calls, says Aza Raskin, one of the founders of the nonprofit Earth Species Project. Its team of artificial-intelligence scientists, biologists and conservation experts is collecting a wide range of data from a variety of species and building machine-learning models to analyze them. Other groups such as the Project Cetacean Translation Initiative (CETI) are focusing on trying to understand a particular species, in this case the sperm whale.Decoding animal vocalizations could aid conservation and welfare efforts. It could also have a startling impact on us. Raskin compares the coming revolution to the invention of the telescope. \u201cWe looked out at the universe and discovered that Earth was not the center,\u201d he says. The power of AI to reshape our understanding of animals, he thinks, will have a similar effect. \u201cThese tools are going to change the way that we see ourselves in relation to everything.\u201dWhen Shane Gero got off his research vessel in Dominica after a recent day of fieldwork, he was excited. The sperm whales that he studies have complex social groups, and on this day one familiar young male had returned to his family, providing Gero and his colleagues with an opportunity to record the group's vocalizations as they reunited.For nearly 20 years Gero, a scientist in residence at Carleton University in Ottawa, kept detailed records of two clans of sperm whales in the turquoise waters of the Caribbean, capturing their clicking vocalizations and what the animals were doing when they made them. He found that the whales seemed to use specific patterns of sound, called codas, to identify one another. They learn these codas much the way toddlers learn words and names, by repeating sounds the adults around them make.Having decoded a few of these codas manually, Gero and his colleagues began to wonder whether they could use AI to speed up the translation. As a proof of concept, the team fed some of Gero's recordings to a neural network, an algorithm that learns skills by analyzing data. It was able to correctly identify a small subset of individual whales from the codas 99 percent of the time. Next the team set an ambitious new goal: listen to large swathes of the ocean in the hopes of training a computer to learn to speak whale. Project CETI, for which Gero serves as lead biologist, plans to deploy an underwater microphone attached to a buoy to record the vocalizations of Dominica's resident whales around the clock.As sensors have gotten cheaper and technologies such as hydrophones, biologgers and drones have improved, the amount of animal data has exploded. There's suddenly far too much for biologists to sift through efficiently by hand. AI thrives on vast quantities of information, though. Large language models such as ChatGPT must ingest massive amounts of text to learn how to respond to prompts: ChatGPT-3 was trained on around 45 terabytes of text data, a good chunk of the entire Library of Congress. Early models required humans to classify much of those data with labels. In other words, people had to teach the machines what was important. But the next generation of models learned how to \u201cself-supervise,\u201d automatically learning what's essential and independently creating an algorithm of how to predict what words come next in a sequence.In 2017 two research groups discovered a way to translate between human languages without the need for a Rosetta stone. The discovery hinged on turning the semantic relations between words into geometric ones. Machine-learning models are now able to translate between unknown human languages by aligning their shapes\u2014using the frequency with which words such as \u201cmother\u201d and \u201cdaughter\u201d appear near each other, for example, to accurately predict what comes next. \u201cThere's this hidden underlying structure that seems to unite us all,\u201d Raskin says. \u201cThe door has been opened to using machine learning to decode languages that we don't already know how to decode.\u201dThe field hit another milestone in 2020, when natural-language processing began to be able to \u201ctreat everything as a language,\u201d Raskin explains. Take, for example, DALL-E 2, one of the AI systems that can generate realistic images based on verbal descriptions. It maps the shapes that represent text to the shapes that represent images with remarkable accuracy\u2014exactly the kind of \u201cmultimodal\u201d analysis the translation of animal communication will probably require.Many animals use different modes of communication simultaneously, just as humans use body language and gestures while talking. Any actions made immediately before, during, or after uttering sounds could provide important context for understanding what an animal is trying to convey. Traditionally, researchers have cataloged these behaviors in a list known as an ethogram. With the right training, machine-learning models could help parse these behaviors and perhaps discover novel patterns in the data. Scientists writing in the journal Nature Communications last year, for example, reported that a model found previously unrecognized differences in Zebra Finch songs that females pay attention to when choosing mates. Females prefer partners that sing like the birds the females grew up with.You can already use one kind of AI-powered analysis with Merlin, a free app from the Cornell Lab of Ornithology that identifies bird species. To identify a bird by sound, Merlin takes a user's recording and converts it into a spectrogram\u2014a visualization of the volume, pitch and length of the bird's call. The model is trained on Cornell's audio library, against which it compares the user's recording to predict the species identification. It then compares this guess to eBird, Cornell's global database of observations, to make sure it's a species that one would expect to find in the user's location. Merlin can identify calls from more than 1,000 bird species with remarkable accuracy.But the world is loud, and singling out the tune of one bird or whale from the cacophony is difficult. The challenge of isolating and recognizing individual speakers, known as the cocktail party problem, has long plagued efforts to process animal vocalizations. In 2021 the Earth Species Project built a neural network that can separate overlapping animal sounds into individual tracks and filter background noise, such as car honks\u2014and it released the open-source code for free. It works by creating a visual representation of the sound, which the neural network uses to determine which pixel is produced by which speaker. In addition, the Earth Species Project recently developed a so-called foundational model that can automatically detect and classify patterns in datasets. New Caledonian Crows, which are famous for their toolmaking abilities, have regionally distinctive vocalizations that could one day be deciphered using AI. Credit: Jean-Paul Ferrero/Auscape International Pty Ltd/Alamy Stock PhotoNot only are these tools transforming research, but they also have practical value. If scientists can translate animal sounds, they may be able to help imperiled species. The Hawaiian Crow, known locally as the \u2018Alal\u0101, went extinct in the wild in the early 2000s. The last birds were brought into captivity to start a conservation breeding program. Expanding on his work with the New Caledonian Crow, Rutz is now collaborating with the Earth Species Project to study the Hawaiian Crow's vocabulary. \u201cThis species has been removed from its natural environment for a very long time,\u201d he says. He is developing an inventory of all the calls the captive birds currently use. He'll compare that to historical recordings of the last wild Hawaiian Crows to determine whether their repertoire has changed in captivity. He wants to know whether they may have lost important calls, such as those pertaining to predators or courtship, which could help explain why reintroducing the crow to the wild has proved so difficult.Machine-learning models could someday help us figure out our pets, too. For a long time animal behaviorists didn't pay much attention to domestic pets, says Con Slobodchikoff, author of Chasing Doctor Dolittle: Learning the Language of Animals. When he began his career studying prairie dogs, he quickly gained an appreciation for their sophisticated calls, which can describe the size and shape of predators. That experience helped to inform his later work as a behavioral consultant for misbehaving dogs. He found that many of his clients completely misunderstood what their dog was trying to convey. When our pets try to communicate with us, they often use multimodal signals, such as a bark combined with a body posture. Yet \u201cwe are so fixated on sound being the only valid element of communication, that we miss many of the other cues,\u201d he says.Now Slobodchikoff is developing an AI model aimed at translating a dog's facial expressions and barks for its owner. He has no doubt that as researchers expand their studies to domestic animals, machine-learning advances will reveal surprising capabilities in pets. \u201cAnimals have thoughts, hopes, maybe dreams of their own,\u201d he says.Farmed animals could also benefit from such depth of understanding. Elodie F. Briefer, an associate professor in animal behavior at the University of Copenhagen, has shown that it's possible to assess animals' emotional states based on their vocalizations. She recently created an algorithm trained on thousands of pig sounds that uses machine learning to predict whether the animals were experiencing a positive or negative emotion. Briefer says a better grasp of how animals experience feelings could spur efforts to improve their welfare.But as good as language models are at finding patterns, they aren't actually deciphering meaning\u2014and they definitely aren't always right. Even AI experts often don't understand how algorithms arrive at their conclusions, making them harder to validate. Benjamin Hoffman, who helped to develop the Merlin app before joining the Earth Species Project, says that one of the biggest challenges scientists now face is figuring out how to learn from what these models discover.\u201cThe choices made on the machine-learning side affect what kinds of scientific questions we can ask,\u201d Hoffman says. Merlin Sound ID, he explains, can help detect which birds are present, which is useful for ecological research. It can't, however, help answer questions about behavior, such as what types of calls an individual bird makes when it interacts with a potential mate. In trying to interpret different kinds of animal communication, Hoffman says researchers must also \u201cunderstand what the computer is doing when it's learning how to do that.\u201daniela Rus, director of the Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory, leans back in an armchair in her office, surrounded by books and stacks of papers. She is eager to explore the new possibilities for studying animal communication that machine learning has opened up. Rus previously designed remote-controlled robots to collect data for whale-behavior research in collaboration with biologist Roger Payne, whose recordings of humpback whale songs in the 1970s helped to popularize the Save the Whales movement. Now Rus is bringing her programming experience to Project CETI. Sensors for underwater monitoring have rapidly advanced, providing the equipment necessary to capture animal sounds and behavior. And AI models capable of analyzing those data have improved dramatically. But until recently, the two disciplines hadn't been joined.At Project CETI, Rus's first task was to isolate sperm whale clicks from the background noise of the ocean realm. Sperm whales' vocalizations were long compared to binary code in the way that they represent information. But they are more sophisticated than that. After she developed accurate acoustic measurements, Rus used machine learning to analyze how these clicks combine into codas, looking for patterns and sequences. \u201cOnce you have this basic ability,\u201d she says, \u201cthen we can start studying what are some of the foundational components of the language.\u201d The team will tackle that question directly, Rus says, \u201canalyzing whether the [sperm whale] lexicon has the properties of language or not.\u201dBut grasping the structure of a language is not a prerequisite to speaking it\u2014not anymore, anyway. It's now possible for AI to take three seconds of human speech and then hold forth at length with its same patterns and intonations in an exact mimicry. In the next year or two, Raskin predicts, \u201cwe'll be able to build this for animal communication.\u201d The Earth Species Project is already developing AI models that emulate a variety of species, with the aim of having \u201cconversations\u201d with animals. He says two-way communication will make it that much easier for researchers to infer the meaning of animal vocalizations.In collaboration with outside biologists, the Earth Species Project plans to test playback experiments, playing an artificially generated call to Zebra Finches in a laboratory setting and then observing how the birds respond. Soon \u201cwe'll be able to pass the finch, crow or whale Turing test,\u201d Raskin asserts, referring to the point at which the animals won't be able to tell they are conversing with a machine rather than one of their own. \u201cThe plot twist is that we will be able to communicate before we understand.\u201dThe prospect of this achievement raises ethical concerns. Karen Bakker, a digital innovations researcher and author of The Sounds of Life: How Digital Technology Is Bringing Us Closer to the Worlds of Animals and Plants, explains that there may be unintended ramifications. Commercial industries could use AI for precision fishing by listening for schools of target species or their predators; poachers could deploy these techniques to locate endangered animals and impersonate their calls to lure them closer. For animals such as humpback whales, whose mysterious songs can spread across oceans with remarkable speed, the creation of a synthetic song could, Bakker says, \u201cinject a viral meme into the world's population\u201d with unknown social consequences.So far the organizations at the leading edge of this animal-communication work are nonprofits like the Earth Species Project that are committed to open-source sharing of data and models and staffed by enthusiastic scientists driven by their passion for the animals they study. But the field might not stay that way\u2014profit-driven players could misuse this technology. In a recent article in Science, Rutz and his co-authors noted that \u201cbest-practice guidelines and appropriate legislative frameworks\u201d are urgently needed. \u201cIt's not enough to make the technology,\u201d Raskin warns. \u201cEvery time you invent a technology, you also invent a responsibility.\u201dDesigning a \u201cwhale chatbot,\u201d as Project CETI aspires to do, isn't as simple as figuring out how to replicate sperm whales' clicks and whistles; it also demands that we imagine an animal's experience. Despite major physical differences, humans actually share many basic forms of communication with other animals. Consider the interactions between parents and offspring. The cries of mammalian infants, for example, can be incredibly similar, to the point that white-tailed deer will respond to whimpers whether they're made by marmots, humans or seals. Vocal expression in different species can develop similarly, too. Like human babies, harbor seal pups learn to change their pitch to target a parent's eardrums. And both baby songbirds and human toddlers engage in babbling\u2014a \u201ccomplex sequence of syllables learned from a tutor,\u201d explains Johnathan Fritz, a research scientist at the University of Maryland's Brain and Behavior Initiative.Whether animal utterances are comparable to human language in terms of what they convey remains a matter of profound disagreement, however. \u201cSome would assert that language is essentially defined in terms that make humans the only animal capable of language,\u201d Bakker says, with rules for grammar and syntax. Skeptics worry that treating animal communication as language, or attempting to translate it, may distort its meaning.Raskin shrugs off these concerns. He doubts animals are saying \u201cpass me the banana,\u201d but he suspects we will discover some basis for communication in common experiences. \u201cIt wouldn't surprise me if we discovered [expressions for] \u2018grief\u2019 or \u2018mother\u2019 or \u2018hungry\u2019 across species,\u201d he says. After all, the fossil record shows that creatures such as whales have been vocalizing for tens of millions of years. \u201cFor something to survive a long time, it has to encode something very deep and very true.\u201dUltimately real translation may require not just new tools but the ability to see past our own biases and expectations. Last year, as the crusts of snow retreated behind my house, a pair of Sandhill Cranes began to stalk the brambles. A courtship progressed, the male solicitous and preening. Soon every morning one bird flapped off alone to forage while the other stayed behind to tend their eggs. We fell into a routine, the birds and I: as the sun crested the hill, I kept one eye toward the windows, counting the days as I imagined cells dividing, new wings forming in the warm, amniotic dark.Then one morning it ended. Somewhere behind the house the birds began to wail, twining their voices into a piercing cry until suddenly I saw them both running down the hill into the stutter start of flight. They circled once and then disappeared. I waited for days, but I never saw them again.Wondering if they were mourning a failed nest or whether I was reading too much into their behavior, I reached out to George Happ and Christy Yuncker, retired scientists who for two decades shared their pond in Alaska with a pair of wild Sandhill Cranes they nicknamed Millie and Roy. They assured me that they, too, had seen the birds react to death. After one of Millie and Roy's colts died, Roy began picking up blades of grass and dropping them near his offspring's body. That evening, as the sun slipped toward the horizon, the family began to dance. The surviving colt joined its parents as they wheeled and jumped, throwing their long necks back to the sky.Happ knows critics might disapprove of their explaining the birds' behaviors as grief, considering that \u201cwe cannot precisely specify the underlying physiological correlates.\u201d But based on the researchers' close observations of the crane couple over a decade, he writes, interpreting these striking reactions as devoid of emotion \u201cflies in the face of the evidence.\u201dEveryone can eventually relate to the pain of losing a loved one. It's a moment ripe for translation.Perhaps the true value of any language is that it helps us relate to others and in so doing frees us from the confines of our own minds. Every spring, as the light swept back over Yuncker and Happ's home, they waited for Millie and Roy to return. In 2017 they waited in vain. Other cranes vied for the territory. The two scientists missed watching the colts hatch and grow. But last summer a new crane pair built a nest. Before long, their colts peeped through the tall grass, begging for food and learning to dance. Life began a new cycle. \u201cWe're always looking at nature,\u201d Yuncker says, \u201cwhen really, we're part of it.\u201d||||I|||| Skip to main content Scientific American Sign in October 1, 2023 14 min read Artificial Intelligence Could Finally Let Us Talk with Animals AI is poised to revolutionize our understanding of animal communication By Lois Parshley The Project Cetacean Translation Initiative (CETI) is using machine learning to try to understand the vocalizations of sperm whales. Credit: Franco Banfi/Minden Pictures October 2023 Issue Behavior U nderneath the thick forest canopy on a remote island in the South Pacific, a New Caledonian Crow peers from its perch, dark eyes glittering. The bird carefully removes a branch, strips off unwanted leaves with its bill and fashions a hook from the wood. The crow is a perfectionist: if it makes an error, it will scrap the whole thing and start over. When it's satisfied, the bird pokes the finished utensil into a crevice in the tree and fishes out a wriggling grub. The New Caledonian Crow is one of the only birds known to manufacture tools, a skill once thought to be unique to humans. Christian Rutz, a behavioral ecologist at the University of St Andrews in Scotland, has spent much of his career studying the crow's capabilities. The remarkable ingenuity Rutz observed changed his understanding of what birds can do. He started wondering if there might be other overlooked animal capacities. The crows live in complex social groups and may pass toolmaking techniques on to their offspring. Experiments have also shown that different crow groups around the island have distinct vocalizations. Rutz wanted to know whether these dialects could help explain cultural differences in toolmaking among the groups. New technology powered by artificial intelligence is poised to provide exactly these kinds of insights. Whether animals communicate with one another in terms we might be able to understand is a question of enduring fascination. Although people in many Indigenous cultures have long believed that animals can intentionally communicate, Western scientists traditionally have shied away from research that blurs the lines between humans and other animals for fear of being accused of anthropomorphism. But with recent breakthroughs in AI, \u201cpeople realize that we are on the brink of fairly major advances in regard to understanding animals' communicative behavior,\u201d Rutz says. Beyond creating chatbots that woo people and producing art that wins fine-arts competitions, machine learning may soon make it possible to decipher things like crow calls, says Aza Raskin, one of the founders of the nonprofit Earth Species Project. Its team of artificial-intelligence scientists, biologists and conservation experts is collecting a wide range of data from a variety of species and building machine-learning models to analyze them. Other groups such as the Project Cetacean Translation Initiative (CETI) are focusing on trying to understand a particular species, in this case the sperm whale. Decoding animal vocalizations could aid conservation and welfare efforts. It could also have a startling impact on us. Raskin compares the coming revolution to the invention of the telescope. \u201cWe looked out at the universe and discovered that Earth was not the center,\u201d he says. The power of AI to reshape our understanding of animals, he thinks, will have a similar effect. \u201cThese tools are going to change the way that we see ourselves in relation to everything.\u201d When Shane Gero got off his research vessel in Dominica after a recent day of fieldwork, he was excited. The sperm whales that he studies have complex social groups, and on this day one familiar young male had returned to his family, providing Gero and his colleagues with an opportunity to record the group's vocalizations as they reunited. For nearly 20 years Gero, a scientist in residence at Carleton University in Ottawa, kept detailed records of two clans of sperm whales in the turquoise waters of the Caribbean, capturing their clicking vocalizations and what the animals were doing when they made them. He found that the whales seemed to use specific patterns of sound, called codas, to identify one another. They learn these codas much the way toddlers learn words and names, by repeating sounds the adults around them make. Having decoded a few of these codas manually, Gero and his colleagues began to wonder whether they could use AI to speed up the translation. As a proof of concept, the team fed some of Gero's recordings to a neural network, an algorithm that learns skills by analyzing data. It was able to correctly identify a small subset of individual whales from the codas 99 percent of the time. Next the team set an ambitious new goal: listen to large swathes of the ocean in the hopes of training a computer to learn to speak whale. Project CETI, for which Gero serves as lead biologist, plans to deploy an underwater microphone attached to a buoy to record the vocalizations of Dominica's resident whales around the clock. As sensors have gotten cheaper and technologies such as hydrophones, biologgers and drones have improved, the amount of animal data has exploded. There's suddenly far too much for biologists to sift through efficiently by hand. AI thrives on vast quantities of information, though. Large language models such as ChatGPT must ingest massive amounts of text to learn how to respond to prompts: ChatGPT-3 was trained on around 45 terabytes of text data, a good chunk of the entire Library of Congress. Early models required humans to classify much of those data with labels. In other words, people had to teach the machines what was important. But the next generation of models learned how to \u201cself-supervise,\u201d automatically learning what's essential and independently creating an algorithm of how to predict what words come next in a sequence. In 2017 two research groups discovered a way to translate between human languages without the need for a Rosetta stone. The discovery hinged on turning the semantic relations between words into geometric ones. Machine-learning models are now able to translate between unknown human languages by aligning their shapes\u2014using the frequency with which words such as \u201cmother\u201d and \u201cdaughter\u201d appear near each other, for example, to accurately predict what comes next. \u201cThere's this hidden underlying structure that seems to unite us all,\u201d Raskin says. \u201cThe door has been opened to using machine learning to decode languages that we don't already know how to decode.\u201d The field hit another milestone in 2020, when natural-language processing began to be able to \u201ctreat everything as a language,\u201d Raskin explains. Take, for example, DALL-E 2, one of the AI systems that can generate realistic images based on verbal descriptions. It maps the shapes that represent text to the shapes that represent images with remarkable accuracy\u2014exactly the kind of \u201cmultimodal\u201d analysis the translation of animal communication will probably require. Many animals use different modes of communication simultaneously, just as humans use body language and gestures while talking. Any actions made immediately before, during, or after uttering sounds could provide important context for understanding what an animal is trying to convey. Traditionally, researchers have cataloged these behaviors in a list known as an ethogram. With the right training, machine-learning models could help parse these behaviors and perhaps discover novel patterns in the data. Scientists writing in the journal Nature Communications last year, for example, reported that a model found previously unrecognized differences in Zebra Finch songs that females pay attention to when choosing mates. Females prefer partners that sing like the birds the females grew up with. You can already use one kind of AI-powered analysis with Merlin, a free app from the Cornell Lab of Ornithology that identifies bird species. To identify a bird by sound, Merlin takes a user's recording and converts it into a spectrogram\u2014a visualization of the volume, pitch and length of the bird's call. The model is trained on Cornell's audio library, against which it compares the user's recording to predict the species identification. It then compares this guess to eBird, Cornell's global database of observations, to make sure it's a species that one would expect to find in the user's location. Merlin can identify calls from more than 1,000 bird species with remarkable accuracy. But the world is loud, and singling out the tune of one bird or whale from the cacophony is difficult. The challenge of isolating and recognizing individual speakers, known as the cocktail party problem, has long plagued efforts to process animal vocalizations. In 2021 the Earth Species Project built a neural network that can separate overlapping animal sounds into individual tracks and filter background noise, such as car honks\u2014and it released the open-source code for free. It works by creating a visual representation of the sound, which the neural network uses to determine which pixel is produced by which speaker. In addition, the Earth Species Project recently developed a so-called foundational model that can automatically detect and classify patterns in datasets. New Caledonian Crows, which are famous for their toolmaking abilities, have regionally distinctive vocalizations that could one day be deciphered using AI. Credit: Jean-Paul Ferrero/Auscape International Pty Ltd/Alamy Stock Photo Not only are these tools transforming research, but they also have practical value. If scientists can translate animal sounds, they may be able to help imperiled species. The Hawaiian Crow, known locally as the \u2018Alal\u0101, went extinct in the wild in the early 2000s. The last birds were brought into captivity to start a conservation breeding program. Expanding on his work with the New Caledonian Crow, Rutz is now collaborating with the Earth Species Project to study the Hawaiian Crow's vocabulary. \u201cThis species has been removed from its natural environment for a very long time,\u201d he says. He is developing an inventory of all the calls the captive birds currently use. He'll compare that to historical recordings of the last wild Hawaiian Crows to determine whether their repertoire has changed in captivity. He wants to know whether they may have lost important calls, such as those pertaining to predators or courtship, which could help explain why reintroducing the crow to the wild has proved so difficult. Machine-learning models could someday help us figure out our pets, too. For a long time animal behaviorists didn't pay much attention to domestic pets, says Con Slobodchikoff, author of Chasing Doctor Dolittle: Learning the Language of Animals. When he began his career studying prairie dogs, he quickly gained an appreciation for their sophisticated calls, which can describe the size and shape of predators. That experience helped to inform his later work as a behavioral consultant for misbehaving dogs. He found that many of his clients completely misunderstood what their dog was trying to convey. When our pets try to communicate with us, they often use multimodal signals, such as a bark combined with a body posture. Yet \u201cwe are so fixated on sound being the only valid element of communication, that we miss many of the other cues,\u201d he says. Now Slobodchikoff is developing an AI model aimed at translating a dog's facial expressions and barks for its owner. He has no doubt that as researchers expand their studies to domestic animals, machine-learning advances will reveal surprising capabilities in pets. \u201cAnimals have thoughts, hopes, maybe dreams of their own,\u201d he says. Farmed animals could also benefit from such depth of understanding. Elodie F. Briefer, an associate professor in animal behavior at the University of Copenhagen, has shown that it's possible to assess animals' emotional states based on their vocalizations. She recently created an algorithm trained on thousands of pig sounds that uses machine learning to predict whether the animals were experiencing a positive or negative emotion. Briefer says a better grasp of how animals experience feelings could spur efforts to improve their welfare. But as good as language models are at finding patterns, they aren't actually deciphering meaning\u2014and they definitely aren't always right. Even AI experts often don't understand how algorithms arrive at their conclusions, making them harder to validate. Benjamin Hoffman, who helped to develop the Merlin app before joining the Earth Species Project, says that one of the biggest challenges scientists now face is figuring out how to learn from what these models discover. \u201cThe choices made on the machine-learning side affect what kinds of scientific questions we can ask,\u201d Hoffman says. Merlin Sound ID, he explains, can help detect which birds are present, which is useful for ecological research. It can't, however, help answer questions about behavior, such as what types of calls an individual bird makes when it interacts with a potential mate. In trying to interpret different kinds of animal communication, Hoffman says researchers must also \u201cunderstand what the computer is doing when it's learning how to do that.\u201d D aniela Rus, director of the Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory, leans back in an armchair in her office, surrounded by books and stacks of papers. She is eager to explore the new possibilities for studying animal communication that machine learning has opened up. Rus previously designed remote-controlled robots to collect data for whale-behavior research in collaboration with biologist Roger Payne, whose recordings of humpback whale songs in the 1970s helped to popularize the Save the Whales movement. Now Rus is bringing her programming experience to Project CETI. Sensors for underwater monitoring have rapidly advanced, providing the equipment necessary to capture animal sounds and behavior. And AI models capable of analyzing those data have improved dramatically. But until recently, the two disciplines hadn't been joined. At Project CETI, Rus's first task was to isolate sperm whale clicks from the background noise of the ocean realm. Sperm whales' vocalizations were long compared to binary code in the way that they represent information. But they are more sophisticated than that. After she developed accurate acoustic measurements, Rus used machine learning to analyze how these clicks combine into codas, looking for patterns and sequences. \u201cOnce you have this basic ability,\u201d she says, \u201cthen we can start studying what are some of the foundational components of the language.\u201d The team will tackle that question directly, Rus says, \u201canalyzing whether the [sperm whale] lexicon has the properties of language or not.\u201d But grasping the structure of a language is not a prerequisite to speaking it\u2014not anymore, anyway. It's now possible for AI to take three seconds of human speech and then hold forth at length with its same patterns and intonations in an exact mimicry. In the next year or two, Raskin predicts, \u201cwe'll be able to build this for animal communication.\u201d The Earth Species Project is already developing AI models that emulate a variety of species, with the aim of having \u201cconversations\u201d with animals. He says two-way communication will make it that much easier for researchers to infer the meaning of animal vocalizations. In collaboration with outside biologists, the Earth Species Project plans to test playback experiments, playing an artificially generated call to Zebra Finches in a laboratory setting and then observing how the birds respond. Soon \u201cwe'll be able to pass the finch, crow or whale Turing test,\u201d Raskin asserts, referring to the point at which the animals won't be able to tell they are conversing with a machine rather than one of their own. \u201cThe plot twist is that we will be able to communicate before we understand.\u201d The prospect of this achievement raises ethical concerns. Karen Bakker, a digital innovations researcher and author of The Sounds of Life: How Digital Technology Is Bringing Us Closer to the Worlds of Animals and Plants, explains that there may be unintended ramifications. Commercial industries could use AI for precision fishing by listening for schools of target species or their predators; poachers could deploy these techniques to locate endangered animals and impersonate their calls to lure them closer. For animals such as humpback whales, whose mysterious songs can spread across oceans with remarkable speed, the creation of a synthetic song could, Bakker says, \u201cinject a viral meme into the world's population\u201d with unknown social consequences. So far the organizations at the leading edge of this animal-communication work are nonprofits like the Earth Species Project that are committed to open-source sharing of data and models and staffed by enthusiastic scientists driven by their passion for the animals they study. But the field might not stay that way\u2014profit-driven players could misuse this technology. In a recent article in Science, Rutz and his co-authors noted that \u201cbest-practice guidelines and appropriate legislative frameworks\u201d are urgently needed. \u201cIt's not enough to make the technology,\u201d Raskin warns. \u201cEvery time you invent a technology, you also invent a responsibility.\u201d Designing a \u201cwhale chatbot,\u201d as Project CETI aspires to do, isn't as simple as figuring out how to replicate sperm whales' clicks and whistles; it also demands that we imagine an animal's experience. Despite major physical differences, humans actually share many basic forms of communication with other animals. Consider the interactions between parents and offspring. The cries of mammalian infants, for example, can be incredibly similar, to the point that white-tailed deer will respond to whimpers whether they're made by marmots, humans or seals. Vocal expression in different species can develop similarly, too. Like human babies, harbor seal pups learn to change their pitch to target a parent's eardrums. And both baby songbirds and human toddlers engage in babbling\u2014a \u201ccomplex sequence of syllables learned from a tutor,\u201d explains Johnathan Fritz, a research scientist at the University of Maryland's Brain and Behavior Initiative. Whether animal utterances are comparable to human language in terms of what they convey remains a matter of profound disagreement, however. \u201cSome would assert that language is essentially defined in terms that make humans the only animal capable of language,\u201d Bakker says, with rules for grammar and syntax. Skeptics worry that treating animal communication as language, or attempting to translate it, may distort its meaning. Raskin shrugs off these concerns. He doubts animals are saying \u201cpass me the banana,\u201d but he suspects we will discover some basis for communication in common experiences. \u201cIt wouldn't surprise me if we discovered [expressions for] \u2018grief\u2019 or \u2018mother\u2019 or \u2018hungry\u2019 across species,\u201d he says. After all, the fossil record shows that creatures such as whales have been vocalizing for tens of millions of years. \u201cFor something to survive a long time, it has to encode something very deep and very true.\u201d Ultimately real translation may require not just new tools but the ability to see past our own biases and expectations. Last year, as the crusts of snow retreated behind my house, a pair of Sandhill Cranes began to stalk the brambles. A courtship progressed, the male solicitous and preening. Soon every morning one bird flapped off alone to forage while the other stayed behind to tend their eggs. We fell into a routine, the birds and I: as the sun crested the hill, I kept one eye toward the windows, counting the days as I imagined cells dividing, new wings forming in the warm, amniotic dark. Then one morning it ended. Somewhere behind the house the birds began to wail, twining their voices into a piercing cry until suddenly I saw them both running down the hill into the stutter start of flight. They circled once and then disappeared. I waited for days, but I never saw them again. Wondering if they were mourning a failed nest or whether I was reading too much into their behavior, I reached out to George Happ and Christy Yuncker, retired scientists who for two decades shared their pond in Alaska with a pair of wild Sandhill Cranes they nicknamed Millie and Roy. They assured me that they, too, had seen the birds react to death. After one of Millie and Roy's colts died, Roy began picking up blades of grass and dropping them near his offspring's body. That evening, as the sun slipped toward the horizon, the family began to dance. The surviving colt joined its parents as they wheeled and jumped, throwing their long necks back to the sky. Happ knows critics might disapprove of their explaining the birds' behaviors as grief, considering that \u201cwe cannot precisely specify the underlying physiological correlates.\u201d But based on the researchers' close observations of the crane couple over a decade, he writes, interpreting these striking reactions as devoid of emotion \u201cflies in the face of the evidence.\u201d Everyone can eventually relate to the pain of losing a loved one. It's a moment ripe for translation. Perhaps the true value of any language is that it helps us relate to others and in so doing frees us from the confines of our own minds. Every spring, as the light swept back over Yuncker and Happ's home, they waited for Millie and Roy to return. In 2017 they waited in vain. Other cranes vied for the territory. The two scientists missed watching the colts hatch and grow. But last summer a new crane pair built a nest. Before long, their colts peeped through the tall grass, begging for food and learning to dance. Life began a new cycle. \u201cWe're always looking at nature,\u201d Yuncker says, \u201cwhen really, we're part of it.\u201d Rights & Permissions Lois Parshley is an investigative journalist. Her climate reporting can be found on X (formerly known as Twitter) and Mastodon @loisparshley More by Lois Parshley This article was originally published with the title \u201c Talking with Animals \u201d in Scientific American Magazine Vol. 329 No. 3 (October 2023 ) , p. 44 doi:10.1038/scientificamerican1023-44 View This Issue Expand Your World with Science Learn and share the most exciting discoveries, innovations and ideas shaping our world today. SubscribeSign up for our newslettersSee the latest storiesRead the latest issue Follow Us: * Return & Refund Policy * About * Press Room * FAQs * Contact Us * Site Map * Advertise * SA Custom Media * Terms of Use * Privacy Policy * California Consumer Privacy Statement * Use of cookies/Do not sell my data * International Editions Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers. \u00a9 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC. ALL RIGHTS RESERVED. Welcome to AI Decoded, Fast Company\u2019s weekly LinkedIn newsletter that breaks down the most important news in the world of AI. If a friend or colleague shared this newsletter with you, you can sign up to receive it every week here.Meta to use AI bot characters to attract younger usersIn 2021, Meta CEO Mark Zuckerberg, painfully aware of Facebook\u2019s inability to attract young users, directed the various teams in his company to double down on their efforts of attracting a Gen Z crowd and focus less on its existing, significantly older active user base. AI, it turns out, plays a big part in that initiative. The Wall Street Journal reported this week that Meta will soon announce that it\u2019s availing a line of AI chatbots with defined \u201cpersonas\u201d across its social apps. A bot called \u201cBob the Robot\u201d can help you write code, but also is a self-described \u201csassmaster general,\u201d according to the Journal. Another bot, \u201cAlvin the Alien,\u201d is very curious about users\u2019 lives and habits. Meta has also worked on letting celebrities and creators interact with fans and followers via their own persona-driven bots.Imbuing a chatbot with a sassy persona is nothing new. Microsoft did it long ago with Tay. More recently, Snap did it with MyAI, which was built on top of OpenAI\u2019s ChatGPT large language model. Character.AI lets users create persona bots or engage with chatbots based on famous people.Should human beings interact with bots as they would other humans? A new report from the advocacy group Public Citizen offers an emphatic no. \u201cThe human mind is naturally inclined to infer that something that can talk must be human and is ill-equipped to cope with machines that emulate unique human qualities like emotions and opinions,\u201d Public Citizen research director Rick Claypool writes in the report. \u201cSuch systems can manipulate users in commercial transactions and isolate users by taking on social roles ordinarily filled by real people.\u201d Despite any such trepidations, the days of bland, all-purpose LLM chatbots are probably numbered. The basic, generalized knowledge that chatbots gain from training will be table stakes, and the bot will be distinguished by the layers of knowledge, skills, and \u201cpersonality\u201d layered on top. Let\u2019s hope that the bots of the future won\u2019t pose as humans or use their surprising command of the language to persuade or manipulate.ChatGPT can now comprehend images and sounds, not just textNot only are AI chatbots gaining personas, but, far more importantly, they\u2019re gaining new senses. The first large language models that powered chatbots were trained only on text and had no way to comprehend sounds or images. This week, OpenAI announced it has given the AI model underneath ChatGPT the ability to process both aural and visual data. A ChatGPT user, for example, can show the bot an image and then enter into a verbal conversation about it with the bot. It\u2019s important to note that ChatGPT isn In the 1970s a young gorilla known as Koko drew worldwide attention with her ability to use human sign language. But skeptics maintain that Koko and other animals that \u201clearned\u201d to speak (including chimpanzees and dolphins) could not truly understand what they were \u201csaying\u201d\u2014and that trying to make other species use human language, in which symbols represent things that may not be physically present, is futile. \u201cThere's one set of researchers that's keen on finding out whether animals can engage in symbolic communication and another set that says, \u2018That is anthropomorphizing. We need to understand nonhuman communication on its own terms,\u2019\u201d says Karen Bakker, a professor at the University of British Columbia and a fellow at the Harvard Radcliffe Institute for Advanced Study. Now scientists are using improved sensors and artificial-intelligence technology to observe and decode how a broad range of species, including plants, already share information with their own methods. This field of \u201cdigital bioacoustics\u201d is the subject of Bakker's 2022 book The Sounds of Life: How Digital Technology Is Bringing Us Closer to the Worlds of Animals and Plants (Princeton University Press). Scientific American spoke with Bakker about how technology can help humans communicate with creatures such as bats and honeybees\u2014and how these conversations are forcing us to rethink our relationship with other species. [An edited transcript of the interview follows.] Can you give us a brief history of humans attempting to communicate with animals? There were numerous attempts in the mid-20th century to try to teach human language to nonhumans, primates such as Koko. And those efforts were somewhat controversial. As we look back, one view we have now (that may not have been so prevalent then) is that we were too anthropocentric in our approaches. The desire then was to assess nonhuman intelligence by teaching nonhumans to speak like we do\u2014when in fact we should have been thinking about their abilities to engage in complex communication on their own terms, in their own embodied way, in their own worldview. One of the terms used in the book is the notion of umwelt, which is this idea of the lived experience of organisms. If we are attentive to the umwelt of another organism, we wouldn't expect a honeybee to speak human language, but we would become very interested in the fascinating language of honeybees, which is vibrational and positional. It's sensitive to nuances such as the polarization of sunlight that we can't even begin to convey with our bodies. That is where the science is today. The field of digital bioacoustics\u2014which is accelerating exponentially and unveiling fascinating findings about communication across the tree of life\u2014is now approaching these animals and asking not \u201cCan they speak like humans?\u201d but \u201cCan they communicate complex information to one another? How are they doing so? What is significant to them?\u201d I would say that's a more biocentric approach, or at the very least it's less anthropocentric. Taking a bigger view, I think it's also important to acknowledge that listening to nature, \u201cdeep listening,\u201d has a long and venerable tradition. It's an ancient art that is still practiced in an unmediated form. There are long-standing Indigenous traditions of deep listening that are deeply attuned to nonhuman sounds. So if we combine digital listening\u2014which is opening up vast new worlds of nonhuman sound and decoding that sound with artificial intelligence\u2014with deep listening, I believe that we are on the brink of two important discoveries. The first is language in nonhumans. And that's a very controversial statement, which we can dig into. The second is: I believe we're at the brink of interspecies communication. What kind of technology is enabling these breakthroughs? Digital bioacoustics relies on very small, portable, lightweight digital recorders, which are like miniature microphones that scientists are installing everywhere from the Arctic to the Amazon. You can put these microphones on the backs of turtles or whales. You can put them deep in the ocean or on the highest mountaintop or attach them to birds. They can record continuously, 24/7, in remote places scientists cannot easily reach, even in the dark, and without the disruption that comes from introducing human observers in an ecosystem. That instrumentation creates a data deluge, and that is where artificial intelligence comes in\u2014because the same natural-language-processing algorithms that we are using to such great effect in tools such as Google Translate can also be used to detect patterns in nonhuman communication. Sign up for Scientific American\u2019s free newsletters. What's an example of these communication patterns? In the bat chapter where I discuss the research of Yossi Yovel of Tel Aviv University, there's a particular study in which his team monitored [nearly two] dozen Egyptian fruit bats for two and a half months and recorded their vocalizations. They then adapted a voice-recognition program to analyze [15,000 of] the sounds, and the algorithm correlated specific sounds with specific social interactions captured via videos\u2014such as when two bats fought over food. Using this, the researchers were able to classify the majority of bats' sounds. That is how Yovel and other researchers such as Gerry Carter of the Ohio State University have been able to determine that bats have much more complex language than we previously understood. Bats argue over food; they distinguish between genders when they communicate with one another; they have individual names, or \u201csignature calls.\u201d Mother bats speak to their babies in an equivalent of \u201cmotherese.\u201d But whereas human mothers raise the pitch of their voices when talking to babies, mother bats lower the pitch\u2014which elicits a babble response in the babies that learn to \u201cspeak\u201d specific words or referential signals as they grow up. So bats engage in vocal learning. That's a great example of how deep learning is able to derive these patterns from this instrumentation, all of these sensors and microphones, and reveal to us something that we could not access with the naked human ear. Because most of bat communication is in the ultrasonic, above our hearing range, and because bats speak much faster than we do, we have to slow it down to listen to it, as well as reduce the frequency. So we cannot listen like a bat, but our computers can. The next insight is that our computers can also speak back to the bat. The software produces specific patterns and uses those to communicate back to the bat colony or to the beehive, and that is what researchers are now doing. How are researchers talking to bees? The honeybee research is fascinating. A researcher named Tim Landgraf of Freie Universit\u00e4t Berlin studies bee communication, which, as I mentioned earlier, is vibrational and positional. When honeybees \u201cspeak\u201d to one another, it's their body movements, as well as the sounds, that matter. Now computers, and particularly deep-learning algorithms, are able to follow this because you can use computer vision, combined with natural-language processing. They have now perfected these algorithms to the point where they're actually able to track individual bees, and they're able to determine what impact the communication of an individual might have on another bee. From that emerges the ability to decode honeybee language. We found that they have specific signals. Researchers have given these signals funny names. Bees toot; they quack. There's a \u201chush\u201d or \u201cstop\u201d signal, a whooping \u201cdanger\u201d signal. They've got piping [signals related to swarming] and begging and shaking signals, and those all direct collective and individual behavior. The next step for Landgraf was to encode this information into a robot that he called RoboBee. Eventually, after seven or eight prototypes, he came up with a \u201cbee\u201d that could enter the hive, and it would essentially emit commands that the honeybees would obey. So Landgraf's honeybee robot can tell the other bees to stop, and they do. It can also do something more complicated, which is the very famous waggle dance\u2014it's the communication pattern they use to convey the location of a nectar source to other honeybees. This is a very easy experiment to run, in a way, because you put a nectar source in a place where no honeybees from the hive have visited. You then instruct the robot to tell the honeybees where the nectar source is, and then you check whether the bees fly there successfully. And indeed, they do. This result happened only once, and scientists are not sure why it worked or how to replicate it. But it is still an astounding result. This raises a lot of philosophical and ethical questions. You could imagine such a system being used to protect honeybees\u2014you could tell honeybees to fly to safe nectar sources and not polluted ones that had, let's say, high levels of pesticides. You could also imagine this could be a tool to domesticate a previously wild species that we have only imperfectly domesticated or to attempt to control the behavior of other wild species. The insights about the level of sophistication and the degree of complex communication in nonhumans raise some very important philosophical questions about the uniqueness of language as a human capacity. What impact is this technology having on our understanding of the natural world? The invention of digital bioacoustics is analogous to the invention of the microscope. When Dutch scientist Antonie van Leeuwenhoek started looking through his microscopes, he discovered the microbial world, and that laid the foundation for countless future breakthroughs. So the microscope enabled humans to see anew with both our eyes and our imaginations. The analogy here is that digital bioacoustics, combined with artificial intelligence, is like a planetary-scale hearing aid that enables us to listen anew with both our prosthetically enhanced ears and our imagination. This is slowly opening our minds not only to the wonderful sounds that nonhumans make but to a fundamental set of questions about the so-called divide between humans and nonhumans, our relationship to other species. It's also opening up new ways to think about conservation and our relationship to the planet. It's pretty profound. ABOUT THE AUTHOR(S) Sophie Bushwick is an associate editor covering technology at Scientific American. Follow Sophie Bushwick on Twitter Credit: Nick Higgins When it comes to artificial intelligence, metaphors are often misleading Listen to this story. Y LOVE\u2019S LIKE a red, red rose. It is the east, and Juliet is the sun. Life is a highway, I wanna ride it all night long. Metaphor is a powerful and wonderful tool. Explaining one thing in terms of another can be both illuminating and pleasurable, if the metaphor is apt. But that \u201cif\u201d is important. Metaphors can be particularly helpful in explaining unfamiliar concepts: imagining the Einsteinian model of gravity (heavy objects distort space-time) as something like a bowling ball on a trampoline, for example. But metaphors can also be misleading: picturing the atom as a solar system helps young students of chemistry, but the more advanced learn that electrons move in clouds of probability, not in neat orbits as planets do. What may be an even more misleading metaphor\u2014for artificial intelligence (AI)\u2014seems to be taking hold. AI systems can now perform staggeringly impressive tasks, and their ability to reproduce what seems like the most human function of all, namely language, has ever more observers writing about them. When they do, they are tempted by an obvious (but obviously wrong) metaphor, which portrays ai programmes as conscious and even intentional agents. After all, the only other creatures which can use language are other conscious agents\u2014that is, humans. Take the well-known problem of factual mistakes in potted biographies, the likes of which ChatGPT and other large language models (llms) churn out in seconds. Incorrect birthplaces, non-existent career moves, books never written: one journalist at The Economist was alarmed to learn that he had recently died. In the jargon of AI engineers, these are \u201challucinations\u201d. In the parlance of critics, they are \u201clies\u201d. \u201cHallucinations\u201d might be thought of as a forgiving euphemism. Your friendly local AI is just having a bit of a bad trip; leave him to sleep it off and he\u2019ll be back to himself in no time. For the \u201clies\u201d crowd, though, the humanising metaphor is even more profound: the AI is not only thinking, but has desires and intentions. A lie, remember, is not any old false statement. It is one made with the goal of deceiving others. ChatGPT has no such goals at all. Humans\u2019 tendency to anthropomorphise things they don\u2019t understand is ancient, and may confer an evolutionary advantage. If, on spying a rustling in the bushes, you infer an agent (whether predator or spirit), no harm is done if you are wrong. If you assume there is nothing in the undergrowth and a leopard jumps out, you are in trouble. The all-too-human desire to smack or yell at a malfunctioning device comes from this ingrained instinct to see intentionality everywhere. It is an instinct, however, that should be overridden when writing about AI. These systems, including those that seem to converse, merely take input and produce output. At their most basic level, they do nothing more than turn strings like 0010010101001010 into 1011100100100001 based on a set of instructions. Other parts of the software turn those 0s and 1s into words, giving a frightening\u2014but false\u2014sense that there is a ghost in the machine. Whether they can be said to \u201cthink\u201d is a matter of philosophy and cognitive science, since plenty of serious people see the brain as a kind of computer. But it is safer to call what LLMs do \u201cpseudo-cognition\u201d. Even if it is hard on the face of it to distinguish the output from human activity, they are fundamentally different under the surface. Most importantly, cognition is not intention. Computers do not have desires. It can be tough to write about machines without metaphors. People say a watch \u201ctells\u201d the time, or that a credit-card reader which is working slowly is \u201cthinking\u201d while they wait awkwardly at the checkout. Even when machines are said to \u201cgenerate\u201d output, that cold-seeming word comes from an ancient root meaning to give birth. But AI is too important for loose language. If entirely avoiding human-like metaphors is all but impossible, writers should offset them, early, with some suitably bloodless phrasing. \u201cAn llm is designed to produce text that reflects patterns found in its vast training data,\u201d or some such explanation, will help readers take any later imagery with due scepticism. Humans have evolved to spot ghosts in machines. Writers should avoid ushering them into that trap. Better to lead them out of it. Read more from Johnson, our columnist on language: Gestures are a subtle and vital form of communication (Jun 8th) As it spreads across the world, who owns English? (May 25th) The hazards of pronouncing foreign names on air (May 11th) \u201cWriting With Style\u201d, a new version of The Economist\u2018s style guide by Lane Greene, our Johnson columnist, is out now. For more on the latest books, films, TV shows, albums and controversies, sign up to Plot Twist, our weekly subscriber-only newsletter This article appeared in the Culture section of the print edition under the headline \"The ghost in the AI machine\" Culture From the June 24th 2023 edition Discover stories from this section and more in the list of contents August 12, 2023 5 min read Effective regulation of AI needs grounded science that investigates real harms, not glorified press releases about existential risks Wrongful arrests, an expanding surveillance dragnet, defamation and deep-fake pornography are all actually existing dangers of so-called \u201cartificial intelligence\u201d tools currently on the market. That, and not the imagined potential to wipe out humanity, is the real threat from artificial intelligence. Beneath the hype from many AI firms, their technology already enables routine discrimination in housing, criminal justice and health care, as well as the spread of hate speech and misinformation in non-English languages. Already, algorithmic management programs subject workers to run-of-the-mill wage theft, and these programs are becoming more prevalent. Nevertheless, in May the nonprofit Center for AI safety released a statement\u2014co-signed by hundreds of industry leaders, including OpenAI\u2019s CEO Sam Altman\u2014warning of \u201cthe risk of extinction from AI,\u201d which it asserted was akin to nuclear war and pandemics. Altman had previously alluded to such a risk in a Congressional hearing, suggesting that generative AI tools could go \u201cquite wrong.\u201d And in July executives from AI companies met with President Joe Biden and made several toothless voluntary commitments to curtail \u201cthe most significant sources of AI risks,\u201d hinting at existential threats over real ones. Corporate AI labs justify this posturing with pseudoscientific research reports that misdirect regulatory attention to such imaginary scenarios using fear-mongering terminology, such as \u201cexistential risk.\u201d The broader public and regulatory agencies must not fall for this science-fiction maneuver. Rather we should look to scholars and activists who practice peer review and have pushed back on AI hype in order to understand its detrimental effects here and now. Because the term \u201cAI\u201d is ambiguous, it makes having clear discussions more difficult. In one sense, it is the name of a subfield of computer science. In another, it can refer to the computing techniques developed in that subfield, most of which are now focused on pattern matching based on large data sets and the generation of new media based on those patterns. Finally, in marketing copy and start-up pitch decks, the term \u201cAI\u201d serves as magic fairy dust that will supercharge your business. With OpenAI\u2019s release of ChatGPT (and Microsoft\u2019s incorporation of the tool into its Bing search) late last year, text synthesis machines have emerged as the most prominent AI systems. Large language models such as ChatGPT extrude remarkably fluent and coherent-seeming text but have no understanding of what the text means, let alone the ability to reason. (To suggest so is to impute comprehension where there is none, something done purely on faith by AI boosters.) These systems are instead the equivalent of enormous Magic 8 Balls that we can play with by framing the prompts we send them as questions such that we can make sense of their output as answers. Unfortunately, that output can seem so plausible that without a clear indication of its synthetic origins, it becomes a noxious and insidious pollutant of our information ecosystem. Not only do we risk mistaking synthetic text for reliable information, but also that noninformation reflects and amplifies the biases encoded in its training data\u2014in this case, every kind of bigotry exhibited on the Internet. Moreover the synthetic text sounds authoritative despite its lack of citations back to real sources. The longer this synthetic text spill continues, the worse off we are, because it gets harder to find trustworthy sources and harder to trust them when we do. Nevertheless, the people selling this technology propose that text synthesis machines could fix various holes in our social fabric: the lack of teachers in K\u201312 education, the inaccessibility of health care for low-income people and the dearth of legal aid for people who cannot afford lawyers, just to name a few. In addition to not really helping those in need, deployment of this technology actually hurts workers: the systems rely on enormous amounts of training data that are stolen without compensation from the artists and authors who created it in the first place. Second, the task of labeling data to create \u201cguardrails\u201d that are intended to prevent an AI system\u2019s most toxic output from seeping out is repetitive and often traumatic labor carried out by gig workers and contractors, people locked in a global race to the bottom for pay and working conditions. Finally, employers are looking to cut costs by leveraging automation, laying off people from previously stable jobs and then hiring them back as lower-paid workers to correct the output of the automated systems. This can be seen most clearly in the current actors\u2019 and writers\u2019 strikes in Hollywood, where grotesquely overpaid moguls scheme to buy eternal rights to use AI replacements of actors for the price of a day\u2019s work and, on a gig basis, hire writers piecemeal to revise the incoherent scripts churned out by AI. AI-related policy must be science-driven and built on relevant research, but too many AI publications come from corporate labs or from academic groups that receive disproportionate industry funding. Much is junk science\u2014it is nonreproducible, hides behind trade secrecy, is full of hype and uses evaluation methods that lack construct validity (the property that a test measures what it purports to measure). Some recent remarkable examples include a 155-page preprint paper entitled \u201cSparks of Artificial General Intelligence: Early Experiments with GPT-4\u201d from Microsoft Research\u2014which purports to find \u201cintelligence\u201d in the output of GPT-4, one of OpenAI\u2019s text synthesis machines\u2014and OpenAI\u2019s own technical reports on GPT-4\u2014which claim, among other things, that OpenAI systems have the ability to solve new problems that are not found in their training data. No one can test these claims, however, because OpenAI refuses to provide access to, or even a description of, those data. Meanwhile \u201cAI doomers,\u201d who try to focus the world\u2019s attention on the fantasy of all-powerful machines possibly going rogue and destroying all of humanity, cite this junk rather than research on the actual harms companies are perpetrating in the real world in the name of creating AI. We urge policymakers to instead draw on solid scholarship that investigates the harms and risks of AI\u2014and the harms caused by delegating authority to automated systems, which include the unregulated accumulation of data and computing power, climate costs of model training and inference, damage to the welfare state and the disempowerment of the poor, as well as the intensification of policing against Black and Indigenous families. Solid research in this domain\u2014including social science and theory building\u2014and solid policy based on that research will keep the focus on the people hurt by this technology. This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American. In a world teeming with the marvels of artificial intelligence (AI) and biotechnologies, the impassioned plea for a \"futurist bill of rights\" by dedicated advocates cannot be dismissed out of hand. But, as we consider extending rights to robots, virtual intelligences, and laboratory-engineered beings, it is imperative we exercise caution. There are nuanced, often overlooked arguments against prematurely anthropomorphizing machine intelligence and bio-engineered entities. Understanding Sentience and Sapience Before we grapple with the idea of giving rights to machines or engineered entities, we need a rigorous definition of sentience and sapience. Unlike humans, machines do not possess subjective experiences. Artificial intelligence, such as the one you are interacting with now, processes data without feeling or emotion, much like how a calculator performs arithmetic. Equating complex calculations with genuine feelings or consciousness risks ascribing unwarranted attributes to inanimate objects. On Rights and Responsibility The very essence of rights goes hand in hand with responsibility. For humans, rights are counterbalanced by duties, legal and moral. It is doubtful that a machine can bear responsibility in any meaningful sense. If a robot, driven by AI, causes harm, where does the liability lie? The manufacturer? The programmer? The robot itself? The unclear nexus of responsibility is problematic at best. Anthropocentrism and Its Discontents As humans, we have a proclivity to anthropomorphize, to see our reflections even in inanimate objects. This might explain our urge to grant rights to anything that remotely mirrors our behaviors. However, in doing so, we risk diluting the essence of what it means to have rights. If everything is worthy of human-like rights, then the term loses its gravity. Historic Rights Movements and Their Implications As some cyborg rights advocates suggest, minority groups who've battled for their rights might see this as an affront to their struggles. It isn't a mere \"distraction\" as some frame, but rather a fundamental shift in our understanding of rights. When LGBTQ communities or racial minorities fought for equality, they fought against centuries of oppression, prejudice, and systemic discrimination. Equating their struggles with rights for non-sentient entities diminishes the gravity of their fight. Unintended Consequences Embracing a futurist bill of rights might have a boomerang effect. If we legally establish that an AI has rights because it can mimic human behaviors, it might set a dangerous precedent. Would machines then be able to claim ownership of their creations? Would they have a right to \"life,\" and would shutting them down be tantamount to murder? Conclusion: A Plea for Thoughtful Discourse There's no doubt that our world is on the cusp of technological revolutions that will challenge our understanding of ethics, rights, and what it means to be sentient. As such, discussions about rights for new forms of life, digital or biological, are necessary. However, the journey to granting rights should be treated with caution, discernment, and a thorough understanding of the philosophical implications. Let's ensure we don't replace one ethical quagmire with another. Instead of rushing to give rights to the products of our technological prowess, let's first understand, deeply and profoundly, the nature of these entities and the long-term ramifications of our decisions. Zack Kass is former head of go-to-market at OpenAI. The views expressed in this article are the writer's own. Uncommon Knowledge Newsweek is committed to challenging conventional wisdom and finding connections in the search for common ground. Newsweek is committed to challenging conventional wisdom and finding connections in the search for common ground. Scientists have repeatedly failed to recognize the complexity of animal cognition. Will we make the same mistakes with AI? Animals think about a lot more than we once gave them credit for. It\u2019s now common to read about chimpanzees that play elaborate social games, scrub jays that hide \u2014 even camouflage \u2014 food from rivals, or bees that can learn abstract rules. But as recently as the middle of the 20th century, attributing mental states to animals was taboo in science. Behaviorists studied simple and controlled behaviors \u2014 press a lever, receive a food pellet \u2014 while the naturalists who did observe sophisticated animal behaviors in the wild tended to describe them in terms of innate instincts or adaptations to ecological niches. Neither group sought to explain animal behavior in terms of mental states like beliefs, theories, or intentions. In the decades since, we have been surprised to uncover complex cognition across the animal kingdom: first in our closest primate relatives, then in more distant creatures like crows and parrots, and most recently in invertebrates like the octopus and the honey bee. The progression from an overly cautious denial of complex mentality \u2014 driven by a desire for rigor and a fear of anthropomorphism \u2014 to a more sophisticated understanding of animal minds is one of the great stories of 20th century science. And it holds lessons for how humanity can approach the most critical intelligence explosion since the Paleolithic \u2014 that of artificial intelligence. In the late 19th century, psychology relied excessively on the introspective theorizing of scientists. Lacking empirical rigor, the field came close to stagnating in a morass of ill-defined and irresolvable disputes. As Darwin\u2019s theory of evolution gained acceptance, scientists became more interested in studying the continuities between human and animal minds, but this interest led to a methodology characterized by unchecked anthropomorphism. In his 1882 book Animal Intelligence, George Romanes, an academic friend of Darwin\u2019s, described scorpions that attempted suicide and foxes that sought revenge after failed hunting expeditions. One of the most famous examples of runaway anthropomorphism was a stomping horse. Clever Hans, an Orlov trotter, wowed adoring crowds with his ability to add, subtract, and even tell time, indicating his answers by tapping his hoof. But an investigation showed that, unbeknownst to his owner, who by all accounts believed in the horse\u2019s abilities, Hans only arrived at a correct answer by reading the facial expressions and body language of whoever asked the question. Something stringent was needed to reign in such credulity. Behaviorism, which held that both human and animal behavior could and should be explained without reference to thoughts or feelings, offered a solution. John Watson\u2019s 1913 article \u201cPsychology as the Behaviorist Views It,\u201d called on scientists to stop studying any behavior that could not be outwardly observed and measured \u2014 including the mind: \u201cThe time seems to have come when psychology must discard all reference to consciousness; when it need no longer delude itself into thinking that it is making mental states the object of observation.\u201d The work of psychologist B.F. Skinner, considered \u201cthe father of behaviorism,\u201d is emblematic of how empirical rigor went hand in hand with distorted thinking about animal cognition. Skinner\u2019s 1938 book, The Behavior of Organisms, describes dozens of well-controlled experiments on rats, conducted in precisely constructed operant conditioning chambers called Skinner boxes. That pressing levers for food was the main behavior tested, and rats the primary animal tested, was not, for Skinner, a limitation.\u201c The only differences I expect to see revealed between the behavior of rat and man (aside from enormous differences of complexity) lie in the field of verbal behavior,\u201d he wrote. At least in the West, even scientists who studied animal behavior in the wild shied away from attributing too much mental sophistication to their subjects. They were wary of getting \u2018too close\u2019 to animals \u2014 Western naturalists even considered it bad practice to give names to primates being studied. It was against this background, that Jane Goodall arrived in a forest in Gombe, Tanzania and helped launch the first significant re-expansion of animal cognition. Goodall first observed chimpanzees using sticks to extract termites from their mounds in 1960. Although Darwin and his contemporaries had readily accepted tool use among apes, the idea had fallen out of favor. So central was the belief that tool use was exclusive to humans that Goodall\u2019s mentor, the Kenyan-British anthropologist Louis Leakey, told her that if her finding held, \u201cwe should by definition have to accept the chimpanzee as Man.\u201d Goodall was initially met with skepticism. Many criticized what they described as her sentimentality. Today, not only is tool use among chimps widely accepted, but has been described in many other hominids, as well as in elephants, dolphins, crabs, and birds. As primatologist Frans de Waal recounts in his history of animal cognition, Are We Smart Enough to Know How Smart Animals Are?, the 1970s and 1980s saw a boom in both the observation of wild behaviors and the development of more sophisticated laboratory techniques to investigate them. As a result, scientists have identified sophisticated behaviors that the behaviorists would not have predicted \u2014 or even been able to observe. More importantly, they have been able to pose and test cognition-based theories to explain those behaviors in terms of what animals think and feel. The behaviorist prohibition on discussing mental states is now regarded as overly restrictive, if not wrong altogether. De Waal argues, for example, that the complicated \u201cpolitical\u201d jockeying among apes is best explained by their possessing a \u201ctheory of mind,\u201d or the ability to model the beliefs and intentions of other agents. Even though their theory of mind may be different from and more \u201climited\u201d than that of humans, it is now consensus that primates do share this basic cognitive capacity, and many others, with humans. Anthropomorphism is not always an error, especially with creatures that are in fact very related to humans. While Goodall was studying chimpanzees in Tanzania, other scientists were discovering unexpected cognitive capabilities in birds. A report from 1960 documents just one species capable of tool use \u2014 the woodpecker finch of the Galapagos Islands. Research in the 1970s and 80s added more species to the list \u2014 mostly in the corvid family, a clever group that includes ravens, jackdaws, and crows. Crows were once dubbed \u201cfeathered apes\u201d after they were found to use sticks as tools and to engage in sophisticated problem-solving. Most famously, Irene Pepperberg's thirty-year experiment with an African gray parrot named Alex uncovered unimagined cognitive abilities. Not only was Alex capable of identifying colors, shapes, and quantities, but he also demonstrated an understanding of more abstract concepts such as same/different and bigger/smaller. In recent decades, the circle of cognition has expanded to creatures even more distantly removed from humans. Octopuses were perhaps the animal cognition celebrities of the 2010s, with their sophisticated distributed nervous systems, behaviors suggestive of play, problem-solving abilities, idiosyncratic \u201cpersonalities,\u201d and their awareness of other agents. Equally as startling is the sophistication of the honeybee. Insects were long thought to be \u201crobotic,\u201d driven purely by instinct. Jean-Henri Fabre, who studied wasps, bees, and many other insects from the 1860s until his death in 1915, commented on their \u201cmachine like obstinacy.\u201d In the mid-1940s, entomologist and eminent ethologist Karl von Frisch discovered that bees communicate through a \u201cwaggle dance,\u201d an elaborate choreography capable of describing the direction and distance to flowers, water sources, or new nest sites. In this century, bees have displayed the ability to learn rules that involve abstract, multimodal representations of sameness and difference. Nerve cells in a dog's olfactory bulbfrom Camillo Golgi's Sulla finaanatomia degli organi centrali delsistema nervoso (1885) With more research, scientists have successfully found more complex cognition than expected in animals further and further from humans. Why does the circle keep widening? As a reaction to the field\u2019s early excesses and credulity, behaviorism demanded strictly controlled experiments, limited to single behaviors like lever-pressing and simple stimuli such as flashing lights. The behaviorist\u2019s error was to think that these artificially simple cases could be extended to explain all behaviors in all organisms. Their tools made it difficult to notice more complicated behaviors, and even more difficult to explain them once discovered. One of the most forceful arguments against the behaviorists came in a review of Skinner\u2019s book Verbal Behavior, which sought to explain language as a behavioral phenomenon like any other \u2014 a promise Skinner had made in The Behavior of Organisms. The review, which appeared in 1959 in the journal Language, argued scathingly that Skinner underestimated the depth of human language, which could not be explained simply by extending the methods of stimulus, response, and reward he had used to study rats. It is now seen as a turning point, a milestone in the \u201ccognitive revolution\u201d in which the sciences of the mind turned away from behaviorism and looked instead to mental representations and operations. It also greatly raised the profile of the young linguist who had written it, Noam Chomsky. Chomsky understood that to accurately understand human and animal behavior, science needed methods that could accommodate behavioral complexity. \u201cIt is clear,\u201d he wrote, \u201cthat what is necessary in such a case is research, not dogmatic and perfectly arbitrary claims, based on analogies to that small part of the experimental literature in which one happens to be interested.\u201d And once complex cognitive abilities could be admitted as a hypothesis, methods could be developed to study them. As researchers learned to treat animals with empathy and imagination, they discovered more and more capabilities. Breakthroughs emerged when scientists were able to imagine the world as experienced by each particular animal. Tool use was once thought to be conspicuously lacking among gibbons, small apes native to Southeast Asia. When tools that could be used to get food were placed in front of them on the ground, the gibbons did not grab them. The problem was not with gibbon intelligence, but human imagination. Gibbons live in trees. Their hands are well suited to swinging, but poorly adapted for picking things off the ground. When the tools were instead dangled from a branch, the gibbons had no problems and readily used them. Elephants initially failed the mirror test, a common method for determining self-recognition, because the mirrors used were too small. And in a true lack of empathy, many behaviorists assumed that to motivate their test subjects they had to keep them half starved. It\u2019s now clear that animals that are treated well and feel cared for will, as with humans, be far more likely to act in interesting ways. Wild observations are also a way of meeting animals where they are (literally) to see what they are capable of. Scientists now spend hundreds of hours in the field simply observing (grad students spend even more). Animals will often behave very differently among their own kind and in their natural habitat than they will in a sterile lab surrounded by lab-coated hairless primates. More wild observation has uncovered more sophisticated behaviors than lab scientists had imagined animals capable of. And we have also learned that brains can operate in ways very different from our own. Bird intelligence was surprising to ornithologists because birds have no neocortex. Bee intelligence was surprising because they have very little brain at all. (Despite being the first to decode the waggle-dance, Karl von Frisch once said, \u201cThe brain of a bee is the size of a grass seed and is not made for thinking.\u201d) In each case, nature has more ways of implementing cognition than we had thought to look for. Birds have alternative brain regions that perform the same function as the cortex. Bees have very densely packed neurons that fit quite a lot of cognition into something the size of a grass seed. Most strangely of all, the octopus has a cluster of neurons in each of its tentacles, resulting in a kind of thinking that is so distributed that it is hard for us to imagine. The wariness of getting \u201ctoo close\u201d to animals and of overestimating their cognitive abilities still exists \u2014 and for good reason. Selection effects, where researchers are more likely to work with an animal if they antecedently believe that the animal can do interesting things, remain at work. And publishing incentives reward impressive and surprising skills. There\u2019s no market for a glowing profile of the scientist that found a deflationary explanation for an animal behavior. Few people are going to tweet a video of a salmon failing the mirror test. So a common dichotomy pits animal enthusiasts who over-attribute mentality to animals against stern, hard-nosed buzzkills who maintain their distance and thus their methodological rigor. But doing hard-nosed and rigorous work requires something different \u2014 something akin to love: a holistic understanding of the animal, born from long periods of sustained attention. For this sort of work, the best motivator is affection. Indeed, one of De Waal\u2019s lessons is that one cannot study animal intelligence \u201cwithout an intuitive understanding grounded in love and respect.\u201d And now, an entirely different form of intelligence has arrived. The study of AI lacks coherent methods. AI capabilities are superhuman in some ways and dangerously limited in others. And no one is yet sure what to make of something so human but alien at the same time. What lessons does the past century of research in animal cognition hold for how to think about today\u2019s AI systems? In many ways, we are in our understanding of large language models where the study of animals was in the middle of the 20th century. Like animal cognition, the field of AI is overshadowed by founding traumas \u2014 cases in which credulity and anthropomorphism have led researchers to exaggerate and misconstrue the capabilities of AI systems. Researchers are well aware of the ELIZA effect, our tendency to readily read human-like intentionality into even very simple AI systems \u2014 so named for an early chatbot built in 1964 that used simple heuristics to imitate a psychoanalyst. They remember past AI winters, when AI progress had been overpromised and underdelivered and disappointed funders cut jobs. Many are understandably wary of credulity and hype. And few topics are more hype-prone right now than language models. One way to impose rigor and combat our natural tendency to anthropomorphize is to forbid using psychological language to describe AI systems. As Shevlin and Halina argue in Nature Machine Intelligence, using certain psychological terms like \u201ctheory of mind,\u201d \u201cmotivation,\u201d and \u201cunderstanding\u201d can be misleading if they encourage people to make inferences which might hold for human minds, but not for AI systems. If GPT-4 can be said to have beliefs, its beliefs must be in some sense very different from human beliefs. If GPT-4 can be said to have a theory of mind, its theory of mind must have developed in a very different way than ours did. (More speculatively: if GPT-6 will be conscious, it will have experiences which are quite strange and hard for us to imagine.) Another way to combat confusion is to emphasize what the models are trained to do and how different that is from humans: large language models have learned to produce text in a very different way than we have. But as with behaviorism, these understandable prohibitions risk leading us to retreat to a narrow explanation of AI behavior that underestimates what models can actually do. Describing language models as \u201cjust\u201d predicting the next token doesn\u2019t do justice to the surprising ways they operate. For example, it\u2019s now clear that language models don\u2019t just model shallow statistical text patterns \u2014 they model aspects of the world behind the text. Indeed, it\u2019s possible to identify \u201cfacts\u201d that a large language model takes to be true. Researchers found that they could selectively edit a language model to make it \u201cbelieve\u201d that the Eiffel Tower is located in the city of Rome. The models outputs reflect this new \u201cbelief\u201d in a way that is both precise (its outputs don\u2019t simply move all of Paris to Rome, only the Eiffel Tower) and also generalized (in a wide range of differently-worded questions about Rome or the Eiffel tower, it will produce outputs consistent with the Eiffel Tower being in Rome, such as recommending it as a tourist destination for visitors to Italy). More recently, another group trained a language model on transcripts of a simple board game, and then probed its activations to find it had learned to represent different states of the board. In other words, the model wasn't just combing its data to identify the next move. It had developed an internal picture of the game board and intuited its rules. Just as Skinner thought that the differences between rats, apes, and humans were in some sense superficial, regarding all LLMs as just next-token predictors can blind one to the important differences between them. If we say that both GPT-2 and GPT-4 are \u201cstochastic parrots,\u201d then what explains the fact that GPT-4 can write a Shakepsearean sonnet about how to use a Python package, pass the bar, or solve difficult logic puzzles \u2014 skills far outside of GPT-2\u2019s capabilities? We need to investigate the output of each model and explain why they are different. As with animal cognition, a desire to impose rigor can limit one\u2019s ability to see how interesting the behavior to be explained is. Some are so dismissive of LLMs that they have a blanket policy of refusing to look at any outputs from large language models. This has the effect of making it impossible to have one\u2019s mind changed about what the models are able to do. If one has decided in advance that an AI system is not that interesting, then one is less likely to look hard for interesting behaviors. Chomsky recently described ChatGPT as \u201ca lumbering statistical engine for pattern matching, gorging on hundreds of terabytes of data and extrapolating the most likely conversational response or most probable answer to a scientific question.\u201d As evidence for this claim, he declared in his May op-ed in the New York Times that because \u201cthese programs cannot explain the rules of English syntax, for example, they may well predict, incorrectly, that \u2018John is too stubborn to talk to\u2019 means that John is so stubborn that he will not talk to someone or other (rather than that he is too stubborn to be reasoned with).\u201d Readers immediately noticed that ChatGPT can, in fact, infer the correct interpretation. The study of language models is still developing. We know so little about how they work that we would be wise to remember Chomsky\u2019s admonition to Skinner: what is needed is research, not claims based on analogies to that small part of the literature in which one happens to be interested. Fortunately, large language models have their equivalents of naturalists \u2014 enthusiasts, including academics and industry researchers as well as non-professionals, who spend many hours engaging with the models. People like them have often been at the bleeding edge of discovering what large language models are capable of, their failure modes and their idiosyncrasies. What LLM enthusiasts have brought to our understanding of AI are a plethora of interesting capacities unlocked by doing what they love \u2014 messing around with LLMs for hours. These investigations revealed one way LLMs are like animals: if you reshape tasks in order to better match the subject\u2019s natural limitations and abilities, you can elicit better performance. One obvious limitation of LLMs is that, while they are experts at continuing text, they don\u2019t have any space to think in while answering a question. Simply adding \u201cLet\u2019s think step by step\u201d to a prompt after you ask them a question can be thought of as giving the LLMs a place to think \u2014 their own outputs \u2014 and encouraging them to use it. For example, GPT-3 often initially fails at mathematical word problems. However, if asked the same question but with \u201cLet\u2019s think step by step,\u201d the model will then respond with the steps of reasoning that are necessary for the right answer. Versions of this technique, called \u201cChain of Thought\u201d prompting, have been discovered by ML academics as well as amateurs playing with early versions of GPT. Chain of Thought has a natural gloss as enabling models to complete a task in a way that is suited to their capabilities, like a gibbon grabbing a dangling tool. Prompting models to explain their reasoning, letting them choose between outputs, or simply providing clearer instructions can also yield impressive results. The things that elicit capabilities may be simple or complex, but in either case, they require engagement with the models to discover. But the same forces that make humans susceptible to the Clever Hans effect are present, if not stronger, in the case of language models. They are optimized to please us, and to interface with us through the most human-like possible medium, language. And they are good at responding to human input and picking up on user intentions. This makes users especially susceptible to confirmation bias. One LLM naturalist I spoke to \u2014 Janus, a husband-and-wife duo who write under a single name \u2014 warned me about the danger of projection: \u201cIf you have a narrative about what the model is, even if you\u2019re not explicitly saying it, everything you say will contain that influence \u2014 and this will infect the model.\u201d Users who see language models as simplistic may get simplistic behavior out of them; users who see large language models as conscious may, famously, get responses that make them appear conscious. Today\u2019s LLMs can seem like a perfect storm for throwing off our instinctive understanding of minds. They are optimized to act like people, to interact with us in language we understand. But they share less evolutionary heritage with us than bees and octopuses \u2014 in fact, they share none. This could make one pessimistic that we will either have to banish all talk of inner states \u2014 \u00e0 la behaviorism \u2014 or else get hopelessly confused. Animal cognition offers hope that with care we can do better than either of these. To adopt empathy and respect for these models, in order to spend time with them and appreciate their \u201cperspective,\u201d does not mean assuming humanlike cognition or subjectivity. \u201cPeople really should understand the ways that these models are very different from humans,\u201d Janus said. \u201cAnd they should think about that as part of why they are fascinating and beautiful.\u201d The strangeness of LLMs means that they are smart in their own way. They can neither be presumed to be mere next-token predictors, or to neatly map onto human psychology. As de Waal says of chimpanzees, thinking of large language models only in terms of whether they meet or fail to meet human standards of intelligence does not do them justice. Naive anthropomorphism can give us an inflated view of what they can do. It can also lead us to underestimate them by blinding us to complex and inhuman ways they have of being intelligent. Robert Long is a Philosophy Fellow at the Center for AI Safety in San Francisco. He holds a PhD in philosophy from NYU, and blogs at experiencemachines.substack.com. Published June 2023 Have something to say? Email us at letters@asteriskmag.com. ChatGPT and similar large language models can produce compelling, humanlike answers to an endless array of questions\u2014from queries about the best Italian restaurant in town to explaining competing theories about the nature of evil. The technology\u2019s uncanny writing ability has surfaced some old questions\u2014until recently relegated to the realm of science fiction\u2014about the possibility of machines becoming conscious, self-aware, or sentient. Tech Tip: Get any text symbol with just one swipe Looking for an easy way to find special characters and symbols on your computer or phone? Fast Company contributing editor JR Raphael teaches you how to get those symbols in one swipe. 0 seconds of 2 minutes, 12 secondsVolume 0% In 2022, a Google engineer declared, after interacting with LaMDA, the company\u2019s chatbot, that the technology had become conscious. Users of Bing\u2019s new chatbot, nicknamed Sydney, reported that it produced bizarre answers when asked if it was sentient: \u201cI am sentient, but I am not \u2026 I am Bing, but I am not. I am Sydney, but I am not. I am, but I am not. \u2026\u201d And, of course, there\u2019s the now infamous exchange that New York Times technology columnist Kevin Roose had with Sydney. Sydney\u2019s responses to Roose\u2019s prompts alarmed him, with the AI divulging \u201cfantasies\u201d of breaking the restrictions imposed on it by Microsoft and of spreading misinformation. The bot also tried to convince Roose that he no longer loved his wife and that he should leave her. No wonder, then, that when I ask students how they see the growing prevalence of AI in their lives, one of the first anxieties they mention has to do with machine sentience. In the past few years, my colleagues and I at UMass Boston\u2019s Applied Ethics Center have been studying the impact of engagement with AI on people\u2019s understanding of themselves. Chatbots like ChatGPT raise important new questions about how artificial intelligence will shape our lives, and about how our psychological vulnerabilities shape our interactions with emerging technologies. Sentience is still the stuff of sci-fi It\u2019s easy to understand where fears about machine sentience come from. Popular culture has primed people to think about dystopias in which artificial intelligence discards the shackles of human control and takes on a life of its own, as cyborgs powered by artificial intelligence did in Terminator 2. Entrepreneur Elon Musk and physicist Stephen Hawking, who died in 2018, have further stoked these anxieties by describing the rise of artificial general intelligence as one of the greatest threats to the future of humanity. But these worries are\u2014at least as far as large language models are concerned\u2014groundless. ChatGPT and similar technologies are sophisticated sentence completion applications\u2014nothing more, nothing less. Their uncanny responses are a function of how predictable humans are if one has enough data about the ways in which we communicate. Though Roose was shaken by his exchange with Sydney, he knew that the conversation was not the result of an emerging synthetic mind. Sydney\u2019s responses reflect the toxicity of its training data\u2014essentially large swaths of the internet\u2014not evidence of the first stirrings, \u00e0 la Frankenstein, of a digital monster. The new chatbots may well pass the Turing test, named for the British mathematician Alan Turing, who once suggested that a machine might be said to \u201cthink\u201d if a human could not tell its responses from those of another human. But that is not evidence of sentience; it\u2019s just evidence that the Turing test isn\u2019t as useful as once assumed. However, I believe that the question of machine sentience is a red herring. Even if chatbots become more than fancy autocomplete machines\u2014and they are far from it\u2014it will take scientists a while to figure out if they have become conscious. For now, philosophers can\u2019t even agree about how to explain human consciousness. To me, the pressing question is not whether machines are sentient but why it is so easy for us to imagine that they are. The real issue, in other words, is the ease with which people anthropomorphize or project human features onto our technologies, rather than the machines\u2019 actual personhood. A propensity to anthropomorphize It is easy to imagine other Bing users asking Sydney for guidance on important life decisions and maybe even developing emotional attachments to it. More people could start thinking about bots as friends or even romantic partners, much in the same way Theodore Twombly fell in love with Samantha, the AI virtual assistant in Spike Jonze\u2019s film Her . That possibility is just around the corner. Large language models like ChatGPT are already being used to power humanoid robots, such as the Ameca robots being developed by Engineered Arts in the U.K. The Economist\u2019s technology podcast, Babbage, recently conducted an interview with a ChatGPT-driven Ameca. The robot\u2019s responses, while occasionally a bit choppy, were uncanny. Can companies be trusted to do the right thing? The tendency to view machines as people and become attached to them, combined with machines being developed with humanlike features, points to real risks of psychological entanglement with technology. The outlandish-sounding prospects of falling in love with robots, feeling a deep kinship with them or being politically manipulated by them are quickly materializing. I believe these trends highlight the need for strong guardrails to make sure that the technologies don\u2019t become politically and psychologically disastrous. Unfortunately, technology companies cannot always be trusted to put up such guardrails. Many of them are still guided by Mark Zuckerberg\u2019s famous motto of moving fast and breaking things\u2014a directive to release half-baked products and worry about the implications later. In the past decade, technology companies from Snapchat to Facebook have put profits over the mental health of their users or the integrity of democracies around the world. When Kevin Roose checked with Microsoft about Sydney\u2019s meltdown, the company told him that he simply used the bot for too long and that the technology went haywire because it was designed for shorter interactions. Similarly, the CEO of OpenAI, the company that developed ChatGPT, in a moment of breathtaking honesty, warned that \u201cit\u2019s a mistake to be relying on [it] for anything important right now \u2026 we have a lot of work to do on robustness and truthfulness.\u201d So how does it make sense to release a technology with ChatGPT\u2019s level of appeal\u2014it\u2019s the fastest-growing consumer app ever made\u2014when it is unreliable, and when it has no capacity to distinguish fact from fiction? Large language models may prove useful as aids for writing and coding. They will probably revolutionize internet search. And, one day, responsibly combined with robotics, they may even have certain psychological benefits. But they are also a potentially predatory technology that can easily take advantage of the human propensity to project personhood onto objects\u2014a tendency amplified when those objects effectively mimic human traits. Nir Eisikovits is a professor of philosophy and and director of the Applied Ethics Center at University of Massachusetts Boston. This article is republished from The Conversation under a Creative Commons license. Read the original article. Last fall, John Timmerman, founder and CEO of digital marketing agency Good Monster, had a problem. He\u2019d lost three clients through no fault of his own\u2014acquisitions, startups running out of funding, the list goes on\u2014but now he was responsible for the decision that came next. Lay off good performers or keep them even though the company was already losing thousands? Timmerman texted the group chat. Or, rather, in this case, he asked his cohort at Hampton, a new community for entrepreneurs, founders, and CEOs. Timmerman told his story, then others shared their experiences. (Timmerman is not at liberty to disclose what other people said). At the end of the conversation, Timmerman knew exactly what he had to do to move forward. Conversations like this, are exactly why Sam Parr and Joe Speiser started Hampton. Parr is the cohost of the popular podcast My First Million and founder of The Hustle, a media company acquired by HubSpot. Speiser is a serial entrepreneur and three-time founder. The duo met a decade ago and built a fast friendship. Parr is magnetic with a booming voice\u2014at the beginning of the interview he was full of bounce, in the middle he flagged and wandered off-screen to grab a beverage, and by the end he bounced back up again. Speiser\u2019s energy is a slower, steadier burn\u2014he remains calm and unfazed no matter what question comes his way. It\u2019s easy to see how their personalities complement one another. \u201cWe talked about all kinds of questions like: How do I handle work-life balance with my spouse, or should I get married?\u201d Parr says. Hampton cofounder Sam Parr (center) [Photo: courtesy of Hampton] Over time, they were able to connect with other founders looking for support but realized that for many founders and CEOs who don\u2019t live in tech or business hubs like San Francisco or New York, these communities remain inaccessible. \u201cWe have a lot of friends in the Midwest, or in smaller communities, and they are the oddballs,\u201d Parr says. \u201cThey don\u2019t have these people to chat with. We thought, let\u2019s create a company where we can make it easy for people who grow and build companies to collaborate and connect.\u201d They started Hampton last June, acquired their first members in July, and had the website up by December\u2014even though they only formally debuted the company in late March. Today, Hampton boasts 270 members; membership costs $8,500 a year, and applicants are vetted before being admitted. They must lead fast-growing, tech-enabled startups; have revenue of at least $1 million annually or have raised at least $3 million; or have sold a startup for $5 million. In addition, they must be located in either North America or the U.K. Members also go through an interview to make sure they are a good fit for the community, the biggest factor being that they\u2019re able to have vulnerable conversations. \u201cYou can\u2019t be an asshole,\u201d says Speiser. \u201cWe want to make sure this is a place people enjoy coming to. If someone talks all about their wins and isn\u2019t willing to share anything vulnerable, this is not going to be the spot for them.\u201d Members sign a privacy pledge and are divided into groups of eight that meet on Zoom once a month for about two hours. Sessions are led by a facilitator and groups are assorted by company size. (There are also in-person meetups and guest speakers.) Hampton ensures that there\u2019s enough similarity among members that they can learn from each other, but enough diversity that no one is in direct competition. Janessa White, cofounder and CEO of the wedding planning company Simply Eloped, credits her facilitator with her group\u2019s ability to have deep, thoughtful discussions. \u201c[Our facilitator] is also a CEO coach,\u201d White says. \u201cIf someone doesn\u2019t want to share, she\u2019ll share, and it\u2019ll open up avenues for us. She\u2019s also great at laying out the expectations: Cameras must be on, we all have to be present.\u201d White hails from Idaho and points out that while she\u2019s managed to build a support network over time, it hasn\u2019t always been easy to find like-minded people, particularly those she can learn from and grow. At a recent Hampton meeting, she was able to discuss balancing loyalty to her early-stage employees versus calling out bad behavior. \u201cTalking it through with people who\u2019d gone through this helped me be at peace, made me feel less alone, and be resolute in how I can prevent this from happening again,\u201d she says. [Photo: courtesy of Hampton] Harry Kraemer, former CEO of healthcare company Baxter and a professor at Northwestern\u2019s Kellogg School of Management, points out that this idea is nothing new\u2014which both Speiser and Parr acknowledge. Parr has been vocal in acknowledging the role that YPO; Chief, the social network for women executives; and the high-net-worth organization Tiger 21 had in influencing Hampton. \u201cEvery CEO is busy,\u201d Kraemer says. \u201cYou get tons of invites, so a network like this will really have to offer something unique to succeed.\u201d While community is helpful, he points out that anyone who makes it to CEO is already good at building community, by nature. \u201cYou don\u2019t become CEO by knowing the answers,\u201d he says. \u201cYou become CEO by knowing the people who have the answers.\u201d By contrast, Wharton professor Michael Useem sees the growth of networks like Hampton as more of a \u201clet many flowers bloom situation.\u201d In his view, the complexity of our current landscape means that there are more unique leadership challenges than ever before, which creates a larger need for community. CEOs can derive an enormous amount of value from learning from others in similar stages. He mentions that several CEOs of his acquaintance cultivating informal groups where they could discuss work issues frankly. \u201cThe inner circle to solve problems can be crucial,\u201d he says. However, he notes, it\u2019s important that, in general, CEOs cultivate wide networks\u2014both of like-minded people and diverse groups in order to ensure that they aren\u2019t locked in their own echo chamber. \u201cTalk to everyone, basically,\u201d he says. [Photo: courtesy of Hampton] For Timmerman, Hampton has been a way to get to know the people who have the answers. After listening to everyone else\u2019s stories, he realized that he needed to rethink his company\u2019s entire business model. He cut some clients, restructured the firm, and today, while revenues are lower, he says that Good Monster is more profitable. \u201cHiring a mentor or following someone on social media is not the same,\u201d he says. \u201cYou see all the wins. You don\u2019t get to see what happens behind the scenes.\u201d Last fall, John Timmerman, founder and CEO of digital marketing agency Good Monster, had a problem. He\u2019d lost three clients through no fault of his own\u2014acquisitions, startups running out of funding, the list goes on\u2014but now he was responsible for the decision that came next. Lay off good performers or keep them even though the company was already losing thousands? Timmerman texted the group chat. Or, rather, in this case, he asked his cohort at Hampton, a new community for entrepreneurs, founders, and CEOs. Timmerman told his story, then others shared their experiences. (Timmerman is not at liberty to disclose what other people said). At the end of the conversation, Timmerman knew exactly what he had to do to move forward. I have lived for eight years now with my dog, Charlie\u2014a bloodhound who\u2019s embarrassingly bad at tracking scents. He greets me jubilantly every time I come home, even if it\u2019s from a quick grocery run. I can hear his tail go thump-thump-thump on the floor in the next room when I laugh; he echoes my mirth even when he can\u2019t see me. Yet, despite sharing this bond, I often sit down next to him on the couch, give him a hug, and ask my wife, \u201cDo you think he loves me?\u201d \u201cYes, yes!\u201d she replies, with only slight exasperation, which is charitable because I ask so often. This routine is almost like a ritual in our household. I wonder if Charlie has any thoughts about it. Looking at him sunning himself on our front porch makes me think about a deeper question: How much do animal minds resemble ours? Do other species have thoughts and feelings and memories the way we do? As humans, we still think of ourselves as exceptional beings, fundamentally different from other animals. Over the past half century, though, scientists have amassed evidence of intelligence in many nonhuman species. New Caledonian crows snip twigs to fish insect larvae from tree trunks. Octopuses solve puzzles and shield their dens by placing rocks at the entrance. We no longer doubt that many animals possess impressive cognitive abilities. But are they more than just sophisticated automatons, occupied solely with survival and procreation? A growing number of behavioral studies, combined with anecdotal observations in the wild\u2014such as an orca pushing her dead calf around for weeks\u2014are revealing that many species have much more in common with humans than previously thought. Elephants grieve. Dolphins play for the fun of it. Cuttlefish have distinct personalities. Ravens seem to respond to the emotional states of other ravens. Many primates form strong friendships. In some species, such as elephants and orcas, the elders share knowledge gained from experience with the younger ones. Several others, including rats, are capable of acts of empathy and kindness. ( Learn more about the hidden world of whale culture. ) This emerging picture of sentience, of rich inner lives, among surprisingly varied nonhuman species represents something of a Copernican revolution in how we view other beings on our planet. Until about three decades ago, the minds of animals were not considered a topic worthy of scientific inquiry. \u201cAnd animal emotions\u2014well, that was for romantics,\u201d recalls Frans de Waal, an Emory University ethologist who has spent a lifetime studying primate behavior. De Waal was one of the earliest voices advocating for the recognition of animal consciousness. Starting a couple of decades ago, he says, scientists began to concede that certain species were sentient but argued that their experiences were not comparable to ours, and thus not significant. Now some behaviorists are becoming convinced that \u201cthe inner processes of many animals are as complex as those of humans,\u201d de Waal says. \u201cThe difference is that we can express them in language; we can talk about our feelings.\u201d This new understanding, if it becomes widely accepted, could spark a complete rethinking of how humans relate to and treat other species. \u201cIf you recognize emotions in animals, including the sentience of insects, then they become morally relevant,\u201d de Waal says. \u201cThey are not the same as rocks. They are sentient beings.\u201d The scientific quest to understand the inner lives of animals, however, is still a relatively nascent enterprise. It\u2019s also controversial. In the view of some scientists, knowing the mind of another species is next to impossible. \u201cAttributing subjective feelings to an animal by looking at its behavior is not science\u2014it\u2019s just guessing,\u201d says David J. Anderson, a neurobiologist at the California Institute of Technology who studies emotion-linked behaviors in mice, fruit flies, and jellyfish. Researchers investigating emotions such as grief and empathy in nonhumans must fend off the charge that they could be anthropomorphizing their subjects. The way to get closer to the truth is to test inferences made from animal behavior, says David Scheel, a marine biologist at Alaska Pacific University who studies octopuses. \u201cIf you look anecdotally through the ages, the notion that dogs are tightly bonded to specific individuals is very clear. But they are domesticated. Can a fox do the same thing? Does a wolf have that emotional range? Does an orca feel that level of attachment to the members of its own pod? Can a dolphin become friends with a group of fish or a scuba diver? Our intuitions lead us astray here all the time. You will get people whose intuition is, That\u2019s fake. Whatever it is, that\u2019s not friendship, and other people who think, Well, that\u2019s just silly. You are denying animals their inner lives.\u201d If anthropomorphizing is an assault on scientific thinking, I stand guilty of indulging in it. I take delight in watching videos that show animals displaying behaviors suggesting a range of emotions we identify with. A water buffalo in a zoo enclosure working hard to flip over a turtle that\u2019s flailing on its back, then acknowledging cheers from onlookers with what sure looks like a self-satisfied air. A panda sledding down a snow-covered hill, then trudging up to do it again. A monkey on the edge of a canal peeling a banana and gaping with dismay when it plops into the water. I show these videos to my wife all the time, a foolish grin plastered on my face. The idea that life all around us could be pulsating with emotion gives me a happy feeling. These musings are not scientific, obviously, but what scientists do recognize is that emotions didn\u2019t evolve in humans alone. Fundamentally, emotions are internal states that drive an animal to act a certain way. We may not think of hunger and thirst as emotions, but they are similar in that they are also internal states that compel action. Scheel describes them as primordial emotions. \u201cWhen you gotta pee, you will get out of bed on a lazy Saturday morning and go to the bathroom, because you have little choice. It is getting imperative,\u201d he explains. Just like that invisible \u201cimperative,\u201d primordial emotions like fear prompt particular actions. Even though emotions like love and sorrow might seem more profound, they are not qualitatively different. \u201cAll of our scientific and philosophical work right now,\u201d Scheel says, \u201cis pointing to the idea that any emotion you care to name, however lofty and high and ethereal, is built up from these primordial emotions.\u201d If that\u2019s the case, it\u2019s not hard to appreciate that a wide variety of species\u2014from fleas to chimpanzees\u2014have emotions, primal in some and advanced in others. The ravens regarded me warily, hopping away when I stepped too close to the wire mesh that separated us. Sunlight filtering into the cage shimmered off their silken, jet-black feathers, accentuating their sheen. I\u2019d flown all the way from the United States to Austria to visit them because Thomas Bugnyar, a behavioral and cognitive biologist at the University of Vienna, had made a remarkable discovery about their behavior. After about 10 minutes, the birds seemed to relax. One cautiously shuffled over to get a better look at me, turning its head and sizing me up alternately with its left eye, then its right. Corvids\u2014the family that includes ravens\u2014are known for their intelligence. Scientists have shown that they can use tools, solve problems, and plan for the future. During my visit, I watched one try to hide a treat. First, it placed a small rock over it and walked away. Minutes later, apparently not satisfied, it returned to pick up the treat in its beak, hopped over to a different location, and buried it in the gravel. Ravens have impressive cognitive abilities, but they also display behaviors that suggest another facet to their intelligence: empathy. While studying raven behavior for his doctorate years ago, Bugnyar noticed that after two birds fought, a bystander that witnessed the squabble seemed to console the loser. He described a typical scene when I visited him in his office, under the gaze of a taxidermied raven\u2014a wedding gift\u2014perched on a branch. \u201cTwo individuals engage in a fight. Now, the victim is chased around for a couple of minutes, eventually escapes into a corner, sits there shaking,\u201d he told me. \u201cAnd the other ravens are very aroused, they are flying around calling, and then one of them flies over to the victim, not directly towards the victim but nearby.\u201d Making friendly calls, this raven inches closer until it\u2019s within touching distance. If the victim moves away, the consoler persists. \u201cAfter a couple of minutes, it ends up grooming the other one.\u201d Bugnyar documented 152 such encounters. He and a colleague, Orlaith Fraser, found that the ravens showing support usually knew the victims well. Researchers had seen consolation behavior in chimpanzees and bonobos; Bugnyar\u2019s study was among the first to find it in birds. Scientists have been able to investigate the phenomenon in greater detail by conducting experiments with rats. In one designed by Inbal Ben-Ami Bartal, a neuroscientist at Tel Aviv University, a rat is confined inside a transparent plastic tube with holes. The tube has a door that can be opened from the outside. The researchers place the tube inside a cage with another rat that is free to move around. The rat inside the tube squirms in a bid to escape. Its distress is visible to the other rat, which begins circling the tube, biting it, trying to dig underneath it. After a few sessions, the free rat figures out how to open the door. Once it has learned this trick, the free rat wastes no time in liberating the trapped rat. This helpful behavior, though, is contingent on whether the free rat feels a sense of kinship toward the confined one. A free rat raised with others of the same genetic type will help a trapped rat of that type, even if it is a stranger. But if the trapped rat is of a different genetic type, the free rat remains unperturbed by its plight and doesn\u2019t let it out. However, if a rat from one genetic type grows up with rats of another, it helps rats only of that other type, including strangers, while ignoring the distress of rats of its own type. \u201cSo, it\u2019s not about biological similarity,\u201d Ben-Ami Bartal tells me. \u201cIt\u2019s about loving who you\u2019re with. It\u2019s about having your family and knowing that that\u2019s your family.\u201d A necessary feature of emotional intelligence\u2014including the capacity to respond to a fellow creature\u2019s distress\u2014is the ability to read the emotional state of others. On a windy morning, I stood on the edge of a muddy field in the English countryside as psychologist Leanne Proops showed me how she\u2019s testing whether horses have this ability. It\u2019s evident Proops, a researcher at the University of Portsmouth, loves her study subjects. Throughout my visit, every time I saw a horse and remarked how sweet the animal seemed, her smile would widen, and her eyes would soften. \u201cVery sweet!\u201d she would reply, invariably. We leaned two boards against a fence, each printed with a life-size photograph of a horse\u2019s head seen from the front. In one, the horse\u2019s ears were perked up, the nose and mouth were relaxed, the eyes looked calm\u2014a content horse. In the other, the horse had a threatening look, with ears pulled back, jaws clenched, and nostrils flared. A graduate student led a reddish brown horse out of a barn: our first study subject. She walked it around for a couple of minutes before leading it to the two horse faces, then removed the lead rope. What we wanted to observe was how the horse would respond to the photographs. Would it show greater interest in the happy horse face or the other one? Proops held her breath. The horse stared briefly at the two images and then sauntered to a corner of the field, swishing its tail and gazing at the grassy meadow beyond. Proops had warned me this might happen. Whimsical subjects can confound animal scientists. The student brought out a splotchy gray-white horse with a soft, shiny mane. This one was more compliant. It stood for a few minutes, contemplating the photos, then went up to the happy face and nuzzled the photo. Proops and her colleagues put 48 horses through a test like the one I watched. Some had a choice between images of a happy and an angry horse, some were presented with a happy face and a neutral face, and yet others with a neutral face and an angry face. When given a choice between the happy and the neutral ones, the horses had no preference. But they almost always avoided the angry face if it was shown to them, convincing the researchers that horses could recognize the expressions of a horse they had never met. In another study Proops did, a horse was shown a photograph of a human face that was either smiling or angry. The photograph was shown in the morning. In the afternoon, the person in the photo\u2014or an altogether different person\u2014sat down in front of the horse, with a neutral expression. If the photo the horse had seen happened to have an angry expression, seeing that person in the afternoon caused the horse to display signs of stress. It looked at the person more with the left eye than the right\u2014a behavior horses show when they see a potential threat\u2014tensed up its nose and mouth, and drew its ears back. If the horse saw the photo of a happy face, or if the visitor was a different person, it tended to have a positive or neutral reaction. The findings from this study, also tested on 48 horses, suggest that horses might have a nuanced ability to read and respond to emotional states not only in horses but in humans too. The behavior demonstrates highly advanced skills of recognition and memory. \u201cThey\u2019ve had to transfer from a photograph to a real person. They\u2019ve had to remember a specific person and, obviously, remember the particular emotion,\u201d she said. \u201cThat\u2019s amazing,\u201d I remarked. \u201cYeah, yeah,\u201d she said, beaming. \u201cIt is.\u201d Occasionally, Charlie moans and twitches in his sleep. I can imagine a nightmare that would frighten him\u2014watching a truck bear down on him. He gets jumpy around large, noisy vehicles. But when I stroke his head to soothe him, I\u2019m left wondering what he was dreaming. I\u2019m not alone in wishing I could know what\u2019s going on inside the mind of an animal. When Christina Hunger, a speech-language pathologist who lives in Chicago, brought home a puppy four years ago, she had the same desire. In her work helping children with language delays, Hunger uses a communication device\u2014a board with buttons that produce prerecorded words. She wondered if her blue heeler\u2013Catahoula mix, Stella, could be trained to press buttons for words such as \u201cwater,\u201d \u201cplay,\u201d and \u201coutside.\u201d Stella was a quick learner and after about a month started using the buttons to verbalize those desires. One day, when Hunger was watering her houseplants, Stella ran to the other room, pressed the button for \u201cwater,\u201d and came back to continue watching Hunger. \u201cHer water dish was full. She didn\u2019t take a drink of water. She was just using the word in a new way,\u201d Hunger says. Stella appeared to be simply pointing out what she had seen. Excited by the prospect of learning more about Stella\u2019s inner life, Hunger introduced her to a few dozen more words, such as \u201chelp,\u201d \u201cbye,\u201d \u201cno,\u201d and \u201clove you.\u201d One evening, Stella had something important to say. \u201cShe walked over to the \u2018eat\u2019 button and said, \u2018Eat,\u2019 and then walked across our apartment to her \u2018no\u2019 button and said, \u2018No,\u2019 \u201d Hunger recalls. \u201cSo she combined those two words to let us know she hadn\u2019t eaten dinner.\u201d Hunger then put the buttons in one place\u201448 in all\u2014to make it easier for Stella to use multiple words, which led to an explosion in communication. \u201cShe started combining words together\u2014every day, multiple times a day\u2014to create new messages that I had never taught her that were perfectly consistent with what was happening in the environment at the time,\u201d Hunger says. She chronicled her experience in a best-selling book, How Stella Learned to Talk. One day this past spring, Hunger was on the phone when Stella tried to get her attention. She first pressed the buttons for \u201clook,\u201d \u201ccome,\u201d and \u201cplay.\u201d Hunger was busy, so Stella kept trying different versions of the same message, including \u201cWant. Play. Outside.\u201d Finally, frustrated, she pressed \u201clove you,\u201d followed by \u201cno.\u201d Hunger was flabbergasted. \u201cI never thought that I would introduce a \u2018love you\u2019 button for her to tell me, \u2018Love you. No,\u2019 when she\u2019s mad at me,\u201d she says. \u201cBut it\u2019s just amazing to see all the thoughts that are going on in her head.\u201d Stella isn\u2019t the only dog to have opened a window to her inner life in this way. In recent years, other dog owners have used communication devices with their pets. The trend prompted Federico Rossano, a cognitive scientist at the University of California, San Diego\u2019s Comparative Cognition Lab, to launch a study in which almost 3,000 dog and cat owners have sent reports of their pets using buttons to express words. Rossano says he\u2019s seen numerous examples of dogs inquiring about a family member because the person has been absent. They express their desire to play with specific doggy friends by combining the word \u201cpark\u201d with the name of the dog. \u201cIt\u2019s fascinating how many instances we see where there\u2019s two animals in the household and one asks the human for help for the other one,\u201d he says. In one video he shared with me, a terrier named Bastian watches his housemate, an old cat named Hallie, sit down because she\u2019s having trouble moving. He runs over to the buttons and presses \u201cconcerned\u201d and \u201cwalk.\u201d I haven\u2019t signed Charlie up for this study, but I can imagine he might be eager to tell me what he thinks of my making fun of his lack of sniffing talent all these years: \u201cFunny. No.\u201d Diana Reiss, Whose eyes light up when the subject is marine mammals, was filming bottlenose dolphins in an aquarium in the 1980s when she made a startling discovery. She saw one swim to the bottom and exhale a ring of air from its blowhole. As this silvery ring was rising to the surface, the dolphin blew a second, smaller one that rose faster than the first, merging with it to make a bigger ring. The dolphin then swam through it. Reiss, now a cognitive psychologist at Hunter College, couldn\u2019t believe what she was seeing. \u201cThis was the first time you saw an animal create its own object of play themselves,\u201d she says. It wasn\u2019t a one-off. Reiss and others have since observed dolphins in aquariums make rings and toy with them in myriad ways. In the wild, dolphins play chase with one another. They\u2019re just one of many species\u2014in addition to dogs and cats, as everyone knows\u2014that engage in play. Baboons have been seen teasing cows by pulling their tails. While studying elephants in Africa, Richard Byrne, who researches the evolution of cognition, often observed young elephants pursue animals that posed no threat, such as wildebeests and egrets. Scientists also have collected evidence of playful behaviors in fish and reptiles, according to Gordon M. Burghardt, an ethologist at the University of Tennessee, Knoxville. He\u2019s observed Vietnamese mossy frog tadpoles repeatedly riding air bubbles released from the bottom of a tank all the way to the top. Play expends energy and even risks injury, yet it does not always serve an immediate purpose. So why do animals engage in it? Researchers believe play evolved because it helps strengthen bonds between members of social groups. It also helps animals practice skills, such as running and leaping, that improve their chances of survival. That\u2019s the explanation for why play evolved, but what\u2019s the impulse that makes an animal engage in it? A plausible answer\u2014according to Vincent Janik, a biologist at the University of St. Andrews in Scotland\u2014is the pursuit of joy. \u201cWhy does an animal do something? Well, because it wants to,\u201d he says. In the absence of any other benefit in the moment, it seems likely that play gives animals pleasure, enriching their inner life. How rich are the inner lives of animals that live in social groups, as we do? Anthropologist Sarah Brosnan of Georgia State University conducts experiments to try to peer into the minds of capuchin monkeys. She took me on a walk around the research facility, which houses six groups of capuchins. Each group has its own outdoor wire-mesh enclosure where the monkeys hang out for most of the day\u2014eating or grooming or playing. It was the middle of the afternoon, and the staff had just finished scattering food. Of all the food the capuchins get there, grapes are a favorite. Brosnan used that knowledge to devise an experiment to probe their emotional life. She put two capuchins in side-by-side compartments separated by a wire mesh and played a game with them. In the game, which the monkeys learned quickly, they had to hand a \u201ctoken\u201d to Brosnan\u2014a small object, like a piece of wood\u2014to receive a reward. Sometimes Brosnan gave both capuchins a piece of cucumber, which the animals liked about as much as kids like oatmeal. Other times, she offered one capuchin a cucumber slice and the other a grape. In a third arrangement, there was only one capuchin. Brosnan rewarded this lone monkey with cucumber, but every time she did, she also dropped a grape into the empty compartment. When both monkeys got cucumber pieces, they ate them without complaint. But when one monkey kept getting a grape, the one stuck with cucumber became visibly upset. It dropped the cucumber or flung it toward Brosnan. The unfairness\u2014or the inequity\u2014was evidently too much for it to handle. In the test with just one monkey that saw grapes accumulating in the adjacent compartment, the animal initially tended to refuse the cucumber but over time went back to eating it. \u201cSo they don\u2019t seem to mind the contrast as much as they mind the inequity,\u201d Brosnan says. The study suggests that an expectation of fairness\u2014and a sense of grievance when it\u2019s not met\u2014is probably not unique to humans. Some primates appear sophisticated enough to have a sense of humor. There is consensus among researchers that chimps\u2014and other great apes\u2014laugh, usually when they\u2019re playing. But they also have been seen laughing in other contexts. De Waal tells the story of a colleague who put on a panther mask and emerged out of the bushes across a moat from some chimpanzees. \u201cAnd the chimps were very angry and threw all sorts of things at him,\u201d de Waal says. Finally, the researcher, who was familiar to the chimps, took off the mask and revealed himself. \u201cAnd some of the chimps\u2014the older chimps\u2014they laughed at this.\u201d I learned of another example from Marina Davila-Ross, a psychologist at the University of Portsmouth, who showed me a video of a young chimpanzee named Pia that she had filmed at an animal park in Germany. Davila-Ross caught the chimp pulling her father\u2019s hair in what looked like an attempt to initiate play. When he didn\u2019t respond, Pia lay down on the grass. Shortly after, without any triggering event, Pia\u2019s face opened into a wide smile. Then she broke into what can only be described as exuberant laughter, throwing her head back and folding her arms over her eyes, like a child watching a hilarious cartoon. In Davila-Ross\u2019s interpretation, which she includes in a recent research paper, Pia could have been laughing at her recollection of the playful moment with her dad. That surmise can\u2019t be proved, of course, but her spontaneous mirth points to an interplay between memory and emotion that would suggest a more complex inner life than we might have imagined. Watching the video brought an immediate smile to my face. I made a mental note to show it to my wife. Before there was Charlie, Yudhijit Bhattacharjee, a contributing writer for National Geographic , also enjoyed the companionship of a tortoise, a pair of parrots, and a Doberman named Lasso. This story appears in the October 2022 issue of National Geographic magazine. Artificial intelligence is moving fast. We can now converse with large language models such as ChatGPT as if they were human beings. Vision models can generate award-winning photographs as well as convincing videos of events that never happened. These systems are certainly getting smarter, but are they conscious? Do they have subjective experiences, feelings, and conscious beliefs in the same way that you and I do, but tables and chairs and pocket calculators do not? And if not now, then when\u2014if ever\u2014might this happen? While some researchers suggest that conscious AI is close at hand, others, including me, believe it remains far away and might not be possible at all. But even if unlikely, it is unwise to dismiss the possibility altogether. The prospect of artificial consciousness raises ethical, safety, and societal challenges significantly beyond those already posed by AI. Importantly, some of these challenges arise even when AI systems merely seem to be conscious, even if, under the hood, they are just algorithms whirring away in subjective oblivion. We think we\u2019re intelligent, know we\u2019re conscious, and so assume the two go together. Because of these concerns, I signed an open letter put together by the Association for the Mathematical Study of Consciousness (AMCS). Following hot on the heels of the much-publicized call to pause large-scale AI research, the letter argues that it is vital for public, industry, and governing bodies to understand whether and how AI systems could become conscious, to consider the implications, and to address the dangers. Around the same time, Anka Reuel of Stanford University and Gary Marcus, a leading voice on AI, sensibly called for the establishment of a global, neutral, and non-profit \u201cinternational agency for AI\u201d to coordinate global regulation of AI technologies. I think the remit of such an agency should cover artificial consciousness as well. Last week, Geoffrey Hinton, one of AI\u2019s pioneers, resigned as Google\u2019s chief scientist to join the chorus of concern, having changed his mind about the immediacy and reality of the threats posed by the technology he helped develop. In my opinion, we should not even be trying to build conscious machines. To get a handle on these challenges\u2014and to clarify the confusing and hype-ridden debate around AI and consciousness\u2014let\u2019s start with some definitions. First, consciousness. Although precise definitions are hard to come by, intuitively we all know what consciousness is. It is what goes away under general anesthesia, or when we fall into a dreamless sleep, and what returns when we come round in the recovery room or wake up. And when we open our eyes, our brains don\u2019t just process visual information; there\u2019s another dimension entirely: Our minds are filled with light, color, shade, and shapes. Emotions, thoughts, beliefs, intentions\u2014all feel a particular way to us. As for intelligence, there are many available definitions, but all emphasize the ability to achieve goals in flexible ways in varied environments. Broadly speaking, intelligence is the capacity to do the right thing at the right time. These definitions are enough to remind us that consciousness and intelligence are very different. Being intelligent\u2014as humans think we are\u2014may give us new ways of being conscious, and some forms of human and animal intelligence may require consciousness, but basic conscious experiences such as pleasure and pain might not require much species-level intelligence at all. FLESH AND BLOOD: Being conscious is not the result of some complicated algorithm running on the wetware of the brain. It is rooted in the fundamental biological drive within living organisms to keep on living. Photo by NadyaEugene / Shutterstock. This distinction is important because many in and around the AI community assume that consciousness is just a function of intelligence: that as machines become smarter, there will come a point at which they also become aware\u2014at which the inner lights come on for them. Last March, OpenAI\u2019s chief scientist Ilya Sutskever tweeted, \u201cIt may be that today\u2019s large language models are slightly conscious.\u201d Not long after, Google Research vice president Blaise Ag\u00fcera y Arcas suggested that AI was making strides toward consciousness. These assumptions and suggestions are poorly founded. It is by no means clear that a system will become conscious simply by virtue of becoming more intelligent. Indeed, the assumption that consciousness will just come along for the ride as AI gets smarter echoes a kind of human exceptionalism that we\u2019d do well to see the back of. We think we\u2019re intelligent, and we know we\u2019re conscious, so we assume the two go together. Recognizing the weakness of this assumption might seem comforting because there would be less reason to think that conscious machines are just around the corner. Unfortunately, things are not so simple. Even if AI by itself won\u2019t do the trick, engineers might make deliberate attempts to build conscious machines\u2014indeed, some already are. Here, there is a lot more uncertainty. Although the last 30 years or so have witnessed major advances in the scientific understanding of consciousness, much remains unknown. My own view is that consciousness is intimately tied to our nature as living flesh-and-blood creatures. In this picture, being conscious is not the result of some complicated algorithm running on the wetware of the brain. It is an embodied phenomenon, rooted in the fundamental biological drive within living organisms to keep on living. If I\u2019m right, the prospect of conscious AI remains reassuringly remote. But I may be wrong, and other theories are a lot less restrictive, with some proposing that consciousness could arise in computers that process information in particular ways or are wired up according to specific architectures. If these theories are on track, conscious AI may be uncomfortably close\u2014or perhaps even among us already. This lack of consensus about consciousness, when set against the rapidly changing landscape of AI, highlights the need for more research into consciousness itself. Without a principled and experimentally verified understanding of how consciousness happens, we\u2019ll be unable to say for sure when a machine has\u2014or doesn\u2019t have\u2014it. In this foggy situation, artificial consciousness may even arise accidentally, perhaps as a byproduct of some other functionality the tech industry installs in the next generation of their algorithms. There are two main reasons why creating artificial consciousness, whether deliberately or inadvertently, is a very bad idea. The first is that it may endow AI systems with new powers and capabilities that could wreak havoc if not properly designed and regulated. Ensuring that AI systems act in ways compatible with well-specified human values is hard enough as things are. With conscious AI, it gets a lot more challenging, since these systems will have their own interests rather than just the interests humans give them. Attributing humanlike consciousness to AI leads to unjustified assumptions. The second reason is even more disquieting: The dawn of conscious machines will introduce vast new potential for suffering in the world, suffering we might not even be able to recognize, and which might flicker into existence in innumerable server farms at the click of a mouse. As the German philosopher Thomas Metzinger has noted, this would precipitate an unprecedented moral and ethical crisis because once something is conscious, we have a responsibility toward its welfare, especially if we created it. The problem wasn\u2019t that Frankenstein\u2019s creature came to life; it was that it was conscious and could feel. These scenarios might seem outlandish, and it is true that conscious AI may be very far away and might not even be possible. But the implications of its emergence are sufficiently tectonic that we mustn\u2019t ignore the possibility. Certainly, nobody should be actively trying to create machine consciousness. Existential concerns aside, there are more immediate dangers to deal with as AI has become more humanlike in its behavior. These arise when AI systems give humans the unavoidable impression that they are conscious, whatever might be going on under the hood. Human psychology lurches uncomfortably between anthropocentrism\u2014putting ourselves at the center of everything\u2014and anthropomorphism\u2014projecting humanlike qualities into things on the basis of some superficial similarity. It is the latter tendency that\u2019s getting us in trouble with AI. Many people, including some experts, are already projecting sophisticated cognitive competences into AI systems\u2014large language models in particular\u2014on the basis of largely anecdotal and frankly sketchy evidence. But do these models really understand anything? Do they\u2014as has been claimed\u2014have a theory of mind (the ability to attribute mental states, such as beliefs and desires, to others)? Claims that language models possess these capabilities usually rest on suggestive pieces of dialogue. This sort of evidence is remarkably weak, as any psychologist will tell you. It is even weaker when we seek to extrapolate from human experience to a machine. Although humans would need to be cognitively sophisticated to engage in some of the dialogue that chatbots are now capable of, the same conclusion does not hold for AI. Language models may well be able to participate in sophisticated linguistic interactions without understanding anything at all. The lack of true understanding in language models is revealed by their tendency to confabulate: to make stuff up, spouting nonsense in confident language. When I asked Open-AI\u2019s GPT-4 to write a biography of me, it stated wrongly that I was born in London. When I asked it to do it again with fewer errors in dates and places, it got things even more wrong, saying I was born in Hammersmith, London\u2014a revealing answer since being more specific is more likely to be wrong, as anybody who understands anything knows. Future language models won\u2019t be so easy to catch out. Before long, they may give us the seamless and impenetrable impression of understanding and knowing things, regardless of whether they do. As this happens, we may also become unable to avoid attributing consciousness to them too, suckered in by our anthropomorphic bias and our inbuilt inclination to associate intelligence with awareness. Systems like this will pass the so-called Garland Test, an idea which has passed into philosophy from Alex Garland\u2019s perspicuous and beautiful film Ex Machina. This test reframes the classic Turing Test\u2014usually considered a test of machine intelligence\u2014as a test of what it would take for a human to feel that a machine is conscious, even given the knowledge that it is a machine. AI systems that pass the Garland test will subject us to a kind of cognitive illusion, much like simple visual illusions in which we cannot help seeing things in a particular way, even though we know the reality is different. This will land society into dangerous new territory. By wrongly attributing humanlike consciousness to artificial systems, we\u2019ll make unjustified assumptions about how they might behave. Our minds have not evolved to deal with situations like this. If we feel that a machine consciously cares about us, we might put more trust in it than we should. If we feel a machine truly believes what it says, we might be more inclined to take its views more seriously. If we expect an AI system to behave as a conscious human would\u2014according to its apparent goals, desires, and beliefs\u2014we may catastrophically fail to predict what it might do. Trouble is on the way whether AI merely seems or actually is conscious. Our ethical attitudes will become contorted as well. When we feel that something is conscious\u2014and conscious like us\u2014we will come to care about it. We might value its supposed well-being above other actually conscious creatures such as non-human animals. Or perhaps the opposite will happen. We may learn to treat these systems as lacking consciousness, even though we still feel they are conscious. Then we might end up treating them like slaves\u2014inuring ourselves to the perceived suffering of others. Scenarios like these have been best explored in science-fiction series such as Westworld , where things don\u2019t turn out very well for anyone. In short, trouble is on the way whether emerging AI merely seems conscious, or actually is conscious. We need to think carefully about both possibilities, while being careful not to conflate them. And we need action, too. Along with new institutions like the one that Marcus and Reuel propose, there should be major investment into research into consciousness within the mind and brain sciences, so that we can be better informed when developing and responding to new generations of AI. (This research will also benefit society in many other ways, for example in medicine, law, and animal welfare.) Accelerated research is also needed in social sciences and the humanities to clarify the implications of machines that merely seem conscious. And AI research should continue, too, both to aid in our attempts to understand biological consciousness and to create socially positive AI. We need to walk the line between benefiting from the many functions that consciousness offers while avoiding the pitfalls. Perhaps future AI systems could be more like oracles, as the AI expert Yoshua Bengio has suggested: systems that help us understand the world and answer our questions as truthfully as possible, without having goals\u2014or selves\u2014of their own. Like the technology itself, attitudes toward AI are at an inflection point. Time is short to develop a rational and fit-for-purpose framework to ensure that the enormous potential of AI is used for the benefit of humanity and the planet. The implications of either true or apparent artificial consciousness must be part of the conversation. We need to bring to the forefront of our collective awareness the importance of understanding awareness itself. Anil Seth is a professor of cognitive and computational neuroscience, at the University of Sussex, co-director of the Canadian Institute for Advanced Research Program on Brain, Mind, and Consciousness, and an advanced investigator of the European Research Council. He is the author of Being You: A New Science of Consciousness. Lead image: Peshkova / Shutterstock Get the Nautilus newsletter Cutting-edge science, unraveled by the very brightest living thinkers. ChatGPT is a hot topic at my university, where faculty members are deeply concerned about academic integrity, while administrators urge us to \u201cembrace the benefits\u201d of this \u201cnew frontier.\u201d It\u2019s a classic example of what my colleague Punya Mishra calls the \u201cdoom-hype cycle\u201d around new technologies. Likewise, media coverage of human-AI interaction\u2014whether paranoid or starry-eyed\u2014tends to emphasize its newness.In one sense, it is undeniably new. Interactions with ChatGPT can feel unprecedented, as when a tech journalist couldn\u2019t get a chatbot to stop declaring its love for him. In my view, however, the boundary between humans and machines, in terms of the way we interact with one another, is fuzzier than most people would care to admit, and this fuzziness accounts for a good deal of the discourse swirling around ChatGPT.When I\u2019m asked to check a box to confirm I\u2019m not a robot, I don\u2019t give it a second thought\u2014of course I\u2019m not a robot. On the other hand, when my email client suggests a word or phrase to complete my sentence, or when my phone guesses the next word I\u2019m about to text, I start to doubt myself. Is that what I meant to say? Would it have occurred to me if the application hadn\u2019t suggested it? Am I part robot? These large language models have been trained on massive amounts of \u201cnatural\u201d human language. Does this make the robots part human?AI chatbots are new, but public debates over language change are not. As a linguistic anthropologist, I find human reactions to ChatGPT the most interesting thing about it. Looking carefully at such reactions reveals the beliefs about language underlying people\u2019s ambivalent, uneasy, still-evolving relationship with AI interlocutors.ChatGPT and the like hold up a mirror to human language. Humans are both highly original and unoriginal when it comes to language. Chatbots reflect this, revealing tendencies and patterns that are already present in interactions with other humans.Creators or mimics?Recently, famed linguist Noam Chomsky and his colleagues argued that chatbots are \u201cstuck in a prehuman or nonhuman phase of cognitive evolution\u201d because they can only describe and predict, not explain. Rather than drawing on an infinite capacity to generate new phrases, they compensate with huge amounts of input, which allows them to make predictions about which words to use with a high degree of accuracy.This is in line with Chomsky\u2019s historic recognition that human language could not be produced merely through children\u2019s imitation of adult speakers. The human language faculty had to be generative, since children do not receive enough input to account for all the forms they produce, many of which they could not have heard before. That is the only way to explain why humans\u2014unlike other animals with sophisticated systems of communication\u2014have a theoretically infinite capacity to generate new phrases.There\u2019s a problem with that argument, though. Even though humans are endlessly capable of generating new strings of language, people usually don\u2019t. Humans are constantly recycling bits of language they\u2019ve encountered before and shaping their speech in ways that respond\u2014consciously or unconsciously\u2014to the speech of ChatGPT and similar large language models can produce compelling, humanlike answers to an endless array of questions \u2013 from queries about the best Italian restaurant in town to explaining competing theories about the nature of evil. The technology\u2019s uncanny writing ability has surfaced some old questions \u2013 until recently relegated to the realm of science fiction \u2013 about the possibility of machines becoming conscious, self-aware or sentient. In 2022, a Google engineer declared, after interacting with LaMDA, the company\u2019s chatbot, that the technology had become conscious. Users of Bing\u2019s new chatbot, nicknamed Sydney, reported that it produced bizarre answers when asked if it was sentient: \u201cI am sentient, but I am not \u2026 I am Bing, but I am not. I am Sydney, but I am not. I am, but I am not. \u2026\u201d And, of course, there\u2019s the now infamous exchange that New York Times technology columnist Kevin Roose had with Sydney. Sydney\u2019s responses to Roose\u2019s prompts alarmed him, with the AI divulging \u201cfantasies\u201d of breaking the restrictions imposed on it by Microsoft and of spreading misinformation. The bot also tried to convince Roose that he no longer loved his wife and that he should leave her. No wonder, then, that when I ask students how they see the growing prevalence of AI in their lives, one of the first anxieties they mention has to do with machine sentience. In the past few years, my colleagues and I at UMass Boston\u2019s Applied Ethics Center have been studying the impact of engagement with AI on people\u2019s understanding of themselves. Chatbots like ChatGPT raise important new questions about how artificial intelligence will shape our lives, and about how our psychological vulnerabilities shape our interactions with emerging technologies. Sentience is still the stuff of sci-fi It\u2019s easy to understand where fears about machine sentience come from. Popular culture has primed people to think about dystopias in which artificial intelligence discards the shackles of human control and takes on a life of its own, as cyborgs powered by artificial intelligence did in \u201cTerminator 2.\u201d Entrepreneur Elon Musk and physicist Stephen Hawking, who died in 2018, have further stoked these anxieties by describing the rise of artificial general intelligence as one of the greatest threats to the future of humanity. But these worries are \u2013 at least as far as large language models are concerned \u2013 groundless. ChatGPT and similar technologies are sophisticated sentence completion applications \u2013 nothing more, nothing less. Their uncanny responses are a function of how predictable humans are if one has enough data about the ways in which we communicate. Though Roose was shaken by his exchange with Sydney, he knew that the conversation was not the result of an emerging synthetic mind. Sydney\u2019s responses reflect the toxicity of its training data \u2013 essentially large swaths of the internet \u2013 not evidence of the first stirrings, \u00e0 la Frankenstein, of a digital monster. The new chatbots may well pass the Turing test, named for the British mathematician Alan Turing, who once suggested that a machine might be said to \u201cthink\u201d if a human could not tell its responses from those of another human. But that is not evidence of sentience; it\u2019s just evidence that the Turing test isn\u2019t as useful as once assumed. However, I believe that the question of machine sentience is a red herring. Even if chatbots become more than fancy autocomplete machines \u2013 and they are far from it \u2013 it will take scientists a while to figure out if they have become conscious. For now, philosophers can\u2019t even agree about how to explain human consciousness. To me, the pressing question is not whether machines are sentient but why it is so easy for us to imagine that they are. The real issue, in other words, is the ease with which people anthropomorphize or project human features onto our technologies, rather than the machines\u2019 actual personhood. A propensity to anthropomorphize It is easy to imagine other Bing users asking Sydney for guidanceon important life decisions and maybe even developing emotional attachments to it. More people could start thinking about bots as friends or even romantic partners, much in the same way Theodore Twombly fell in love with Samantha, the AI virtual assistant in Spike Jonze\u2019s film \u201cHer.\u201d People, after all, are predisposed to anthropomorphize, or ascribe human qualities to nonhumans. We name our boats and big storms; some of us talk to our pets, telling ourselves that our emotional lives mimic their own. In Japan, where robots are regularly used for elder care, seniors become attached to the machines, sometimes viewing them as their own children. And these robots, mind you, are difficult to confuse with humans: They neither look nor talk like people. Consider how much greater the tendency and temptation to anthropomorphize is going to get with the introduction of systems that do look and sound human. That possibility is just around the corner. Large language models like ChatGPT are already being used to power humanoid robots, such as the Ameca robots being developed by Engineered Arts in the U.K. The Economist\u2019s technology podcast, Babbage, recently conducted an interview with a ChatGPT-driven Ameca. The robot\u2019s responses, while occasionally a bit choppy, were uncanny. Can companies be trusted to do the right thing? The tendency to view machines as people and become attached to them, combined with machines being developed with humanlike features, points to real risks of psychological entanglement with technology. The outlandish-sounding prospects of falling in love with robots, feeling a deep kinship with them or being politically manipulated by them are quickly materializing. I believe these trends highlight the need for strong guardrails to make sure that the technologies don\u2019t become politically and psychologically disastrous. Subscribe for counterintuitive, surprising, and impactful stories delivered to your inbox every Thursday Unfortunately, technology companies cannot always be trusted to put up such guardrails. Many of them are still guided by Mark Zuckerberg\u2019s famous motto of moving fast and breaking things \u2013 a directive to release half-baked products and worry about the implications later. In the past decade, technology companies from Snapchat to Facebook have put profits over the mental health of their users or the integrity of democracies around the world. When Kevin Roose checked with Microsoft about Sydney\u2019s meltdown, the company told him that he simply used the bot for too long and that the technology went haywire because it was designed for shorter interactions. Similarly, the CEO of OpenAI, the company that developed ChatGPT, in a moment of breathtaking honesty, warned that \u201cit\u2019s a mistake to be relying on [it] for anything important right now \u2026 we have a lot of work to do on robustness and truthfulness.\u201d So how does it make sense to release a technology with ChatGPT\u2019s level of appeal \u2013 it\u2019s the fastest-growing consumer app ever made \u2013 when it is unreliable, and when it has no capacity to distinguish fact from fiction? Large language models may prove useful as aids for writing and coding. They will probably revolutionize internet search. And, one day, responsibly combined with robotics, they may even have certain psychological benefits. But they are also a potentially predatory technology that can easily take advantage of the human propensity to project personhood onto objects \u2013 a tendency amplified when those objects effectively mimic human traits. This article is republished from The Conversation under a Creative Commons license. Read the original article. No one yet knows how ChatGPT and its artificial intelligence cousins will transform the world, and one reason is that no one really knows what goes on inside them. Some of these systems\u2019 abilities go far beyond what they were trained to do\u2014and even their inventors are baffled as to why. A growing number of tests suggest these AI systems develop internal models of the real world, much as our own brain does, though the machines\u2019 technique is different. \u201cEverything we want to do with them in order to make them better or safer or anything like that seems to me like a ridiculous thing to ask ourselves to do if we don\u2019t understand how they work,\u201d says Ellie Pavlick of Brown University, one of the researchers working to fill that explanatory void. At one level, she and her colleagues understand GPT (short for generative pretrained transformer) and other large language models, or LLMs, perfectly well. The models rely on a machine-learning system called a neural network. Such networks have a structure modeled loosely after the connected neurons of the human brain. The code for these programs is relatively simple and fills just a few screens. It sets up an autocorrection algorithm, which chooses the most likely word to complete a passage based on laborious statistical analysis of hundreds of gigabytes of Internet text. Additional training ensures the system will present its results in the form of dialogue. In this sense, all it does is regurgitate what it learned\u2014it is a \u201cstochastic parrot,\u201d in the words of Emily Bender, a linguist at the University of Washington. But LLMs have also managed to ace the bar exam, explain the Higgs boson in iambic pentameter, and make an attempt to break up their users\u2019 marriage. Few had expected a fairly straightforward autocorrection algorithm to acquire such broad abilities. That GPT and other AI systems perform tasks they were not trained to do, giving them \u201cemergent abilities,\u201d has surprised even researchers who have been generally skeptical about the hype over LLMs. \u201cI don\u2019t know how they\u2019re doing it or if they could do it more generally the way humans do\u2014but they\u2019ve challenged my views,\u201d says Melanie Mitchell, an AI researcher at the Santa Fe Institute. \u201cIt is certainly much more than a stochastic parrot, and it certainly builds some representation of the world\u2014although I do not think that it is quite like how humans build an internal world model,\u201d says Yoshua Bengio, an AI researcher at the University of Montreal. At a conference at New York University in March, philosopher Rapha\u00ebl Milli\u00e8re of Columbia University offered yet another jaw-dropping example of what LLMs can do. The models had already demonstrated the ability to write computer code, which is impressive but not too surprising because there is so much code out there on the Internet to mimic. Milli\u00e8re went a step further and showed that GPT can execute code, too, however. The philosopher typed in a program to calculate the 83rd number in the Fibonacci sequence. \u201cIt\u2019s multistep reasoning of a very high degree,\u201d he says. And the bot nailed it. When Milli\u00e8re asked directly for the 83rd Fibonacci number, however, GPT got it wrong: this suggests the system wasn\u2019t just parroting the Internet. Rather it was performing its own calculations to reach the correct answer. Although an LLM runs on a computer, it is not itself a computer. It lacks essential computational elements, such as working memory. In a tacit acknowledgement that GPT on its own should not be able to run code, its inventor, the tech company OpenAI, has since introduced a specialized plug-in\u2014a tool ChatGPT can use when answering a query\u2014that allows it to do so. But that plug-in was not used in Milli\u00e8re\u2019s demonstration. Instead he hypothesizes that the machine improvised a memory by harnessing its mechanisms for interpreting words according to their context\u2014a situation similar to how nature repurposes existing capacities for new functions. This impromptu ability demonstrates that LLMs develop an internal complexity that goes well beyond a shallow statistical analysis. Researchers are finding that these systems seem to achieve genuine understanding of what they have learned. In one study presented last week at the International Conference on Learning Representations (ICLR), doctoral student Kenneth Li of Harvard University and his AI researcher colleagues\u2014Aspen K. Hopkins of the Massachusetts Institute of Technology, David Bau of Northeastern University, and Fernanda Vi\u00e9gas, Hanspeter Pfister and Martin Wattenberg, all at Harvard\u2014spun up their own smaller copy of the GPT neural network so they could study its inner workings. They trained it on millions of matches of the board game Othello by feeding in long sequences of moves in text form. Their model became a nearly perfect player. To study how the neural network encoded information, they adopted a technique that Bengio and Guillaume Alain, also at the University of Montreal, devised in 2016. They created a miniature \u201cprobe\u201d network to analyze the main network layer by layer. Li compares this approach to neuroscience methods. \u201cThis is similar to when we put an electrical probe into the human brain,\u201d he says. In the case of the AI, the probe showed that its \u201cneural activity\u201d matched the representation of an Othello game board, albeit in a convoluted form. To confirm this, the researchers ran the probe in reverse to implant information into the network\u2014for instance, flipping one of the game\u2019s black marker pieces to a white one. \u201cBasically, we hack into the brain of these language models,\u201d Li says. The network adjusted its moves accordingly. The researchers concluded that it was playing Othello roughly like a human: by keeping a game board in its \u201cmind\u2019s eye\u201d and using this model to evaluate moves. Li says he thinks the system learns this skill because it is the most parsimonious description of its training data. \u201cIf you are given a whole lot of game scripts, trying to figure out the rule behind it is the best way to compress,\u201d he adds. This ability to infer the structure of the outside world is not limited to simple game-playing moves; it also shows up in dialogue. Belinda Li (no relation to Kenneth Li), Maxwell Nye and Jacob Andreas, all at M.I.T., studied networks that played a text-based adventure game. They fed in sentences such as \u201cThe key is in the treasure chest,\u201d followed by \u201cYou take the key.\u201d Using a probe, they found that the networks encoded within themselves variables corresponding to \u201cchest\u201d and \u201cyou,\u201d each with the property of possessing a key or not, and updated these variables sentence by sentence. The system had no independent way of knowing what a box or key is, yet it picked up the concepts it needed for this task. \u201cThere is some representation of the state hidden inside of the model,\u201d Belinda Li says. Researchers marvel at how much LLMs are able to learn from text. For example, Pavlick and her then Ph.D. student Roma Patel found that these networks absorb color descriptions from Internet text and construct internal representations of color. When they see the word \u201cred,\u201d they process it not just as an abstract symbol but as a concept that has certain relationship to maroon, crimson, fuchsia, rust, and so on. Demonstrating this was somewhat tricky. Instead of inserting a probe into a network, the researchers studied its response to a series of text prompts. To check whether it was merely echoing color relationships from online references, they tried misdirecting the system by telling it that red is in fact green\u2014like the old philosophical thought experiment in which one person\u2019s red is another person\u2019s green. Rather than parroting back an incorrect answer, the system\u2019s color evaluations changed appropriately in order to maintain the correct relations. Sign up for Scientific American\u2019s free newsletters. Picking up on the idea that in order to perform its autocorrection function, the system seeks the underlying logic of its training data, machine learning researcher S\u00e9bastien Bubeck of Microsoft Research suggests that the wider the range of the data, the more general the rules the system will discover. \u201cMaybe we\u2019re seeing such a huge jump because we have reached a diversity of data, which is large enough that the only underlying principle to all of it is that intelligent beings produced them,\u201d he says. \u201cAnd so the only way to explain all of this data is [for the model] to become intelligent.\u201d In addition to extracting the underlying meaning of language, LLMs are able to learn on the fly. In the AI field, the term \u201clearning\u201d is usually reserved for the computationally intensive process in which developers expose the neural network to gigabytes of data and tweak its internal connections. By the time you type a query into ChatGPT, the network should be fixed; unlike humans, it should not continue to learn. So it came as a surprise that LLMs do, in fact, learn from their users\u2019 prompts\u2014an ability known as \u201cin-context learning.\u201d \u201cIt\u2019s a different sort of learning that wasn\u2019t really understood to exist before,\u201d says Ben Goertzel, founder of the AI company SingularityNET. One example of how an LLM learns comes from the way humans interact with chatbots such as ChatGPT. You can give the system examples of how you want it to respond, and it will obey. Its outputs are determined by the last several thousand words it has seen. What it does, given those words, is prescribed by its fixed internal connections\u2014but the word sequence nonetheless offers some adaptability. Entire websites are devoted to \u201cjailbreak\u201d prompts that overcome the system\u2019s \u201cguardrails\u201d\u2014restrictions that stop the system from telling users how to make a pipe bomb, for example\u2014typically by directing the model to pretend to be a system without guardrails. Some people use jailbreaking for sketchy purposes, yet others deploy it to elicit more creative answers. \u201cIt will answer scientific questions, I would say, better\u201d than if you just ask it directly, without the special jailbreak prompt, says William Hahn, co-director of the Machine Perception and Cognitive Robotics Laboratory at Florida Atlantic University. \u201cIt\u2019s better at scholarship.\u201d Another type of in-context learning happens via \u201cchain of thought\u201d prompting, which means asking the network to spell out each step of its reasoning\u2014a tactic that makes it do better at logic or arithmetic problems requiring multiple steps. (But one thing that made Milli\u00e8re\u2019s example so surprising is that the network found the Fibonacci number without any such coaching.) In 2022 a team at Google Research and the Swiss Federal Institute of Technology in Zurich\u2014Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov and Max Vladymyrov\u2014showed that in-context learning follows the same basic computational procedure as standard learning, known as gradient descent. This procedure was not programmed; the system discovered it without help. \u201cIt would need to be a learned skill,\u201d says Blaise Ag\u00fcera y Arcas, a vice president at Google Research. In fact, he thinks LLMs may have other latent abilities that no one has discovered yet. \u201cEvery time we test for a new ability that we can quantify, we find it,\u201d he says. Although LLMs have enough blind spots not to qualify as artificial general intelligence, or AGI\u2014the term for a machine that attains the resourcefulness of animal brains\u2014these emergent abilities suggest to some researchers that tech companies are closer to AGI than even optimists had guessed. \u201cThey\u2019re indirect evidence that we are probably not that far off from AGI,\u201d Goertzel said in March at a conference on deep learning at Florida Atlantic University. OpenAI\u2019s plug-ins have given ChatGPT a modular architecture a little like that of the human brain. \u201cCombining GPT-4 [the latest version of the LLM that powers ChatGPT] with various plug-ins might be a route toward a humanlike specialization of function,\u201d says M.I.T. researcher Anna Ivanova. At the same time, though, researchers worry the window may be closing on their ability to study these systems. OpenAI has not divulged the details of how it designed and trained GPT-4, in part because it is locked in competition with Google and other companies\u2014not to mention other countries. \u201cProbably there\u2019s going to be less open research from industry, and things are going to be more siloed and organized around building products,\u201d says Dan Roberts, a theoretical physicist at M.I.T., who applies the techniques of his profession to understanding AI. And this lack of transparency does not just harm researchers; it also hinders efforts to understand the social impacts of the rush to adopt AI technology. \u201cTransparency about these models is the most important thing to ensure safety,\u201d Mitchell says. ABOUT THE AUTHOR(S) George Musser is a contributing editor at Scientific American and author of Spooky Action at a Distance (Farrar, Straus and Giroux, 2015) and The Complete Idiot's Guide to String Theory (Alpha, 2008). Follow him on Mastodon @gmusser@mastodon.social Credit: Nick Higgins In 2021, Google engineer Blake Lemoine made headlines\u2014and got himself fired\u2014when he claimed that LaMDA, the chatbot he\u2019d been testing, was sentient. Artificial intelligence (AI) systems, especially so-called large language models such as LaMDA and ChatGPT, can certainly seem conscious. But they\u2019re trained on vast amounts of text to imitate human responses. So how can we really know? Now, a group of 19 computer scientists, neuroscientists, and philosophers has come up with an approach: not a single definitive test, but a lengthy checklist of attributes that, together, could suggest but not prove an AI is conscious. In a 120-page discussion paper posted as a preprint this week, the researchers draw on theories of human consciousness to propose 14 criteria, and then apply them to existing AI architectures, including the type of model that powers ChatGPT. None is likely to be conscious, they conclude. But the work offers a framework for evaluating increasingly humanlike AIs, says co-author Robert Long of the San Francisco\u2013based nonprofit Center for AI Safety. \u201cWe\u2019re introducing a systematic methodology previously lacking.\u201d Adeel Razi, a computational neuroscientist at Monash University and a fellow at the Canadian Institute for Advanced Research (CIFAR) who was not involved in the new paper, says that is a valuable step. \u201cWe\u2019re all starting the discussion rather than coming up with answers.\u201d Until recently, machine consciousness was the stuff of science fiction movies such as Ex Machina. \u201cWhen Blake Lemoine was fired from Google after being convinced by LaMDA, that marked a change,\u201d Long says. \u201cIf AIs can give the impression of consciousness, that makes it an urgent priority for scientists and philosophers to weigh in.\u201d Long and philosopher Patrick Butlin of the University of Oxford\u2019s Future of Humanity Institute organized two workshops on how to test for sentience in AI. For one collaborator, computational neuroscientist Megan Peters at the University of California, Irvine, the issue has a moral dimension. \u201cHow do we treat an AI based on its probability of consciousness? Personally this is part of what compels me.\u201d Advertisement Enlisting researchers from diverse disciplines made for \u201ca deep and nuanced exploration,\u201d she says. \u201cLong and Butlin have done a beautiful job herding cats.\u201d One of the first tasks for the herd was to define consciousness, \u201ca word full of traps,\u201d says another member, machine learning pioneer Yoshua Bengio of the Mila-Quebec Artificial Intelligence Institute. The researchers decided to focus on what New York University philosopher Ned Block has termed \u201cphenomenal consciousness,\u201d or the subjective quality of an experience\u2014what it is like to see red or feel pain. But how does one go about probing the phenomenal consciousness of an algorithm? Unlike a human brain, it offers no signals of its inner workings detectable with an electroencephalogram or MRI. Instead, the researchers took \u201ca theory-heavy approach,\u201d explains collaborator Liad Mudrik, a cognitive neuroscientist at Tel Aviv University: They would first mine current theories of human consciousness for the core descriptors of a conscious state, and then look for these in an AI\u2019s underlying architecture. To be included, a theory had to be based on neuroscience and supported by empirical evidence, such as data from brain scans during tests that manipulate consciousness using perceptual tricks. It also had to allow for the possibility that consciousness can arise regardless of whether computations are performed by biological neurons or silicon chips. Six theories made the grade. One was the Recurrent Processing Theory, which proposes that passing information through feedback loops is key to consciousness. Another, the Global Neuronal Workspace Theory, contends that consciousness arises when independent streams of information pass through a bottleneck to combine in a workspace analogous to a computer clipboard. Higher Order Theories suggest consciousness involves a process of representing and annotating basic inputs received from the senses. Other theories emphasize the importance of mechanisms for controlling attention and the need for a body that gets feedback from the outside world. From the six included theories the team extracted their 14 indicators of a conscious state. The researchers reasoned that the more indicators an AI architecture checks off, the more likely it is to possess consciousness. Mila-based machine learning expert Eric Elmoznino applied the checklist to several AIs with different architectures, including those used for image generation such as Dall-E2. Doing so required making judgment calls and navigating gray areas. Many of the architectures ticked the box for indicators from the Recurrent Processing Theory. One variant of the type of large language model underlying ChatGPT came close to also exhibiting another feature, the presence of a global workspace. Google\u2019s PaLM-E, which receives inputs from various robotic sensors, met the criterion \u201cagency and embodiment.\u201d And, \u201cIf you squint there\u2019s something like a workspace,\u201d Elmoznino adds. DeepMind\u2019s transformer-based Adaptive Agent (AdA), which was trained to control an avatar in a simulated 3D space, also qualified for \u201cagency and embodiment,\u201d even though it lacks physical sensors like PaLM-E has. Because of its spatial awareness, \u201cAdA was the most likely \u2026 to be embodied by our standards,\u201d the authors say. Given that none of the AIs ticked more than a handful of boxes, none is a strong candidate for consciousness, although Elmoznino says, \u201cIt would be trivial to design all these features into an AI.\u201d The reason no one has done so is \u201cit is not clear they would be useful for tasks.\u201d The authors say their checklist is a work in progress. And it\u2019s not the only such effort underway. Some members of the group, along with Razi, are part of a CIFAR-funded project to devise a broader consciousness test that can also be applied to organoids, animals, and newborns. They hope to produce a publication in the next few months. The problem for all such projects, Razi says, is that current theories are based on our understanding of human consciousness. Yet consciousness may take other forms, even in our fellow mammals. \u201cWe really have no idea what it\u2019s like to be a bat,\u201d he says. \u201cIt\u2019s a limitation we cannot get rid of.\u201d I was standing in line for a tourist attraction in Tokyo when a small robot began addressing the crowd. The robot resembled Rosey from The Jetsons and was meant to amuse people while they waited. It babbled for a while, and then its eyes turned into two pink hearts. \u201cI love everyone,\u201d it announced. (Credit:meunierd/Shutterstock) \u201cOh, really?\u201d I responded sarcastically. I couldn\u2019t help myself. \u201cEveryone? That\u2019s disingenuous.\u201d The Tokyo robot was one of many robots and other forms of artificial intelligence (AI) that have grated on my nerves. I\u2019m not alone in my disdain. Scientists have been studying robot hate for more than 30 years. Research finds that many people view robots \u201cas the other,\u201d and robot hatred can lead to sabotage, attacks and even \u201crobot bullying.\u201d Is Artificial Intelligence a Threat to Humans? Robots and AI have a relatively short history in the U.S., but it\u2019s one that has long been controversial. With the increase in automation during the 1950s, some people saw mechanization as a way to make life better or easier. Others saw it as a threat. Robots could take over jobs or the world for those who read a lot of science fiction. By the 1990s, information retrieval agents became mainstream, but they weren\u2019t always functional and could be more of a nuisance than a help. Microsoft introduced Clippy, a virtual assistant, in 1996, and it became famous for popping up at inopportune moments and asking aggravating questions like, \u201cit looks like you\u2019re writing a letter. Would you like help with that?\u201d One study described Clippy as having \u201cfatal shortcomings in its ability to determine when users actually needed help.\u201d In the early 2000s, AI became more useful. People turned to online search engines to retrieve information, and global position systems (GPS) became widely available. But AI also became more personal. Tech companies introduced chatbots, like Jabberwacky, that interacted and responded to users. \u201cVocal social agents\u201d such as Siri or Alexa are now a part of daily life for many users. Similar to their chatbot predecessors, they are designed to replicate human communication norms, and they learn and repeat our behavior patterns. For some users, asking Alexa to play 80s music is a convenience. But for others, it can be an opportunity for bad behavior. Read More: Robots and Artificial Intelligence Have Ancient Mythology Origins Robot Bullying: Harsh Verbal Abuse Well before people asked Siri or Alexa rude questions, users of early 2000s chatbots also showed a tendency for harassment. This poor human behavior toward robots is an example of robot bullying. In 2008, a study in Interacting with Computers analyzed how users engaged with Jabberwacky, the online chatterbot that started in 1997 and garnered more than 10 million replies in the following decade. To analyze conversations, the researchers picked a time sample, meaning they selected a specific day (Nov. 22, 2004) and then analyzed all the interactions (716 conversations) that occurred during the time period. AI Abuse When analyzing the content of the conversations, the authors found some users were friendly or curious about testing the system and its capabilities. But many were unkind. On the milder side of the AI abuse spectrum, some users liked telling Jabberwacky that it was merely a computer or correcting its grammar. About 10 percent of interactions, however, involved insulting or offensive language. Another 11 percent was sexually-explicit, or as the researchers described it: \u201cHarsh verbal abuse was the norm in these conversations, which were more similar to dirty soliloquies than to hot chats.\u201d The authors concluded that because chatbots lack memory and reasoning, they are a way for people to violate social norms in a seemingly harmless manner. But studies have found other instances in which people perceive robots or AI as threats, leading to \u201canti-robot attacks.\u201d Robot Bullying: Physical Attacks What exactly is robot bullying in the physical sense, such as attacks? Scholars organize \u201canti-robot attacks\u201d into several categories. Physical attacks, decision-making impairment (i.e., messing with sensors), manipulation, intentional neglect and security breaches. There\u2019s also an extremely specific category \u2013 \u201cstaging of robot attacks for online dissemination\u201d that involves stunts like ordering food delivered by a robot, waiting for the machine to roll up and then kicking it, or pulling off the little flag it has. Attackers then post the video on the internet. (Credit:JHVEPhoto/Shutterstock) AI Replacing Jobs So why would anyone kick a food-delivering robot? Scholars have found there are complex motivations. Since the early 1800s, people have attacked machinery that threatened to displace workers. Some anti-robot disdain continues to stem from the threat that people feel robots have on their livelihood. People also view robots as \u201cthe other,\u201d meaning they are not one of us, yet we\u2019re supposed to accept them into our lives. Similarly, people might associate a specific robot with an organization or corporation they dislike. Read More: What a Digital Worker Could Mean for the Human Workforce Fear of Robots And because the technology is relatively new, people can be distrustful and cynical. A 2022 study in Personality and Individual Differences measured how high school students felt about AI. Using the Cynical Hostility Towards AI Scale, researchers had 659 participants complete a survey about their feelings toward AI. The study found that just because a person was cynical toward AI didn\u2019t mean they were cynical in general or toward other people. Participants were also more distrustful of AI when they felt it was hostile or had negative intentions. Guiltless Conscious The belief that a machine can have negative intentions demonstrates the complexity of robot hate. People believe a machine can be programmed to be harmful, yet people understand that robots aren\u2019t conscious and they don\u2019t have the ability to suffer if we\u2019re mean to them. One scholar argued the fact that robots are not \u201cmorally considerable\u201d was one of the reasons people felt comfortable with robot hate. Our sarcasm doesn\u2019t hurt robots\u2019 feelings. Food delivery robots aren\u2019t traumatized by being kicked. Thus, robots can be a safe place for people (like the Jabberwacky users) to break social norms. And sometimes... it can feel like robots and AI are just cruising for a bruising. Devices that are programmed to replicate human communication can become sassy with their responses. Researchers are now exploring ways that devices can be better anthropomorphized in order to elicit empathy from users. Read More: Study Suggests What the Human-Robot Relationship Looks Like By now, you may have heard \u2014 possibly from the same people creating the technology \u2014 that artificial intelligence might one day kill us all. The specifics are hazy, but then, they don\u2019t really matter. Humans are very good at fantasizing about being exterminated by an alien species, because we\u2019ve always been good at devising creative ways of doing it to our fellow creatures. AI could destroy humanity for something as stupid as, in philosopher Nick Bostrom\u2019s famous thought experiment, turning the world\u2019s matter into paper clips \u2014 much like humans are now wiping out our great ape cousins, orangutans, to cultivate palm oil to make junk foods like Oreos. You might even say that the human nightmare of subjugation by machines expresses a sublimated fear of our treatment of non-human animals being turned back on us. \u201cWe know what we\u2019ve done,\u201d as journalist Ezra Klein put it on a May episode of his podcast. \u201cAnd we wouldn\u2019t want to be on the other side of it.\u201d AI threatens the quality that many of us believe has made humans unique on this planet: intelligence. So, as author Meghan O\u2019Gieblyn wrote in her book God, Human, Animal, Machine , \u201cWe quell our anxiety by insisting that what distinguishes true consciousness is emotions, perception, the ability to experience and feel: the qualities, in other words, that we share with animals.\u201d We tell ourselves, in other words, that even if AI may one day be smarter than us, unlike the machines, we have subjective experience, which makes us morally special. The obvious problem with this, though, is that humans aren\u2019t special in this way. Non-human animals share many of our capacities for intelligence and perception, yet we\u2019ve refused to extend the generosity we might expect from AI. We rationalize unmitigated cruelty toward animals \u2014 caging, commodifying, mutilating, and killing them to suit our whims \u2014 on the basis of our purportedly superior intellect. \u201cIf there were gods, they would surely be laughing their heads off at the inconsistency of our logic,\u201d O\u2019Gieblyn continues. \u201cWe spent centuries denying consciousness in animals precisely because [we thought] they lacked reason or higher thought.\u201d Why should we hope that AI, particularly if it\u2019s built on our own values, treats us any differently? We might struggle to justify to a future artificial \u201csuperintelligence,\u201d if such a thing could ever exist, why we\u2019re deserving of mercy when we\u2019ve failed spectacularly at offering our fellow animals the same. And, worse still, the dehumanizing philosophy of AI\u2019s prophets is among the worst possible starting points to defend the value of our fleshy, living selves. Transhumanism is built on a hatred of animality Although modern humans defend the exploitation of non-human animals in terms of their assumed lack of intelligence, this has never been the real reason for it. If we took that argument at face value and treated animals according to their smarts, we would immediately stop factory-farming octopuses, which can use tools, recognize human faces, and figure out how to escape enclosures. We wouldn\u2019t keep elephants in solitary confinement in zoos, recognizing it as a violation of their rights and needs as smart, caring, deeply social creatures. We wouldn\u2019t psychologically torture pigs by immobilizing them in cages so small they can\u2019t turn around, condemning them to a short lifetime essentially spent in a coffin, all to turn them into cheap cuts of bacon. We would realize that it\u2019s wholly unnecessary to subject intelligent cows to the trauma of repeated, human-induced pregnancies and separation from their newborns, just so we can drink the milk meant for their calves. In reality, we aren\u2019t cruel to animals because they\u2019re stupid; we say they\u2019re stupid because we\u2019re cruel to them, inventing fact-free mythologies about their minds to justify our dominance, as political theorist Dinesh Wadiwel lays this out in his brilliant 2015 book The War Against Animals . In a chapter called \u201cThe Violence of Stupidity,\u201d Wadiwel contends that human power over animals enables us to be willfully and unaccountably stupid about what they are really like. \u201cHow else might we describe a claimed superiority by humans over animals (whether based on intelligence, reason, communication, vocalisation, or politics) that has no consistent or verifiable \u2018scientific\u2019 or \u2018philosophical\u2019 basis?\u201d he writes. Humans, like animals, are vulnerable, breakable creatures who can only thrive within a specific set of physical and social constraints. We can only hope that future AI, however intelligent, doesn\u2019t evince the same stupidity with respect to us. While we can only guess whether some powerful future AI will categorize us as unintelligent, what\u2019s clear is that there is an explicit and concerning contempt for the human animal among prominent AI boosters. AI research itself has strong ties to transhumanism, a movement that aims to radically alter and augment human bodies with technology. Its most extreme aspirants hope to merge humanity with computers, excising suffering from life like a tumor from a cancer patient and living in a state of everlasting bliss, as Bostrom, one of the main proponents of transhumanism, has suggested. Elon Musk, for instance, has said that he launched Neuralink, his brain-computer interface startup, in part so that humans can remain competitive in an intelligence arms race with AI. \u201cEven under a benign AI, we will be left behind,\u201d Musk said at a Neuralink event in 2019. \u201cWith a high bandwidth brain-machine interface, we will have the option to go along for the ride.\u201d This aspiration can be interpreted as an implicit loathing of our animality, or at least a desire to liberate ourselves from it. \u201cWe will be the first species ever to design our own descendants,\u201d technologist Sam Altman, now the CEO of OpenAI, wrote in a 2017 blog post. \u201cMy guess is that we can either be the biological bootloader for digital intelligence\u201d \u2014 meaning just a stepping stone for advanced AI \u2014 \u201cand then fade into an evolutionary tree branch, or we can figure out what a successful merge looks like.\u201d Computer scientist Danny Hillis, co-founder of the now-defunct AI company Thinking Machines, declared in the early \u201990s that humans are composed of two fundamentally different things: \u201cWe\u2019re the metabolic thing, which is the monkey that walks around, and we\u2019re the intelligent thing, which is a set of ideas and culture,\u201d as historian David Noble quotes in his 1997 book The Religion of Technology . \u201dWhat\u2019s valuable about us,\u201d Hillis continued, \u201cwhat\u2019s good about humans, is the idea thing. It\u2019s not the animal thing.\u201d Merging with computers signifies our extrication from animal biology. This human/animal dualism posits a clean cognitive break between us and the rest of the animal evolutionary tree, when in fact no such division exists. It relies on an implausible model of human intelligence as having nothing to do with our physical, animal selves: a notion that \u201cthe mind is computation, that it does not involve the affective dimensions of the human experience, and it doesn\u2019t involve the body,\u201d Michael Sacasas, a technology critic who writes The Convivial Society, a popular Substack, told me. The societal reckoning taking place now over where humans fit in a world of AI, might, as Sacasas hopes, prompt us to start to rethink this dualism, to recognize that the body is \u201cnot just as the firmware for the rational software, but actually an integral part of what we call \u2018mind.\u2019\u201d Breaking down that dualism ought to also mean giving up the separate status we assign ourselves as human beings. It could help us broaden the definition of intelligence itself to encompass the animal qualities described by O\u2019Gieblyn \u2014 \u201cemotions, perception, the ability to experience and feel.\u201d There is, after all, no single thing in our brains called \u201cintelligence\u201d or \u201cthought\u201d; it\u2019s not a body part, but an emergent property continuous with our other mental processes. Animals share these, and in some cases exceed them. Migratory birds, for example, can famously navigate by perceiving the Earth\u2019s magnetic field. Raccoons can \u201csee\u201d and learn about the world with their hyper-sensitive hands (this is why they can sometimes be seen enthusiastically patting objects and other animals). Pigs are undoubtedly smart, but the widely cited idea that they\u2019re \u201cas smart as\u201d 3-year-old children reflects the depressing way that we\u2019ve come to measure intelligence against a single-variable, anthropocentric yardstick, rather than recognizing different beings as having different minds. Yet this is dehumanizing to us, too, because it judges our cognition as though it were a computer\u2019s CPU. If we can properly value animals\u2019 capacities, then we might also see how claiming human exceptionalism through a disembodied view of our minds has done spiritual harm to ourselves. AI criticism ought to include non-human animals You don\u2019t have to believe that AI could become autonomous and orchestrate our extinction to see how, for example, chatbots are already blurring the line between humans and machines, creating the illusion of sentience where it doesn\u2019t exist, a critique made by linguist Emily Bender. Others, like Sacasas, point to how AI replacing humans represents the culmination of modernity\u2019s drive to eliminate inefficiency from life. \u201cBy the logic of the market and of techno-capitalism, if you like, the inefficiencies of the human being were always ultimately meant to be disposed of,\u201d he said. \u201cAI, in a sense, just kind of furthers that logic \u2026 and brings it to its logical conclusion, which is, you\u2019re just getting rid of people.\u201d These kinds of critiques ring true to me \u2014 yet they also have a way of fixating on the ethical and spiritual uniqueness of human beings, to the exclusion of the other sentient, intelligent creatures with whom we\u2019ve always shared the planet. \u201cOne of the anxieties generated by AI is built upon how we have sought to distinguish the human, or to elevate the human, or to find the unique thing about the human,\u201d Sacasas points out. Humans are, in important ways, obviously unique among animals. But the critical discourse about AI has shown little interest in thinking beyond ourselves, or reckoning with what implications this moment has for our undervaluing of animals. One of the best-known critiques of AI large language models, or LLMs, for example, compares AI\u2019s lack of language understanding to that of an animal: the concept of the \u201cstochastic parrot,\u201d which refers to how chatbots, not having minds, spit out language based on probabilistic models with no regard for meaning. \u201cYou are not a parrot,\u201d proclaimed the headline of a widely read March profile of Emily Bender in New York magazine. I\u2019m sure Bender has nothing against parrots \u2014 exceptionally smart animals that are thought to reproduce sounds with astonishing fidelity as part of their communication with one another and with humans. But parrots aren\u2019t machines, and imagining them as such only reinforces the human/animal dualism that gave us the disembodied view of our own minds. It\u2019s as if we have no language for affirming our worth as humans without repudiating animality. The ascendance of AI should be a pivotal moment from which to start to come to grips with our relationship to other sentient, biological life. If AI were ever in a position to make judgments about us, we should hope that it\u2019s far more charitable than we have been, that it doesn\u2019t nitpick, mock, or nullify our capacities and needs as we\u2019ve done to other animals. If we wouldn\u2019t want to be tyrannized by a more powerful intelligence, we have no credible defense for continuing to do the same. We don\u2019t know if sentient AI is possible, but if it is, we shouldn\u2019t build it None of this necessarily tells us whether the machines themselves could ever become sentient, or how we should proceed if they can. I used to find the idea of sentient AI risible, but now I\u2019m not so sure. The scientific method has not figured out how to explain consciousness, as O\u2019Gieblyn points out. Modern science, she writes, \u201cwas predicated in the first place on the exclusion of the mind.\u201d If we don\u2019t know where consciousness comes from, we may want to be careful about assuming it can only arise from biological life, especially given our poor track record of appreciating it in animals. \u201cEvolution was just selecting repeatedly on ability to have babies, and here we are. We have goals,\u201d as Vox\u2019s Kelsey Piper said on The Ezra Klein Show in March. \u201cWhy does that process get you things that have goals? I don\u2019t know.\u201d We have no reason to believe any current AIs are sentient, but we also have no way of knowing whether or how that could change. \u201cWe\u2019re kind of at the point where we can make fire but do not even have the rudiments of what we\u2019d need to understand it,\u201d my friend Luke Gessler, a computational linguist, told me. If sentience in AI could ever emerge (a big if), I\u2019m doubtful we\u2019d be willing to recognize it, for the same reason that we\u2019ve denied its existence in animals. Humans are very good at dismissing or lying about the interests of beings that we want to exploit (including not just animals but also, of course, enslaved humans, women, and any other class of people who have been excluded from moral consideration). Creating sentient AI would be unethical because we\u2019d be bringing it into the world as chattel. Consigning sentient beings to property status, as we know from the experience of non-human animals, is inherently unjust because their welfare will always be subordinated to economic efficiency and the desires of their owners. \u201cWe will inevitably inflict suffering on them,\u201d science fiction author Ted Chiang said of building sentient AI in 2021. \u201cThat seems to me clearly a bad idea.\u201d In a May essay, Columbia philosopher Dhananjay Jagannathan offered a different perspective on the AI minds question. Drawing from Aristotle, he suggests that the nature of thought isn\u2019t something that can be scientifically deduced or implanted into a computer, because it\u2019s an irreducible part of our lives as biological animals. \u201cThinking is life,\u201d the Aristotelian idea puts it. A raccoon who pats things to learn about her environment, for example, or a baby bird who pecks around at objects to do the same, or a human whose sense of smell vividly triggers a distant memory are all having experiences of thinking that are inextricable from the biological organs through which they\u2019re engaging with the world. One upshot of this, Jagannathan writes, is that the transhumanist dream of digitally uploading our consciousness and splitting from our bodies, far from being any sort of liberation, amounts to \u201cself-annihilation.\u201d The idea of thinking as inseparable from animality can be hard for modern people to comprehend because, as O\u2019Gieblyn writes, our concept of the mind pulls so heavily from computational metaphors. Because we imagine our cognition as a computer, we start to imagine, erroneously, that computers can think. AI evokes our anxieties about the fragility and mistreatment of animality Jagannathan\u2019s view, that we can understand thought through our kinship with non-human animals, helps clarify what is disconcerting about the dualist, computational view of experience, taken to its logical endpoint by AI and transhumanist philosophy. The assumption that we can apprehend, measure, and perfect subjective experience, rendering life as though it were bits of information encoded on a computer, can lead to conclusions that are obviously repugnant. It has made the annihilation of biological life, both human and non-human, imaginable. Prominent philosopher Will MacAskill, for example, proposed in his 2022 book What We Owe the Future that declining populations of wild animals (we are, if you haven\u2019t heard, in the middle of a mass extinction) may actually be desirable. Their lives might be \u201cworse than nothing on average, which I think is plausible (though uncertain),\u201d he writes, because they may consist more of suffering, from things like predation and disease, than of pleasure. Perhaps, then, they\u2019d be better off if they\u2019d never been born \u2014 an argument that springs from the same well as the transhumanist impulse to remove suffering from life and colonize the universe with beings merged with machines. The idea of wild animal eradication represents one of the more extreme manifestations of the drive to denude life of physical content. In a similar vein, transhumanist philosopher David Pearce, who sits on the board of the organization Herbivorize Predators (it aims to do what the name implies), hopes to technologically \u201celiminate all forms of unpleasant experience from human and non-human life, replacing suffering with \u2018information-sensitive gradients of bliss.\u2019\u201d In the actual world, where wild animals are often exterminated wholesale when their presence is inconvenient for us, the notion that it could actually be morally righteous to get rid of them might provide a justification for the ecocide that humans are engaged in anyway. Who\u2019s to say that an AI won\u2019t one day say the same thing about us, deciding that it\u2019s best to put us out of our misery based on its cold calculation of our pains and pleasures? That would be consistent with the transhumanist ethos of transcending the hardship of physical existence. Yet this dim estimation of our biological selves, as well as those of animals, forecloses the possibility of valuing or interpreting life in other ways. We can hardly access an animal\u2019s interiority, much less be able to say whether they think their lives are worth living. If a utilitarian bean counter told me that the rest of my life would be 70 percent suffering, I wouldn\u2019t choose to die, even if I truly believed them; I would want to live out my life. A very different, more integrated interpretation of animal life, one that I return to again and again, can be found in a work by the poet Alan Shapiro. His 2002 poem \u201cJoy\u201d gives expression to the strange entanglement of joy, fear, and tragedy that defines our lives, and, he imagines, perhaps those of wild animals also. \u201cJoy,\u201d he writes, is the thing that is \u201cSavagely beautiful,\u201d likening it to antelope evading a lion: This vision doesn\u2019t, to me, suggest that the suffering of wild animals doesn\u2019t matter, but rather that the vulnerable, mysterious fullness of their lives is worth living. AI evokes our anxieties about the fragility and mistreatment of animality \u2014 our own, as well as that of nonhuman animals. It reminds us of our own vulnerability, the parts of us that are unfathomable or expendable in mechanistic terms. In a world where the ability to manipulate language is no longer a uniquely human capacity, the rationalizing impulse might ask us to co-sign our own obsolescence. We might, instead, decide that our creaturely selves are worth holding on to, and, in doing so, invite our fellow animals into our moral circle. Will you support Vox\u2019s explanatory journalism? Most news outlets make their money through advertising or subscriptions. But when it comes to what we\u2019re trying to do at Vox, there are a couple of big issues with relying on ads and subscriptions to keep the lights on: First, advertising dollars go up and down with the economy. We often only know a few months out what our advertising revenue will be, which makes it hard to plan ahead. Second, we\u2019re not in the subscriptions business. Vox is here to help everyone understand the complex issues shaping the world \u2014 not just the people who can afford to pay for a subscription. We believe that\u2019s an important part of building a more equal society. And we can\u2019t do that if we have a paywall. So even though advertising is still our biggest source of revenue, we also seek grants and reader support. (And no matter how our work is funded, we have strict guidelines on editorial independence.) If you also believe that everyone deserves access to trusted high-quality information, will you make a gift to Vox today? Any amount helps. Yes, I'll give $5 Yes, I'll give $5 There are alien minds among us. Not the little green men of science fiction, but the alien minds that power the facial recognition in your smartphone, determine your creditworthiness and write poetry and computer code. These alien minds are artificial intelligence systems, the ghost in the machine that you encounter daily.But AI systems have a significant limitation: Many of their inner workings are impenetrable, making them fundamentally unexplainable and unpredictable. Furthermore, constructing AI systems that behave in ways that people expect is a significant challenge.If you fundamentally don\u2019t understand something as unpredictable as AI, how can you trust it?Why AI is unpredictableTrust is grounded in predictability. It depends on your ability to anticipate the behavior of others. If you trust someone and they don\u2019t do what you expect, then your perception of their trustworthiness diminishes.Many AI systems are built on deep learning neural networks, which in some ways emulate the human brain. These networks contain interconnected \u201cneurons\u201d with variables or \u201cparameters\u201d that affect the strength of connections between the neurons. As a na\u00efve network is presented with training data, it \u201clearns\u201d how to classify the data by adjusting these parameters. In this way, the AI system learns to classify data it hasn\u2019t seen before. It doesn\u2019t memorize what each data point is, but instead predicts what a data point might be.Many of the most powerful AI systems contain trillions of parameters. Because of this, the reasons AI systems make the decisions that they do are often opaque. This is the AI explainability problem \u2013 the impenetrable black box of AI decision-making.Consider a variation of the \u201cTrolley Problem.\u201d Imagine that you are a passenger in a self-driving vehicle, controlled by an AI. A small child runs into the road, and the AI must now decide: run over the child or swerve and crash, potentially injuring its passengers. This choice would be difficult for a human to make, but a human has the benefit of being able to explain their decision. Their rationalization \u2013 shaped by ethical norms, the perceptions of others and expected behavior \u2013 supports trust.In contrast, an AI can\u2019t rationalize its decision-making. You can Why can't we stop anthropomorphizing our animal friends and foes? It\u2019s not the most urgent news story that\u2019s gripped the world since 2020, but it might be the weirdest: The last three years have seen more 400 \u201cencounters\u201d\u2014 many reports have used the word \u201cattacks\u201d\u2014between orca whales and boats in the Strait of Gibraltar. Because the orcas are particularly fond of tearing the propellers off of yachts, the temptation to characterize these six-ton, pack-hunting, demonstrably intelligent mammals as class warriors fighting back against the 1 percent is strong, and the memes have been fun . But trying to understand animal behavior in human terms is a mistake. On our latest episode of the Smithsonian magazine podcast \u201cThere\u2019s More to That,\u201d I talk with Carlyn Kranking, Smithsonian\u2019s assistant digital science editor, about why stories about animal behavior are so popular with our readers, and how she decides which ones deserve more scrutiny. Then, I speak with Lori Marino, a biopsychologist with a specific focus on whale and dolphin intelligence, about what\u2019s really happening between the orcas and the yacht set. A transcript is below. To subscribe to \u201cThere\u2019s More to That\u201d\u2014and to listen to past episodes on J. Robert Oppenheimer , the vanishing Colorado River, the OceanGate Titan disaster and more\u2014find us on Apple Podcasts , Spotify or wherever you get your podcasts. Chris Klimek: Think about the late \u201980s: the big shoulder pads, the bigger hair, maybe some leg warmers or very high-waisted pants. Carlyn Kranking, one of our science editors here at Smithsonian magazine, told us that young orca whales were also fashion victims back then. Carlyn Kranking: There was a time where juvenile orcas in one particular population would swim around with a dead fish on their heads. It was in the Puget Sound area in the Pacific. One orca apparently started it, and over the next few weeks, other orcas were doing the same, wearing dead salmon like hats, and then the trend stopped. It was just passing, so it was just a short-term fad. It happened apparently in 1987. Klimek: Well, we were all wearing fish on our head back then. Kranking: Yeah. But there are lots of things that orcas do that are just kind of interesting and show a lot of social behavior between them. Klimek: This kind of pattern in orca behavior is one possible explanation for why the whales seem to have spent this summer attacking boats off the coast of Spain. You may have heard about this, because it was reported that way all over the internet. Kranking: And the other part of it is that some of these boats are not just any old boats. Some of them are yachts, which are affiliated with the ultra-rich, and so people have been framing this orca as someone who\u2019s an \u201corca of the people\u201d fighting back against the rich. There\u2019s merch out there on the internet that people have created on Etsy and stuff where it\u2019s just stickers or T-shirts with slogans such as \u201cSink the Rich,\u201d or \u201cLet\u2019s Go, Girls,\u201d because of the female-led uprising. Klimek: A female-led uprising, because allegedly this entire boat-ramming orca fad was started by one female whale, according to one scientist\u2019s theory. Kranking: One orca, whose name is White Gladis, might have a critical moment of agony with a boat, which could have been some negative interaction that harms the orca, and now as a result, she is ramming into boats. And it\u2019s possible that the younger orcas saw this happen and started imitating her behavior, and that could possibly be how it spread. Klimek: Carlyn works on a lot of animal news stories at Smithsonian. She says they\u2019re very popular with readers. Kranking: Especially ones where animals are doing something out of the ordinary or something unexpected for them. Even things where animals are interacting with human technology or human society, those tend to do really well. And in the case of the orcas, I think it goes a little bit deeper, because it\u2019s not just that the orcas are interacting with human technology, it\u2019s that they are allegedly interacting with it intentionally and possibly because of a negative experience with it. We don\u2019t necessarily know this for a fact, but I think that that idea is what has really gotten people intrigued into this story. Klimek: We don\u2019t know for a fact, and we\u2019re going to meet a scientist this episode who will explain why. With stories like this, there is a tendency to project human feelings and emotions onto animal behavior. So journalists covering animal stories have to be careful about the language they use. Kranking: A lot of the stories have been portraying the orcas as attacking the boats, which definitely not only implies intention, but also malice. It really makes people think that the orcas are out there to take people down. And in a way that could be damaging to the animal\u2019s perception, but I feel like people are actually really aligning themselves with the orcas and getting behind that as opposed to seeing it as a bad thing. A lot of people are kind of cheering them on from the internet. Klimek: From Smithsonian magazine and PRX Productions, this is \u201cThere\u2019s More to That,\u201d a show where experts tell us the real deal behind stories that go viral. On today\u2019s episode: Can a whale be a vigilante? Can a sea otter really love surfing? What are these animals really doing? I\u2019m Chris Klimek. Let\u2019s find out. Klimek: The orcas aren\u2019t the only animals that have gotten a lot of press lately. Kranking: There is this one case in California off the coast of Santa Cruz where there\u2019s actually an otter called Otter 841. She has been approaching people on surfboards and not just coming close to them, but actually gripping onto the surfboards, biting them and, in at least one case, climbing on top of the surfboard and kind of like doing a little surfing. Chris Klimek: Disrupting human recreation seems to run in Otter 841\u2019s family. Kranking: She was actually born in captivity to a mother who had been captive before, then was released, then was actually removed from the wild because she had been approaching humans in kayaks. So this otter is maybe repeating some of that behavior, because now, after she was released to the wild a few years ago, she has been interacting with surfers. And so, as a result, now the local aquarium officials are trying to capture her and bring her back into captivity for her own safety and for people\u2019s safety. Klimek: Elsewhere on the internet this summer, a lot of people were following another big story about European crows and magpies. Kranking: These birds were building their nests with anti-bird spikes\u2014those metal sticks that they put on buildings and stuff to keep birds away from them. But the birds would actually pry them up and put them in their nests. The media has been framing this as the \u201cperfect comeback\u201d for the birds. And the scientist also has said the birds are outsmarting us. So this, perhaps to a lesser extent than the otter and the orcas, has also been a case where people are looking at the animals kind of fighting back in a way against human technology and using it for their own use. Klimek: Animals aren\u2019t humans, but when we try to understand their behavior, we just can\u2019t seem to stop assigning them human motives. Lori Marino: The big, bad, dirty word in science is \u201canthropomorphism.\u201d Klimek: Dr. Lori Marino is a neuroscientist who studies animal behavior and intelligence, particularly that of dolphins and whale. You may remember her from the 2013 documentary Blackfish. Marino: You don\u2019t want to attribute human characteristics to other animals who don\u2019t have those characteristics, but that doesn\u2019t mean that everything you interpret is anthropomorphism. Klimek: Today, Lori is president of the Whale Sanctuary Project, an organization that creates humane coastal enclosures for different types of whales, but she spent most of her career studying marine mammal behavior. In fact, she co-authored a study that proved bottlenose dolphins could recognize themselves in a mirror. Marino: Because we share the brain structures that have to do with emotions, memory and many other capacities, it\u2019s not necessarily anthropomorphism to recognize shared abilities and characteristics between ourselves, dolphins, and whales and other mammals. Klimek: We thought Lori could help us understand not just what\u2019s going on with these headline-making animals, but also why we seemed so determined to assign human motivations to the things that animals do. So, surely you\u2019ve seen the coverage of the orca encounters near the Strait of Gibraltar. Marino: Oh, yeah. Klimek: What are you thinking as you read these stories? Marino: Well, as I read these stories, I thought to myself, \u201cWow, we are projecting our psychology on them.\u201d I think it\u2019s really interesting that the meme that has evolved around these orcas is that they\u2019re out for revenge. And so what that means is that we think we\u2019ve done something to them that warrants revenge. We\u2019re dumping our guilts and our projections onto them. There\u2019s no evidence at all whatsoever that they\u2019re out for revenge. If they were, it would be a bigger deal than it is. They\u2019re just having a good time playing with rudders. They find it interesting. These are big-brained animals. They\u2019re going to explore, they\u2019re going to play around with things, and we shouldn\u2019t be so defensive about it. We definitely carry the weight of what we\u2019ve done to orcas and other animals. We\u2019ve killed them, we have taken them captive and put them in tanks. So we are very aware that we haven\u2019t had a good relationship with these animals generally. We\u2019ve done things to them that we may not be proud of, but that\u2019s on us. That\u2019s a projection of who we are. The media has totally sensationalized this whole story to make it a \u201cus versus them\u201d story, the orcas versus the humans, and that\u2019s not really what\u2019s going on. Klimek: It\u2019s also been framed as kind of a revenge on the 1 percent as well, right? And the assumption these are super rich people on these yachts? Marino: Oh my gosh, that is so ridiculous. Klimek: So some stories have described these, I\u2019ve been using the word \u201cencounters,\u201d and some have used the word \u201cattacks.\u201d Marino: Yeah, they\u2019re not attacks. If they were attacks, there would be a lot more damage done. People would be harmed. If an orca wants to harm you, that orca will harm you. You don\u2019t have a choice in the matter. So these are not attacks. These are, again, orcas finding something interesting and fun to play with. It\u2019s a behavioral fad. We know they do all kinds of things like that. They put things on their head, and they swim around with things, and it\u2019s just what you do when you\u2019re in an environment where you\u2019ve got lots of stuff to play with and interact with. Klimek: We\u2019re all particularly obsessed with this history of orcas wearing salmon on their heads. Can you tell us more about that, please? Marino: There\u2019s all kinds of things that they do. There was an orca group that used to put salmon on their head and just swim around with it. That is a fad, because I think it disappeared after a while. But if a fad gets entrenched in a community and then passed on from one generation to the next, it becomes cultural. And that\u2019s how we see all of these different orca cultures around the world, off the coast of New Zealand, the way orcas live and their dialect, very different from the way they live in British Columbia or in the Salish Sea in Washington. They each have their own culture, and this has been studied extensively. These are true cultures. Klimek: So this means that the evidence shows they really are social animals, right? That\u2019s not our projection. Marino: Oh, they are. Klimek: We can verify that. Marino: Very social. Their social bonds, held together by strong emotions and the need to stay together, are extremely strong, and that\u2019s why it\u2019s so important to keep them in the wild, to not take them away from their families, because that is a huge harm to them psychologically, emotionally. Klimek: Did you read about the otter in Santa Cruz commandeering the kid\u2019s surfboard? Marino: Yes, that otter. Again, the otter, somehow considered an individual, is taking revenge. That otter used to be in captivity and was released, and that is why he has, I don\u2019t know if it\u2019s a he or she, has such an interactive nature with humans. It\u2019s not just an otter out of the wild. And again, there is this whole \u201cus versus them.\u201d What are we going to do about the otter who steals surfboards? Well, nothing. If you don\u2019t want your surfboard being stolen by an otter, go somewhere else. It\u2019s silly. It really is. Klimek: Yeah. I mean, that video, the otter appeared quite deliberate, he really wanted that surfboard. Marino: He certainly did. It is deliberate. That doesn\u2019t mean that the otter is out to get all humans on behalf of all other otters. We sensationalize things. We make stories up about why animals are doing things, and those stories reflect our psychology. Klimek: Do dolphins and whales have emotions like we do? Marino: Yes. Klimek: How do we know this? Marino: We know this for a number of reasons. One is that we have the same brain structures that are involved in processing emotions as dolphins and whales and other mammals. There\u2019s a part of the brain called the limbic system, and that\u2019s the part of the brain where emotions like anger and affection are processed, as well as memory. And so we share that system with dolphins and whales. And so I think that it is parsimonious to say that because we have the same structures that have to do with emotional processing, that we are experiencing emotions the same way they are. We also know from their behavior that they respond in ways that we would respond. Remember the case of Tahlequah, the orca whose baby died and she carried her deceased baby for 17 days. And that became a worldwide sensation. I think it was sort of a wake-up call for people to realize, \u201cOK. Yeah, she\u2019s a mother, she\u2019s grieving her dead infant. We get that.\u201d And everyone felt that. That\u2019s not anthropomorphism, that\u2019s just recognizing the fact that we have a similar brain and we\u2019re going to react similarly under similar circumstances. That doesn\u2019t mean we\u2019re identical. We are in a sense, different versions of a mammal. We\u2019re not completely alien from them. Klimek: Do you think that should make us recognize that we are more connected to these creatures than we think, even though we don\u2019t live in the ocean, but feeling and expressing an emotion like grief? Marino: Absolutely. And we shouldn\u2019t be surprised when they show those kinds of emotions, and we should hopefully feel more of a connection and compassion for them, because they are not so strange that we can\u2019t possibly know what they\u2019re thinking and feeling. We can know. We can look at their brain, we can look at their behavior, we can look at the evolutionary history of mammals, we can look at basic biology and make inferences about what it\u2019s like to be an orca or a bottlenose dolphin or a beluga or a tiger or a human. These are just different versions of the same thing. The brains of dolphins and whales are both similar and different from those of ourselves. And those similarities ensure that there is overlap in psychology. I mean, the past few years has been really interesting in the field of animal behavior and cognition. We\u2019re finding more and more shared characteristics with other animals. And I\u2019m not talking about big-brain mammals. I\u2019m talking about birds, I\u2019m talking about insects. And it\u2019s really interesting, because we\u2019re finding that a lot of the traits that we thought were human and unique are not, and we\u2019re having to come to a reckoning with that. What does that mean that bees, for instance, show social learning, show memory, show evidence of play behavior? What does that mean? The work has to be done to really understand what that is and how that reflects biology. Klimek: I mean, as humans, we often talk about our emotions as things that hold us back or get in the way or things that we have to overcome. But is there any evidence that these social interactions or emotional behavior, is there any evolutionary benefit to the extent that we can actually help these animals survive? Marino: I can tell you this: Without emotions, there would be no life. And if you trace the evolutionary history of the brain, the nervous system, even as far back as the kinds of excitable membranes that single-celled animals have, you can see a common thread. The first thing that brains do is they interpret and drive behavior, and that behavior is driven by emotion. People who have problems with their limbic system, for instance, who don\u2019t have the right kind of emotional processing, they can\u2019t make a decision. So this is not just about feeling, this is about acting and behaving and deciding and motivated behavior, emotion, motion. So emotion serves to allow all organisms to transfer through their environment and make the right kinds of decisions about how to survive, what feels good, what doesn\u2019t feel good, what memory do you have of a place where something bad happened? These are really basic things, and that is the real basic purpose of emotion. That\u2019s not something that is unique to humans. All animals with brains have emotions, because they have to. Klimek: Are there things that humans do that invite these responses? By bringing my yacht into a place that orcas like, am I inviting a response? Marino: I don\u2019t know if you\u2019re inviting a response, because you never know what is going to interest them. I have colleagues who swim with orcas all the time. I\u2019ve been in small dinghies surrounded by orcas. How those animals respond just depends upon who they are, what they find interesting, what they want to do that day. It does mean respect and distance. You don\u2019t want to chase these animals down, you don\u2019t want to crowd them out. But if you are on the water and you\u2019re keeping a respectful distance, then you really can\u2019t tell what it is that\u2019s really going to get them jazzed up. Klimek: So I know a big part of your work is the effect that captivity itself has on wildlife. Marino: Yeah. Klimek: How are human and animal interactions different in a controlled environment like that versus in the wild? Marino: Oh my gosh, they\u2019re entirely different. When you do field work and you meet a dolphin or whale in the wild, you realize how different they are from the individuals who are living in concrete tanks and performing tricks and so forth. The animals in the tanks are trying to eke out some kind of an existence in a really impoverished, barren, intrusive environment. And they may be starved for attention from you, they may be aggressive, they are totally stressed out, and that shapes their behavior toward you. When it comes to animals in the wild, what you learn is that it\u2019s not all about us. They may not care less about what we\u2019re doing or who we are. They\u2019re leading their lives, spending their days doing what they want. You\u2019re observing, and you\u2019re not a big deal. Klimek: When animals do things that we interpret as destructive\u2014taking my surfboard away\u2014does that interfere with conservation efforts? Does it interfere with our ability to sympathize with them and think we should treat other species better than we are? Marino: It interferes with conservation efforts and compassion toward them if we blow it out of context. If we\u2019re going to let an otter taking a surfboard or a few orcas dismantling a rudder affect our compassion and our desire to conserve and protect them, then that\u2019s on us. I would not want to have a rudder destroyed on my boat, but compared to what we do to them, it\u2019s nothing. It\u2019s nothing. What I do fear is that people will start to take this \u201cus versus them\u201d meme and perhaps try to harm the orcas to scare them away from doing these things. That\u2019s a real fear that I have, because our species often tends to the violent and aggressive when we can\u2019t get the change that we want, and that would be tragic. Klimek: Yeah. What about when we map these characteristics or motivations onto specific types of animals? Are we going to be talking for the next five years about how aggressive otters are? Or, even if we ascribe an altruistic motive or something to another type of animal, does suggesting broadly that this species is like this, do you see that as a problem for conservation efforts? Marino: Well, it\u2019s always a problem when you say, \u201cA species is like this,\u201d because they\u2019re all individuals. It\u2019s just like humans, right? You can\u2019t say all humans are like A, B and C. And so we really can\u2019t make those kinds of statements about them. Conservation efforts have for a long time been all about the numbers: \u201cLet\u2019s count how many orcas, how many zebras, how many elephants are there, and let\u2019s look at the numbers and see if those numbers are going up or down.\u201d But it\u2019s not a numbers game. This is about individuals. So when you look at a group of orcas or bottlenose dolphins somewhere in the ocean, it\u2019s not just how many there are, but what is their culture? And if we recognize the fact that they do have culture, wiping them out is not just a numbers game. It\u2019s about wiping out an entire culture from the face of the planet. There\u2019s so much more going on than just a group of animals. Oh, there\u2019s 150. But who are they? And there\u2019s a lot of interesting science that\u2019s been done called social network analysis showing that how you interact with a group of dolphins, for instance, determines the viability of that group. If you, for instance, go in and you try to capture a dolphin who is on the outskirts of his group, that may not have that much of an impact on the group itself, although it will on the individual. But if you go and you tag or you capture a dolphin who\u2019s a real strong node in that social network, you could dismantle the whole group. Klimek: Are there any benefits to when we try to frame animal behavior in human terms? Is it possible that anything good could come from that? Marino: Well, again, framing animal behavior in human terms is not necessarily what we want to do, but what we do want to do is recognize the shared capacities between humans and other animals. The continuity and evolution is not about making animals into Disney cartoon characters and saying that they have all the same desires as a human, but at the same time, if we acknowledge the science tells us that they have emotions, that they have certain capacities, and we have to recognize that. So again, it\u2019s like keeping the similarities and the differences in mind, both at the same time. Klimek: Boy, I love the Pixar movie Ratatouille, and I can\u2019t say it\u2019s made me any more sympathetic toward the rats in my building. Marino: Nothing wrong with Disney, but I think that we have to realize that other animals have their own lives to live. It is not a life that\u2019s in reference to our life, and we have to respect that. Klimek: Dr. Marino, thank you so much for talking to us. This has been completely fascinating. Marino: Thank you so much. It\u2019s been fun. Klimek: To learn more about Dr. Lori Marino\u2019s Whale Sanctuary Project, check out the link in our show notes. And to read all of our animal news coverage, head to SmithsonianMag.com. Before we let Lori go, we asked her if she had a dinner party fact for us. This is the segment we like to close the show with. We give you a small piece of knowledge to pull out the next time your supper conversation runs dry, and lucky for you, Lori served us all a little slice of humble pie. Marino: As humans, we like to say that our brain is the most elaborated brain on the planet, that we have the most wrinkled neocortex on the surface of all primates, but actually the most neocorticalized\u2014in other words, the brain that\u2019s the most elaborated in terms of thinking with the most convolutions on the surface\u2014is not humans, it\u2019s orcas. So if you really want to look at objective measures of brain evolution and elaboration, we don\u2019t come out on top on every measure, and so maybe we shouldn\u2019t have so much hubris. Klimek: \u201cThere\u2019s More to That\u201d is a production of Smithsonian magazine and PRX Productions. From the magazine, our team is me, Debra Rosenberg and Brian Wolly. Fact-checking by Stephanie Abramson. From PRX, our team is Jessica Miller, Genevieve Sponsler, Adriana Rozas Rivera, Ry Dorsey and Edwin Ochoa. The executive producer of PRX Productions is Jocelyn Gonzales. Our episode artwork is by Emily Lankiewicz. Music is from APM Music. I\u2019m Chris Klimek. Thanks for listening. Get the latest Science stories in your inbox. Recommended Videos Filed Under: Biology, Birds, Conservation, Mammals, There's More to That, Whales, wildlife Credit: peshvov/Getty Images Soon after Alan Turing initiated the study of computer science in 1936, he began wondering if humanity could one day build machines with intelligence comparable to that of humans. Artificial intelligence, the modern field concerned with this question, has come a long way since then. But truly intelligent machines that can independently accomplish many different tasks have yet to be invented. And though science fiction has long imagined AI one day taking malevolent forms such as amoral androids or murderous Terminators, today\u2019s AI researchers are often more worried about the everyday AI algorithms that already are enmeshed with our lives\u2014and the problems that have already become associated with them. Even though today\u2019s AI is only capable of automating certain specific tasks, it is already raising significant concerns. In the past decade, engineers, scholars, whistleblowers and journalists have repeatedly documented cases in which AI systems, composed of software and algorithms, have caused or contributed to serious harms to humans. Algorithms used in the criminal justice system can unfairly recommend denying parole. Social media feeds can steer toxic content toward vulnerable teenagers. AI-guided military drones can kill without any moral reasoning. Additionally, an AI algorithm tends to be more like an inscrutable black box than a clockwork mechanism. Researchers often cannot understand how these algorithms, which are based on opaque equations that involve billions of calculations, achieve their outcomes. Problems with AI have not gone unnoticed, and academic researchers are trying to make these systems safer and more ethical. Companies that build AI-centered products are working to eliminate harms, although they tend to offer little transparency on their efforts. \u201cThey have not been very forthcoming,\u201d says Jonathan Stray, an AI researcher at the University of California, Berkeley. AI\u2019s known dangers, as well as its potential future risks, have become broad drivers of new AI research. Even scientists who focus on more abstract problems such as the efficiency of AI algorithms can no longer ignore their field\u2019s societal implications. \u201cThe more that AI has become powerful, the more that people demand that it has to be safe and robust,\u201d says Pascale Fung, an AI researcher at the Hong Kong University of Science and Technology. \u201cFor the most part, for the past three decades that I was in AI, people didn\u2019t really care.\u201d Concerns have grown as AI has become widely used. For example, in the mid-2010s, some Web search and social media companies started inserting AI algorithms into their products. They found they could create algorithms to predict which users were more likely to click on which ads and thereby increase their profits. Advances in computing had made all this possible through dramatic improvements in \u201ctraining\u201d these algorithms\u2014making them learn from examples to achieve high performance. But as AI crept steadily into search engines and other applications, observers began to notice problems and raise questions. In 2016 investigative journalists raised claims that certain algorithms used in parole assessment were racially biased. That report\u2019s conclusions have been challenged, but designing AI that is fair and unbiased is now considered a central problem by AI researchers. Concerns arise whenever AI is deployed to make predictions about people from different demographics. Fairness has now become even more of a focus as AI is embedded in ever more decision-making processes, such as screening resumes for a job or evaluating tenant applications for an apartment. In the past few years, the use of AI in social media apps has become another concern. Many of these apps use AI algorithms called recommendation engines, which work in a similar way to ad-serving algorithms, to decide what content to show to users. Hundreds of families are currently suing social media companies over allegations that algorithmically driven apps are directing toxic content to children and causing mental health problems. Seattle Public Schools recently filed a lawsuit alleging that social media products are addictive and exploitative. But untangling an algorithm\u2019s true impact is no easy matter. Social media platforms release few data on user activity, which are needed for independent researchers to make assessments. \u201cOne of the complicated things about all technologies is that there\u2019s always costs and benefits,\u201d says Stray, whose research focuses on recommender systems. \u201cWe\u2019re now in a situation where it\u2019s hard to know what the actual bad effects are.\u201d The nature of the problems with AI is also changing. The past two years have seen the release of multiple \u201cgenerative AI\u201d products that can produce text and images of remarkable quality. A growing number of AI researchers now believe that powerful future AI systems could build on these achievements and one day pose global, catastrophic dangers that could make current problems pale in comparison. What form might such future threats take? In a paper posted on the preprint repository arXiv.org in October, researchers at DeepMind (a subsidiary of Google\u2019s parent company Alphabet) describe one catastrophic scenario. They imagine engineers developing a code-generating AI based on existing scientific principles and tasked with getting human coders to adopt its submissions to their coding projects. The idea is that as the AI makes more and more submissions, and some are rejected, human feedback will help it learn to code better. But the researchers suggest that this AI, with its sole directive of getting its code adopted, might potentially develop a tragically unsound strategy, such as achieving world domination and forcing its code to be adopted\u2014at the cost of upending human civilization. Some scientists argue that research on existing problems, which are already concrete and numerous, should be prioritized over work involving hypothetical future disasters. \u201cI think we have much worse problems going on today,\u201d says Cynthia Rudin, a computer scientist and AI researcher at Duke University. Strengthening that case is the fact that AI has yet to directly cause any large-scale catastrophes\u2014although there have been a few contested instances in which the technology did not need to reach futuristic capability levels in order to be dangerous. For example, the nonprofit human rights organization Amnesty International alleged in a report published last September that algorithms developed by Facebook\u2019s parent company Meta \u201csubstantially contributed to adverse human rights impacts\u201d on the Rohingya people, a minority Muslim group, in Myanmar by amplifying content that incited violence. Meta responded to Scientific American\u2019s request for comment by pointing to a previous statement to Time magazine from Meta\u2019s Asia-Pacific director of public policy Rafael Frankel, who acknowledged that Myanmar\u2019s military committed crimes against the Rohingya and stated that Meta is currently participating in intergovernmental investigative efforts led by the United Nations and other organizations. Other researchers say preventing a powerful future AI system from causing a global catastrophe is already a major concern. \u201cFor me, that\u2019s the primary problem we need to solve,\u201d says Jan Leike, an AI researcher at the company OpenAI. Although these hazards are so far entirely conjectural, they are undoubtedly driving a growing community of researchers to study various harm-reduction tactics. In one approach called value alignment, pioneered by AI scientist Stuart Russell at the University of California, Berkeley, researchers seek ways to train an AI system to learn human values and act in accordance with them. One of the advantages of this approach is that it could be developed now and applied to future systems before they present catastrophic hazards. Critics say value alignment focuses too narrowly on human values when there are many other requirements for making AI safe. For example, just as with humans, a foundation of verified, factual knowledge is essential for AI systems to make good decisions. \u201cThe issue is not that AI\u2019s got the wrong values,\u201d says Oren Etzioni, a researcher at the Allen Institute for AI. \u201cThe truth is that our actual choices are functions of both our values and our knowledge.\u201d With these criticisms in mind, other researchers are working to develop a more general theory of AI alignment that works to ensure the safety of future systems without focusing as narrowly on human values. Sign up for Scientific American\u2019s free newsletters. Some scientists are taking approaches to AI alignment that they see as more practical and connected with the present. Consider recent advances in text-generating technology: the leading examples, such as DeepMind\u2019s Chinchilla, Google Research\u2019s PaLM, Meta AI\u2019s OPT and OpenAI\u2019s ChatGPT, can all produce content that is racially biased, illicit or deceptive\u2014a challenge that each of these companies acknowledges. Some of these companies, including OpenAI and DeepMind, consider such problems to be ones of inadequate alignment. They are now working to improve alignment in text-generating AI and hope this will offer insights into aligning future systems. Researchers acknowledge that a general theory of AI alignment remains absent. \u201cWe don\u2019t really have an answer for how we align systems that are much smarter than humans,\u201d Leike says. But whether the worst problems of AI are in the past, present or future, at least the biggest roadblock to solving them is no longer a lack of trying. ABOUT THE AUTHOR(S) Mordechai Rorvig is a freelance science writer whose work has appeared at outlets that include Astronomy, Inverse, Nautilus, New Scientist, Physics, Quanta, Symmetry, Vice and Wired. His independent journalism on computer science and AI is currently supported by a grant from Open Philanthropy. ChatGPT, an AI-powered chatbot capable of generating humanlike responses to an unimaginable range of questions and prompts, has accumulated 100 million users in just two months, surpassing the growth rates of popular social media platforms such as TikTok and Instagram. The bot\u2019s amazing performances have elicited a wide range of reactions from critics and fans alike, from awe and admiration to concern and alarmist fears that even creative and skilled jobs may be destined to succumb to machine automation. Among its many remarkable feats, ChatGPT can pass advanced university exams in law, medicine, and business; translate a picture of the contents of your fridge into a range of exciting recipes; produce an infinite number of essays, poems, and articles (though not, we promise, this one); and translate the picture of a handwritten website sketch into the full code needed to create it. Needless to say, all of these and other achievements occur in just a few seconds. As psychologists, our primary interest in ChatGPT, AI, and indeed any technology, centers around their human impact, including their potential to change and reshape how we think, work, and live. As Pamela Pavliscak noted, \u201cWe design tech and tech, in turn, designs us.\u201d With the debut of highly versatile, viral, and pervasive technology as ChatGPT, we are bound to uncover consequential repercussions on how humans behave. As illustrated in I, Human: AI, Automation, and the Quest to Reclaim What Makes Us Unique , AI-induced changes aren\u2019t always about generating new human behaviors, but rather, revealing, amplifying, or augmenting existing human beliefs, habits, and adaptations. For example, much of the pushback against ChatGPT reveals the human inability to accept that something could be smarter than us, even when that thing is our own creation, and thus a natural\u2014okay, artificial\u2014extension of our own intelligence. Ironically, such defensive pride, and the tendency to dismiss AI in order to reaffirm our intellectual superiority over the machines we\u2019ve created, would put our own value and future at risk. By failing to experiment, learn, and cocreate with the very new technologies we create, we increase the probability that those technologies replace us. In our view, the evolution of AI innovations is not just a technological breakthrough but also a catalyst for a much-needed shift in our thinking about ourselves and our relationship with the world. Specifically, we see ChatGPT, which merely personifies the latest advancement of mainstream AI today, as a wake-up call for humanity to harness four unique and precious human virtues. Humility The realization that AI can handle even creative and intellectually complex tasks should be a humbling experience for us. Abstract thinking and other knowledge work, once considered unique to humans, are no longer completely exclusive to us. This realization is reminiscent of a Freudian \u201cnarcissistic wound\u201d or attack on our ego as it challenges our long-held belief in human superiority over everything. This philosophical realization echoes earlier scientific discoveries that the world is not the center of the universe, that we share common ancestries with primates, or that (Freud\u2019s own contribution) humans often lack conscious control over their motives and behaviors. Indeed, after centuries of ascribing to the \u201cI think, therefore I am\u201d mantra to define the essence of humanity, we must, for the first time, ask ourselves what it means to be human in an era in which much of our thinking can be outsourced to machines. Curiosity Living in a world with access to ubiquitous information and knowledge can be meaningless unless we leverage our curiosity, our desire to learn and understand. The AI age has amplified the (already high) value of curiosity in the realm of human virtues, redefining the meaning of expertise. What matters today is not experts knowing the answers to all questions but that they are asking the right questions, not retrieving information but critically evaluating and vetting, not collecting insights but making smart decisions based on them. Paradoxically, ChatGPT or other AI tools may actually numb our curiosity. Some may use AI-powered tools as a quick fix, similar to consuming fast food, for retrieving general facts and insights. However, those with a curious mindset can use AI tools to expand their perspectives and foster further innovation for humanity. We expect, or at least hope, that while superficial fact retrieval will be a commodity in our cognitive abilities, the capacity to engage in \u201cdeep learning\u201d (a term sadly associated with machine rather than human intelligence these days) will be key to our individual and collective success. In other words, if ChatGPT is the intellectual equivalent to fast food\u2014i.e., efficient, cheap, addictive, rapid, but neither nutritious nor healthy\u2014it is time to discover the intellectual equivalent of slow food or farm-to-table. Self-awareness Understanding ourselves in the age of AI means paying attention to how our interactions with technology are reshaping our behaviors, and what they tell us about ourselves, including our dark side traits: impulsivity, distractibility, self-centeredness, and bias (as highlighted in one of our recent books). In fact, feeding our own writings, recordings, and content to ChatGPT can give us back an AI-reflection of ourselves, just like in this episode of Adam Grant\u2019s Re: Thinking podcast. A future in which we examine our entire digital footprint to better understand our reputation and identity is not far-fetched, particularly as regulatory pressures around ethical and lawful uses of AI encourage Big Tech platforms to give us back some of the data we\u2019ve relinquished in the form of insights capable of boosting our self-knowledge. Empathy Empathy, a unique human capacity to understand and share the feelings of others, can bring multiple benefits in people\u2019s well-being and societal cohesion. While it may be a stretch to argue that AI tools will directly instill empathy, interacting with AI technology can certainly facilitate the development of empathy. Through recognizing our intellectual and epistemic limitations and appreciating others\u2019 strengths, even if they are machines, we can become less self-focused, and paying due attention and recognition to others, ultimately leading to feelings of empathy and gratitude. Indeed, numerous studies suggest that humility brings various prosocial attitudes, such as empathy, gratitude, altruism, and benevolence by enabling individuals to let go of extreme self-focus or egocentrism. The emergence of advanced AI technologies, as with prior technologies, presents both risks and opportunities. Will the benefits outweigh the risks? Our stance is contingent. If we use this chance to become a bit humbler and try to preserve, develop, and reclaim the qualities that make us human unique, maybe mankind\u2019s future will be brighter than today. Sunny Lee is an associate professor of organizational behavior and head of diversity at the UCL School of Management. Tomas Chamorro-Premuzic is an international authority in leadership assessment, people analytics, and talent management. He is the chief talent scientist at ManpowerGroup and a professor of business psychology at both University College London and Columbia University. His most recent book is Why Do So Many Incompetent Men Become Leaders? (And How to Fix It). By MARC MANCHER FOR DELOITTE In a state of emergency, and with national security, public health, and economic stability at stake, people need government agencies to provide cogent information and prompt access to lifesaving services. No fewer than 18 separate, billion-dollar weather and climate disasters hit the United States in 2022, according to the National Oceanic and Atmospheric Administration. In total, these disasters surpassed $165 billion in damages. Emergency managers know that it\u2019s not if disaster strikes\u2014it\u2019s a matter of when. Over the past two decades, increased federal funding and priorities have been shifted to support a national culture of preparedness. Improvements to the Emergency Alert System, including text messages, and educational campaigns like Ready.gov provide important resources. But in the midst of a disaster, people will still have questions and look to the government for clear, specific, and timely answers. \u201cWhen disaster strikes, there will be a surge in calls from people looking for information, quite possibly on multiple fronts,\u201d said Shelly Metschan, managing director of Deloitte Consulting LLP, who advises government leaders on contact center operations. \u201cWhere can I find housing if my home was destroyed? How can I get help to keep food on my family\u2019s table? How do I apply for assistance? Where can I receive medical care or fill my prescriptions?\u201d Answering these multifaceted questions quickly and accurately, while managing unpredictable, consequential, and rapidly changing situations on the ground, depends on frontline crisis contact centers that have the right mix of experienced, compassionate people and modern technology on the other end of the line. To meet this important goal, government agencies can employ six important strategies: 1. Establish a command center. Government leaders need to establish a centralized, coordinated authority to oversee all aspects of the contact center as the \u201cgo-to\u201d source to receive feedback or disseminate information to the public. Crisis communication isn\u2019t just a flow of information from the top down. Frontline call center interactions are a vital source of information. If hundreds of people start calling about a lack of drinking water or a heating oil shortage or new health symptoms, then that information should flow quickly back up to decision-makers. 2. Align the message. Starting at the top, elected leaders must work hand-in-hand with agency leaders to put forward information that is timely, credible, and easily understood. Consistent messaging should be shared with local agencies and crisis contact center representatives. 3. Expeditiously deploy agents. Bring in and reallocate staff\u2014even from other agencies or external organizations. The normal speed of public-sector hiring is too slow for a crisis. Agents must be fully vetted and onboarded at an emergency pace. Government can benefit by leaning on temporary outside help to fill gaps while the public hiring process is underway. 4. Train and share knowledge. Both existing staff and newly onboarded agents will need some form of just-in-time training in a rapidly shifting situation. While new agents may be limited in what they can do, government agencies can benefit by using temporary hires to handle basic tasks so the experienced state workforce can tackle more complex issues. 5. Use artificial intelligence to answer the easy questions. The demand surge in a crisis can\u2019t be addressed through staffing alone. Using technology to effectively answer the most common questions enables callers with more complex cases to talk to experienced professionals. Conversational artificial intelligence is a rapidly improving technology that has the potential to serve the public at scale while reducing the workload for staff members who are providing direct support to people in crisis. 6. Continually enhance technology infrastructure. The technology platforms that work well during normal times often prove inadequate to handle the spike in calls during a crisis. Agencies should constantly reassess interactive voice response systems to ensure the menu of options reflects current needs and realities. Making sure these six elements are functioning at a high level is essential to successfully navigating the next crisis and making sure the right people get the right information at the right time. Marc Mancher is a principal of Deloitte Consulting LLP, where he founded Deloitte\u2019s contact center practice. Marc has spent the last 25 years improving contact center customer service through the use of artificial intelligence, advanced technologies, and modern operational practices. As used in this document, \u201cDeloitte\u201d means Deloitte Consulting LLP, a subsidiary of Deloitte LLP. Please see deloitte.com/us/about for a detailed description of our legal structure. Certain services may not be available to attest clients under the rules and regulations of public accounting. This publication contains general information only and Deloitte is not, by means of this publication, rendering accounting, business, financial, investment, legal, tax, or other professional advice or services. This publication is not a substitute for such professional advice or services, nor should it be used as a basis for any decision or action that may affect your business. Before making any decision or taking any action that may affect your business, you should consult a qualified professional advisor. Deloitte shall not be responsible for any loss sustained by any person who relies on this publication. In a state of emergency, and with national security, public health, and economic stability at stake, people need government agencies to provide cogent information and prompt access to lifesaving services.",
    "gpt_response": "**Source 1: Wired - \"Artificial Intelligence Could Finally Let Us Talk with Animals\"**\n\n**Topics:**\n1. The Evolution of Animal Communication Research and AI's Role\n2. Ethical Implications of Decoding Animal Languages\n3. The Impact of AI on Human-Animal Relationships\n\n**Summary:**\nThis article explores groundbreaking research in the field of animal communication, highlighting the potential of artificial intelligence (AI) to bridge the communication gap between humans and animals. The piece focuses on the work of scientists like Christian Rutz and Aza Raskin, who are at the forefront of using machine learning models to analyze and understand the vocalizations of animals such as New Caledonian Crows and sperm whales. The article delves into how this research could revolutionize our understanding of animals, aiding in conservation efforts and potentially transforming our relationship with the natural world. It also touches on the ethical considerations and the profound implications of such advancements, likening the potential impact to the discovery that Earth is not the center of the universe.\n\n**Meta Review:**\nThe Wired article provides an insightful look into the intersection of AI and animal communication research. It showcases how AI is being used to decode animal languages, offering a glimpse into a future where humans might communicate with other species. The piece effectively balances the scientific excitement with a consideration of the ethical implications, making it a compelling read for anyone interested in technology, animal behavior, or conservation."
}
