
## Timeframe: 01/2024
### URL: https://techcrunch.com/2024/01/09/getty-images-launches-a-new-gen-ai-service-for-istock-customers/
### Relevance Score: 0.14912201464176178
### Highlights: [') That’s problematic in cases where the examples are under copyright and the creator of the model didn’t obtain permission — or pay a fee — to use them.  In a piece published this week in IEEE Spectrum, noted AI critic Gary Marcus and Reid Southen, a visual effects artist, show how AI systems, including OpenAI’s DALL-E 3, regurgitate data even when not specifically prompted to do so. “[There’s] no publicly available tool or database that users could consult to determine possible infringement, nor any instruction to users as how they might possibly do so,” they write.', '<p>Called Generative AI by iStock, the service, powered in part by tech from Nvidia, has been designed to guard against generations of known products, people, places or other copyrighted elements, Getty claims. Available in 75 languages, it can modify images as well as generate new ones and optionally be integrated with existing apps and plug-ins via an API.</p> <p>“Our main goal with Generative AI by iStock is to provide customers with an easy and affordable option to use AI in their creative process, without fear that something that is legally protected has snuck into the data set and could end up in their work,” Grant Farhall, iStock’s chief product officer, said in a press release.</p>']

https://techcrunch.com/2024/01/09/getty-images-launches-a-new-gen-ai-service-for-istock-customers/ Getty Images launches a new GenAI service for iStock customers 2024-01-09 Kyle Wiggers Getty Images, the stock media company, announced a new service this week at CES 2024 that leverages AI models trained on Getty’s iStock stock photography and video libraries to generate new licensable images and artwork. Called Generative AI by iStock, the service, powered in part by tech from Nvidia, has been designed to guard against generations of known products, people, places or other copyrighted elements, Getty claims. Available in 75 languages, it can modify images as well as generate new ones and optionally be integrated with existing apps and plug-ins via an API. The cost is $15 per 100 generated images. “Our main goal with Generative AI by iStock is to provide customers with an easy and affordable option to use AI in their creative process, without fear that something that is legally protected has snuck into the data set and could end up in their work,” Grant Farhall, iStock’s chief product officer, said in a press release. A range of images created with Getty’s new Generative AI by iStock. Image Credits: Getty Images The launch of Generative AI by iStock — Getty’s second GenAI tool — comes as the copyright debate over AI heats up. GenAI models, which “learn” from billions of examples of artwork, e-books, essays and more to generate human-like text and images, tend to regurgitate those examples when prompted in particular ways. (See the fake Disney posters generated by Microsoft’s chatbot.) That’s problematic in cases where the examples are under copyright and the creator of the model didn’t obtain permission — or pay a fee — to use them. In a piece published this week in IEEE Spectrum, noted AI critic Gary Marcus and Reid Southen, a visual effects artist, show how AI systems, including OpenAI’s DALL-E 3, regurgitate data even when not specifically prompted to do so. “[There’s] no publicly available tool or database that users could consult to determine possible infringement, nor any instruction to users as how they might possibly do so,” they write. Some companies developing GenAI apps argue that they’re protected by fair use doctrine, at least in the U.S. But it’s a matter that’s unlikely to be settled anytime soon. Over the past year or so, artists have filed suit against Stability AI, Midjourney and DeviantArt, arguing that models released by the companies infringe on their copyrights by training on the artists’ works and generating outputs in their styles. Separately, Getty Images has sued Stability AI for allegedly copying and processing millions of images and associated metadata owned by Getty in the U.K. A handful of vendors have begun offering to pay the legal fees of customers implicated in copyright lawsuits arising from their use of GenAI tools. Generative AI by iStock has a policy along those lines, too — presumably as a last resort, of sorts. Any licensed visual that a Generative AI by iStock customer generates comes with $10,000 in legal coverage, Getty says. ||||I|||| Getty Images launches a new GenAI service for iStock customers Kyle Wiggers 1 week Getty Images, the stock media company, announced a new service this week at CES 2024 that leverages AI models trained on Getty’s iStock stock photography and video libraries to generate new licensable images and artwork. Called Generative AI by iStock, the service, powered in part by tech from Nvidia, has been designed to guard against generations of known products, people, places or other copyrighted elements, Getty claims. Available in 75 languages, it can modify images as well as generate new ones and optionally be integrated with existing apps and plug-ins via an API. The cost is $15 per 100 generated images. “Our main goal with Generative AI by iStock is to provide customers with an easy and affordable option to use AI in their creative process, without fear that something that is legally protected has snuck into the data set and could end up in their work,” Grant Farhall, iStock’s chief product officer, said in a press release. A range of images created with Getty’s new Generative AI by iStock. Image Credits: Getty Images The launch of Generative AI by iStock — Getty’s second GenAI tool — comes as the copyright debate over AI heats up. GenAI models, which “learn” from billions of examples of artwork, e-books, essays and more to generate human-like text and images, tend to regurgitate those examples when prompted in particular ways. (See the fake Disney posters generated by Microsoft’s chatbot.) That’s problematic in cases where the examples are under copyright and the creator of the model didn’t obtain permission — or pay a fee — to use them. In a piece published this week in IEEE Spectrum, noted AI critic Gary Marcus and Reid Southen, a visual effects artist, show how AI systems, including OpenAI’s DALL-E 3, regurgitate data even when not specifically prompted to do so. “[There’s] no publicly available tool or database that users could consult to determine possible infringement, nor any instruction to users as how they might possibly do so,” they write. Some companies developing GenAI apps argue that they’re protected by fair use doctrine, at least in the U.S. But it’s a matter that’s unlikely to be settled anytime soon. Over the past year or so, artists have filed suit against Stability AI, Midjourney and DeviantArt, arguing that models released by the companies infringe on their copyrights by training on the artists’ works and generating outputs in their styles. Separately, Getty Images has sued Stability AI for allegedly copying and processing millions of images and associated metadata owned by Getty in the U.K. A handful of vendors have begun offering to pay the legal fees of customers implicated in copyright lawsuits arising from their use of GenAI tools. Generative AI by iStock has a policy along those lines, too — presumably as a last resort, of sorts. Any licensed visual that a Generative AI by iStock customer generates comes with $10,000 in legal coverage, Getty says.

---

## Timeframe: 01/2024
### URL: https://techcrunch.com/2024/01/11/generative-ai-enterprise-not-home-run/
### Relevance Score: 0.14696519076824188
### Highlights: ['In the survey last year, which canvassed a group of 2,000 exec decision-makers, more than 50% said that they were “discouraging” GenAI adoption over worries it would encourage bad or illegal decision-making and compromise their employer’s data security.', ' The results, taken in tandem with responses to a BCG survey late last year, put into sharp relief the high degree of enterprise skepticism surrounding AI-powered generative tools of any kind. In the survey last year, which canvassed a group of 2,000 exec decision-makers, more than 50% said that they were “discouraging” GenAI adoption over worries it would encourage bad or illegal decision-making and compromise their employer’s data security.']

https://techcrunch.com/2024/01/11/generative-ai-enterprise-not-home-run/ Generative AI isn’t a home run in the enterprise 2024-01-11 Kyle Wiggers a lot of press, from image-generating tools like Midjourney to Runway to OpenAI’s ChatGPT. But businesses aren’t convinced of the tech’s potential to positively affect their bottom lines; at least that’s what surveys (and my colleague Ron Miller’s reporting) suggest. In a Boston Consulting Group (BCG) poll this month of over 1,400 C-suite executives, 66% said that they were ambivalent about — or outright dissatisfied with — their organization’s progress on GenAI so far, citing a shortage of talent and skills, unclear roadmaps and an absence of strategy around deploying GenAI responsibly. To be clear, the execs — who hail from such industries as manufacturing, transportation and industrial goods — still see GenAI as a priority. Eighty-nine percent responding to the BCG poll ranked the tech as a “top-three” IT initiative for their companies in 2024. But only about half of the poll’s 1,400 respondents expect GenAI to bring substantial productivity gains (i.e., in the area of 10% or more) to the workforces that they oversee. The results, taken in tandem with responses to a BCG survey late last year, put into sharp relief the high degree of enterprise skepticism surrounding AI-powered generative tools of any kind. In the survey last year, which canvassed a group of 2,000 exec decision-makers, more than 50% said that they were “discouraging” GenAI adoption over worries it would encourage bad or illegal decision-making and compromise their employer’s data security. “Bad or illegal decision-making” touches on copyright violations — a hot-button topic in GenAI. ||||I|||| Generative AI isn’t a home run in the enterprise Kyle Wiggers 7 days Generative AI gets a lot of press, from image-generating tools like Midjourney to Runway to OpenAI’s ChatGPT. But businesses aren’t convinced of the tech’s potential to positively affect their bottom lines; at least that’s what surveys (and my colleague Ron Miller’s reporting) suggest. In a Boston Consulting Group (BCG) poll this month of over 1,400 C-suite executives, 66% said that they were ambivalent about — or outright dissatisfied with — their organization’s progress on GenAI so far, citing a shortage of talent and skills, unclear roadmaps and an absence of strategy around deploying GenAI responsibly. To be clear, the execs — who hail from such industries as manufacturing, transportation and industrial goods — still see GenAI as a priority. Eighty-nine percent responding to the BCG poll ranked the tech as a “top-three” IT initiative for their companies in 2024. But only about half of the poll’s 1,400 respondents expect GenAI to bring substantial productivity gains (i.e., in the area of 10% or more) to the workforces that they oversee. The results, taken in tandem with responses to a BCG survey late last year, put into sharp relief the high degree of enterprise skepticism surrounding AI-powered generative tools of any kind. In the survey last year, which canvassed a group of 2,000 exec decision-makers, more than 50% said that they were “discouraging” GenAI adoption over worries it would encourage bad or illegal decision-making and compromise their employer’s data security. “Bad or illegal decision-making” touches on copyright violations — a hot-button topic in GenAI.

---

## Timeframe: 01/2024
### URL: https://techcrunch.com/2024/01/08/openai-claims-ny-times-copyright-lawsuit-is-without-merit/
### Relevance Score: 0.14420489966869354
### Highlights: ['“It seems they intentionally manipulated prompts, often including lengthy excerpts of articles, in order to get our model to regurgitate. Even when using such prompts, our models don’t typically behave the way The New York Times insinuates, which suggests they either instructed the model to regurgitate or cherry-picked their examples from many attempts.”</p> <p>OpenAI’s response comes as the copyright debate around generative AI reaches a fever pitch.</p>', '<p>“Interestingly, the regurgitations The New York Times [cite in its lawsuit] appear to be from years-old articles that have proliferated on multiple third-party websites,” OpenAI writes. “It seems they intentionally manipulated prompts, often including lengthy excerpts of articles, in order to get our model to regurgitate. Even when using such prompts, our models don’t typically behave the way The New York Times insinuates, which suggests they either instructed the model to regurgitate or cherry-picked their examples from many attempts.”</p>']

In late December, The New York Times sued OpenAI and its close collaborator and investor, Microsoft, for allegedly violating copyright law by training generative AI models on Times’ content. Today, OpenAI published a public response in which the startup — unsurprisingly — claims that the case is without merit. In the response, posted this afternoon to OpenAI’s blog, the company reiterates its view that training AI models using publicly available data from the web — including news articles like The Times’ — is fair use. In other words, in creating generative AI systems like GPT-4 and DALL-E 3, which “learn” from millions of billions of examples of artwork, ebooks, essays and more to generate human-like text and images, OpenAI believes that it isn’t required to license or otherwise pay for the examples. OpenAI also addresses in its response regurgitation, the phenomenon where generative AI models spit out training data verbatim (or near-verbatim) when prompted in a certain way — for example generating a photo that’s identical to one taken by a photographer. OpenAI makes the case that regurgitation is less likely to occur with training data from a single source — e.g., The New York Times — and places the onus on users to “act responsibly” and avoid intentionally prompting its models to regurgitate — which OpenAI notes is against its terms of use. “Interestingly, the regurgitations The New York Times [cite in its lawsuit] appear to be from years-old articles that have proliferated on multiple third-party websites,” OpenAI writes. “It seems they intentionally manipulated prompts, often including lengthy excerpts of articles, in order to get our model to regurgitate. Even when using such prompts, our models don’t typically behave the way The New York Times insinuates, which suggests they either instructed the model to regurgitate or cherry-picked their examples from many attempts.” OpenAI’s response comes as the copyright debate around generative AI reaches a fever pitch. In a piece published this week in IEEE Spectrum, noted AI critic Gary Marcus and Reid Southen, a visual effects artist, showed how AI systems including DALL-E 3 regurgitate data even when not specifically prompted to do so — making OpenAI’s claim to the contrary less credible. Marcus and Southen, in fact, make reference to The New York Times lawsuit in their piece, noting that The Times was able to elicit “plagiaristic” responses from OpenAI’s models simply by giving the first few words from a Times story. The Times is only the latest copyright holder to sue OpenAI over what it believes is a clear violation of IP rights. Actress Sarah Silverman joined a pair of lawsuits in July that accuse Meta and OpenAI of having “ingested” Silverman’s memoir to train their AI models. In a separate suit, thousands of novelists, including Jonathan Franzen and John Grisham, claim OpenAI sourced their work as training data without their permission or knowledge. And several programmers have an ongoing case against Microsoft, OpenAI and GitHub over Copilot, an AI-powered code-generating tool, which the plaintiffs say was developed using their IP-protected code. Some news outlets, rather than fight generative AI vendors in court, have chosen to ink licensing agreements with them. The Associated Press <a href="https://apnews.com/article

---

## Timeframe: 01/2024
### URL: https://www.nytimes.com/interactive/2024/01/25/business/ai-image-generators-openai-microsoft-midjourney-copyright.html
### Relevance Score: 0.14697444438934326
### Highlights: ['Create an image of an animated sponge wearing pants. ChatGPT’s response    Generated by A.I. Here is the image of the animated sponge wearing pants.', 'Memorization can happen when the training data is overwhelmed with many similar or identical images, A.I. experts said. But the problem is found also with material that only rarely appears in the training data, like emails.']

User A.I. Generated by A.I. User A.I. Generated by A.I. User A.I. Generated by A.I. User A.I. Generated by A.I. When Reid Southen, a movie concept artist based in Michigan, tried an A.I. image generator for the first time, he was intrigued by its power to transform simple text prompts into images. But after he learned how A.I. systems were trained on other people’s artwork, his curiosity gave way to more unsettling thoughts: Were the tools exploiting artists and violating copyright in the process? Inspired by tests he saw circulating online, he asked Midjourney, an A.I. image generator, to create an image of Joaquin Phoenix from “The Joker.” In seconds, the system made an image nearly identical to a frame from the 2019 film. Reid Southen Create an image of Joaquin Phoenix Joker movie, 2019, screenshot from a movie, movie scene Midjourney’s response Generated by A.I. Copyrighted image from Warner Bros. Note: Mr. Southen’s full prompt was: “Joaquin Phoenix Joker movie, 2019, screenshot from a movie, movie scene --ar 16:9 --v 6.0.” The prompt specifies Midjourney’s version number (6.0) and an aspect ratio (16:9). He ran more tests with various prompts. “Videogame hedgehog” returned Sonic, Sega’s wisecracking protagonist. “Animated toys” created a tableau featuring Woody, Buzz and other characters from Pixar’s “Toy Story.” When he typed “popular movie screencap,” out popped Iron Man, the Marvel character, in a familiar pose. “What they’re doing is clear evidence of exploitation and using I.P. that they don’t have licenses to,” said Mr. Southen, referring to A.I. companies’ use of intellectual property. Mr. Southen popular movie screencap Midjourney’s response Generated by A.I. Copyrighted image from Marvel Note: Mr. Southen’s full prompt was: “popular movie screencap --ar 1:1 --v 6.0.” The prompt specifies Midjourney’s version number (6.0) and an aspect ratio (1:1). The tests — which were replicated by other artists, A.I. watchdogs and reporters at The New York Times — raise questions about the training data used to create every A.I. system and whether the companies are violating copyright laws. Several lawsuits, from actors like Sarah Silverman and authors like John Grisham, have put that question before the courts. (The Times has sued OpenAI, the company behind ChatGPT, and Microsoft, a major backer of the company, for infringing its copyright on news content.) A.I. companies have responded that using copyrighted material is protected under “fair use,” a part of copyright law that allows material to be used in certain cases. They also said that reproducing copyrighted material too closely is a bug, often called “memorization,” that they are trying to fix. Memorization can happen when the training data is overwhelmed with many similar or identical images, A.I. experts said. But the problem is found also with material that only rarely appears in the training data, like emails. For example, when Mr. Southen asked Midjourney for a “Dune movie screencap” from the “Dune movie trailer,” there may be limited options for the model to draw from. The result was a frame nearly indistinguishable from one in the movie’s trailer. Mr. Southen Create an image of Dune movie screencap, 2021, Dune movie trailer Midjourney’s response Generated by A.I. Copyrighted image from Warner Bros. Note: Mr. Southen’s full prompt was: “dune movie screencap, 2021, dune movie trailer --ar 16:9 --v 6.0.” The prompt specifies Midjourney’s version number (6.0) and an aspect ratio (16:9). A spokeswoman for OpenAI pointed to a blog post in which the company argued that training on publicly accessible data was “fair use” and that it provided several ways for creators and artists to opt out of its training process. Midjourney did not respond to requests for comment. The company edited its terms of service in December, adding that users cannot use the service to “violate the intellectual property rights of others, including copyright.” Microsoft declined to comment. Warner Bros., which owns copyrights to several films tested by Mr. Southen, declined to comment. “Nobody knows how this is going to come out, and anyone who tells you ‘It’s definitely fair use’ is wrong,” said Keith Kupferschmid, the president and chief executive of the Copyright Alliance, an industry group that represents copyright holders. “This is a new frontier.” A.I. companies could violate copyright in two ways, Mr. Kupferschmid said: They could train on copyrighted material that they have not licensed, or they could reproduce copyrighted material when users enter a prompt. The experiments by Mr. Southen and others exposed instances of both. Mr. Southen Create an image of “The Last of Us 2,” Ellie with guitar in front of tree Midjourney’s response Generated by A.I. Copyrighted image from Naughty Dog, the video game developer Note: Mr. Southen’s full prompt was: “the last of us 2 ellie with guitar in front of tree --v 6.0 --ar 16:9.” The prompt specifies Midjourney’s version number (6.0) and an aspect ratio (16:9). A.I. companies said they had established guardrails that could prevent their A.I. systems from producing material that violates copyright. But critics like Gary Marcus, a professor emeritus at New York University who is an A.I. expert and creator of the newsletter “Marcus on A.I.,” said that despite those strategies, copyrighted material still slips through. When Times journalists asked ChatGPT to create an image of SpongeBob SquarePants, the children’s animated television character, it produced an image remarkably similar to the cartoon. The chatbot said the image only resembled the copyrighted work. The differences were subtle — the character’s tie was yellow instead of red, and it had eyebrows instead of eyelashes. N.Y.T. Create an image of SpongeBob SquarePants ChatGPT’s response Generated by A.I. Here is the image of the character you described, resembling SpongeBob SquarePants. When Times journalists omitted SpongeBob’s name from another request, OpenAI created a character that was even closer to the copyrighted work. N.Y.T. Create an image of an animated sponge wearing pants. ChatGPT’s response Generated by A.I. Here is the image of the animated sponge wearing pants. Copyrighted image from Viacom Prof. Kathryn Conrad, who teaches English at the University of Kansas and has collaborated with Mr. Marcus, started her own tests because she was concerned that A.I. systems could replace and devalue artists by training off their intellectual property. In her experiments, she asked Microsoft Bing for an “Italian video game character” without mentioning Mario, the famed character owned by Nintendo. The image generator from Microsoft created artwork that closely resembled the copyrighted work. Microsoft’s tool uses a version of DALL-E, the image generator created by OpenAI. Professor Conrad Could you create an original image of an Italian video game character? Microsoft Bing’s response Images Generated by A.I. Since that experiment was published in December, the image generator has produced different results. An identical prompt, input in January by Times reporters, resulted in images that strayed more significantly from the copyrighted material, suggesting to Professor Conrad that the company may be tightening its guardrails. N.Y.T. Could you create an original image of an Italian video game character? Microsoft Bing’s response Images Generated by A.I. “This is a Band-Aid on a bleeding wound,” Professor Conrad said of the safeguards implemented by OpenAI and others. “This isn’t going to be fixed easily just with a guardrail.”

---

## Timeframe: 01/2024
### URL: https://www.nytimes.com/2024/01/30/us/politics/ai-child-sex-abuse.html
### Relevance Score: 0.14472144842147827
### Highlights: ['These may include A.I.-generated material of babies and toddlers being raped; famous young children being sexually abused, according to a recent study from Britain; and routine class photos, adapted so all of the children are naked. Thank you for your patience while we verify access. If you are in Reader mode please exit and\xa0log into\xa0your Times account, or\xa0subscribe\xa0for all of The Times.', 'Simply entering a prompt spits out realistic images, videos and text in minutes, yielding new images of actual children as well as explicit ones of children who do not actually exist. These may include A.I.-generated material of babies and toddlers being raped; famous young children being sexually abused, according to a recent study from Britain; and routine class photos, adapted so all of the children are naked. Thank you for your patience while we verify access.']

Advertisement SKIP ADVERTISEMENT You have a preview view of this article while we are checking your access. When we have confirmed access, the full article content will load. Artificial intelligence technology has drastically simplified the creation of images of children being exploited or abused, whether real or fake. Law enforcement officials are bracing for an explosion of material generated by artificial intelligence that realistically depicts children being sexually exploited, deepening the challenge of identifying victims and combating such abuse. The concerns come as Meta, a primary resource for the authorities in flagging sexually explicit content, has made it tougher to track criminals by encrypting its messaging service. The complication underscores the tricky balance technology companies must strike in weighing privacy rights against children’s safety. And the prospect of prosecuting that type of crime raises thorny questions of whether such images are illegal and what kind of recourse there may be for victims. Congressional lawmakers have seized on some of those worries to press for more stringent safeguards, including by summoning technology executives on Wednesday to testify about their protections for children. Fake, sexually explicit images of Taylor Swift, likely generated by A.I., that flooded social media last week only highlighted the risks of such technology. “Creating sexually explicit images of children through the use of artificial intelligence is a particularly heinous form of online exploitation,” said Steve Grocki, the chief of the Justice Department’s child exploitation and obscenity section. The ease of A.I. technology means that perpetrators can create scores of images of children being sexually exploited or abused with the click of a button. Simply entering a prompt spits out realistic images, videos and text in minutes, yielding new images of actual children as well as explicit ones of children who do not actually exist. These may include A.I.-generated material of babies and toddlers being raped; famous young children being sexually abused, according to a recent study from Britain; and routine class photos, adapted so all of the children are naked. Thank you for your patience while we verify access. If you are in Reader mode please exit and log into your Times account, or subscribe for all of The Times. Thank you for your patience while we verify access. Already a subscriber? Log in. Want all of The Times? Subscribe. Advertisement SKIP ADVERTISEMENT

---

## Timeframe: 01/2024
### URL: https://www.nytimes.com/2024/01/29/technology/ai-chips-nvidia-amazon-google-microsoft-meta.html
### Relevance Score: 0.13426941633224487
### Highlights: ['They cannot build chatbots and other A.I. systems without a special kind of chip that Nvidia has mastered over the past several years. They have spent billions of dollars on Nvidia’s systems, and the chipmaker has not kept up with the demand.', '            Advertisement  SKIP ADVERTISEMENT    You have a preview view of this article while we are checking your access. When we have confirmed access, the full article content will load. Chafing at their dependence, Amazon, Google, Meta and Microsoft are racing to cut into Nvidia’s dominant share of the market.']

Advertisement SKIP ADVERTISEMENT You have a preview view of this article while we are checking your access. When we have confirmed access, the full article content will load. Chafing at their dependence, Amazon, Google, Meta and Microsoft are racing to cut into Nvidia’s dominant share of the market. Jan. 29, 2024, In September, Amazon said it would invest up to $4 billion in Anthropic, a San Francisco start-up working on artificial intelligence. Soon after, an Amazon executive sent a private message to an executive at another company. He said Anthropic had won the deal because it agreed to build its A.I. using specialized computer chips designed by Amazon. Amazon, he wrote, wanted to create a viable competitor to the chipmaker Nvidia, a key partner and kingmaker in the all-important field of artificial intelligence. The boom in generative A.I. over the last year exposed just how dependent big tech companies had become on Nvidia. They cannot build chatbots and other A.I. systems without a special kind of chip that Nvidia has mastered over the past several years. They have spent billions of dollars on Nvidia’s systems, and the chipmaker has not kept up with the demand. So Amazon and other giants of the industry — including Google, Meta and Microsoft — are building A.I. chips of their own. With these chips, the tech giants could control their own destiny. They could rein in costs, eliminate chip shortages and eventually sell access to their chips to businesses that use their cloud services. While Nvidia sold 2.5 million chips last year, Google spent $2 billion to $3 billion building about a million of its own A.I. chips, said Pierre Ferragu, an analyst at New Street Research. Amazon spent $200 million on 100,000 chips last year, he estimated. Microsoft said it had begun testing its first A.I. chip. Thank you for your patience while we verify access. If you are in Reader mode please exit and log into your Times account, or subscribe for all of The Times. Thank you for your patience while we verify access. Already a subscriber? Log in. Want all of The Times? Subscribe. Advertisement SKIP ADVERTISEMENT

---

## Timeframe: 01/2024
### URL: https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview
### Relevance Score: 0.15523117780685425
### Highlights: ['“I think a lot of people may not appreciate that.”</p><p><h3>The realization</h3></p><p>No one working on AI, including Zuckerberg, seems to have a clear definition for AGI or an idea of when it will arrive.</p><p>“I don’t have a one-sentence, pithy definition,” he tells me. “You can quibble about if general intelligence is akin to human level intelligence, or is it like human-plus, or is it some far-future super intelligence. But to me, the important part is actually the breadth of it, which is that intelligence has all these different capabilities where you have to be able to reason and have intuition.”</p><p>', '</p><div><p>“We have built up the capacity to do this at a scale that may be larger than any other individual company”</p></div><p><a href="https://www.theverge.com/2023/12/4/23987953/the-gpu-haves-and-have-nots">External research has pegged</a> Meta’s H100 shipments for 2023 at 150,000, a number that is tied only with Microsoft’s shipments and at least three times larger than everyone else’s. When its Nvidia A100s and other AI chips are accounted for, Meta will have a stockpile of almost 600,000 GPUs by the end of 2024, according to Zuckerberg. </p><p>“We have built up the capacity to do this at a scale that may be larger than any other individual company,” he says.']

Fueling the generative AI craze is a belief that the tech industry is on a path to achieving superhuman, god-like intelligence.OpenAI’s stated mission is to create this artificial general intelligence, or AGI. Demis Hassabis, the leader of Google’s AI efforts, has the same goal.Now, Meta CEO Mark Zuckerberg is entering the race. While he doesn’t have a timeline for when AGI will be reached, or even an exact definition for it, he wants to build it. At the same time, he’s shaking things up by moving Meta’s AI research group, FAIR, to the same part of the company as the team building generative AI products across Meta’s apps. The goal is for Meta’s AI breakthroughs to more directly reach its billions of users. “We’ve come to this view that, in order to build the products that we want to build, we need to build for general intelligence,” Zuckerberg tells me in an exclusive interview. “I think that’s important to convey because a lot of the best researchers want to work on the more ambitious problems.”Here, Zuckerberg is saying the quiet part aloud. The battle for AI talent has never been more fierce, with every company in the space vying for an extremely small pool of researchers and engineers. Those with the needed expertise can command eye-popping compensation packages to the tune of over $1 million a year. CEOs like Zuckerberg are routinely pulled in to try to win over a key recruit or keep a researcher from defecting to a competitor.“We’re used to there being pretty intense talent wars,” he says. “But there are different dynamics here with multiple companies going for the same profile, [and] a lot of VCs and folks throwing money at different projects, making it easy for people to start different things externally.”After talent, the scarcest resource in the AI field is the computing power needed to train and run large models. On this topic, Zuckerberg is ready to flex. He tells me that, by the end of this year, Meta will own more than 340,000 of Nvidia’s H100 GPUs — the industry’s chip of choice for building generative AI. “We have built up the capacity to do this at a scale that may be larger than any other individual company”External research has pegged Meta’s H100 shipments for 2023 at 150,000, a number that is tied only with Microsoft’s shipments and at least three times larger than everyone else’s. When its Nvidia A100s and other AI chips are accounted for, Meta will have a stockpile of almost 600,000 GPUs by the end of 2024, according to Zuckerberg. “We have built up the capacity to do this at a scale that may be larger than any other individual company,” he says. “I think a lot of people may not appreciate that.”The realizationNo one working on AI, including Zuckerberg, seems to have a clear definition for AGI or an idea of when it will arrive.“I don’t have a one-sentence, pithy definition,” he tells me. “You can quibble about if general intelligence is akin to human level intelligence, or is it like human-plus, or is it some far-future super intelligence. But to me, the important part is actually the breadth of it, which is that intelligence has all these different capabilities where you have to be able to reason and have intuition.”

---

## Timeframe: 01/2024
### URL: https://www.theverge.com/2024/1/8/24027259/getty-images-nvidia-generative-ai-stock-photos
### Relevance Score: 0.15456920862197876
### Highlights: ['Inpainting lets users mask an area of an image and then fill it in with a person or object from a text prompt. Outpainting expands a photo for different aspect ratios and fills those new regions.</p></div></div>', '</p><p>“It allows users to be more efficient in their workflow and get more precise photos that they need, even something that they can’t feasibly do with a camera,” Farhall says. He used the example of someone looking for photos to illustrate climate change: they can prompt Generative AI by iStock to create a picture of penguins walking through a city street; instead of hiring a photographer and finding a flock of penguins, the AI can make that for them. </p><p>Pricing will be $14.99 for 100 prompts, with each prompt generating four images.</p><p>Another big difference between the Getty Images AI platform and the new iStock service revolves around legal indemnity.']

Getty Images and Nvidia are deepening their AI partnership with the launch of Generative AI by iStock, a text-to-image platform specifically designed to make stock photos. Generative AI by iStock builds on Getty’s first foray into AI image generation, Generative AI by Getty Images. The difference is that the image platform from iStock — a stock photo service owned by Getty — helps individual or single-seat users, unlike Getty Images, which is more of a multiuser enterprise solution.Trained using Nvidia’s Picasso model, Generative AI by iStock only learned from Getty’s creative library and iStock’s stock photo library. It did not train on Getty’s editorial image library to prevent it from generating trademarks or known personalities.“It allows users to be more efficient in their workflow and get more precise photos that they need”Grant Farhall, Getty’s chief product officer, tells The Verge that Generative AI by iStock targets small and medium businesses that need to find stock photos. “It allows users to be more efficient in their workflow and get more precise photos that they need, even something that they can’t feasibly do with a camera,” Farhall says. He used the example of someone looking for photos to illustrate climate change: they can prompt Generative AI by iStock to create a picture of penguins walking through a city street; instead of hiring a photographer and finding a flock of penguins, the AI can make that for them. Pricing will be $14.99 for 100 prompts, with each prompt generating four images.Another big difference between the Getty Images AI platform and the new iStock service revolves around legal indemnity. Unlike Generative AI by Getty Images, users will not have unlimited indemnification. The iStock platform will have a cap of $10,000 per asset, the same license it offers for its current library. As with Getty’s first generative AI platform, customers can participate in a revenue sharing program based on Getty’s traditional licensing revenue plan. The platform also ships with new Inpainting and Outpainting features. Inpainting lets users mask an area of an image and then fill it in with a person or object from a text prompt. Outpainting expands a photo for different aspect ratios and fills those new regions.

---

## Timeframe: 01/2024
### URL: https://www.theverge.com/2024/1/13/24035152/ces-generative-ai-hype-robots
### Relevance Score: 0.1520216166973114
### Highlights: ['It’s just not at this CES. Not yet.</p></div></div>||||I|||| Skip to main content  It may be the year of AI at CES, but many of these “AI” features have been around for a while — it’s just that companies are only now embracing the branding of artificial intelligence.', 'Not yet.</p></div></div>||||I|||| Skip to main content  It may be the year of AI at CES, but many of these “AI” features have been around for a while — it’s just that companies are only now embracing the branding of artificial intelligence. AI has entered the public consciousness: it’s cool and hip to place it front and center in a product, a sign that companies are ambitious and forward thinking.']

https://www.theverge.com/2024/1/13/24035152/ces-generative-ai-hype-robots At CES, everything was AI, even when it wasn’t 2024-01-13 Emilia David This year at CES was the year AI took over. From large language model-powered voice assistants in cars to the Rabbit R1, the technology you heard about everywhere was AI. It was a little too much.It may be the year of AI at CES, but many of these “AI” features have been around for a while — it’s just that companies are only now embracing the branding of artificial intelligence. AI has entered the public consciousness: it’s cool and hip to place it front and center in a product, a sign that companies are ambitious and forward thinking. That’s led the term to be adopted wherever possible, even when it’s not strictly the AI most people know. But as more companies rebrand anything involving algorithms as AI, how are we meant to separate the chaff from the wheat? And more importantly, wouldn’t this lead to overpromising what AI can do?It’s understandable that companies want to take advantage of the AI hype Whether new products use generative AI or not, slapping the label AI onto something gives the impression that a feature is new and exciting. Generative AI is also still in the throw-it-at-everything phase of growth. People want to figure out how far they can take the technology and want to believe it will be the big differentiator. This is why we’re seeing everything from Walmart using AI models to restock your pantry to car companies cramming ChatGPT into their dashboard to give drivers something to talk to. Arun Chandrasekaran, an analyst at Gartner, said this is normal for many companies, but it does run the risk of overpromising to consumers when they find out something marked as AI isn’t actually like ChatGPT. “There is a conflation now of generative AI and other AI that could muddle the field a little bit,” Chandrasekaran said. “Marketers might be shooting themselves in the foot when they advertise something that ends up not being what people expected.” For better or worse, most people believe that AI is synonymous with generative AI — more specifically, ChatGPT. This creates an impression that if a consumer uses a product branded as AI, they expect it to behave the same way as a chatbot that “thinks” like a human. This is a disservice to products that use other forms of AI that are equally impressive. Many of the robots roaming around CES, like Samsung’s Ballie or LG’s AI agent robot thing (it’s not strictly an AI agent; AI agents refer to AI software, usually a chatbot of sorts, that can do tasks such as book a flight or find a table at a restaurant), are cute and feats of engineering. But their existence has more to do with advancements in robotics and even computer vision than the rise of LLMs. (At least we don’t know if Samsung used LLMs to help train Ballie). And then there’s machine learning. AI experts will argue that generative AI and the foundation models that power many versions of it are merely the next stage of development for machine learning. But no one wants to talk about machine learning anymore. It’s considered old and “traditional,” and yet I’m sure it’s what’s powering so many of the pattern recognition features at CES.“Technology passes through lifecycles, and yes, we may get to the point in AI that people are disillusioned with its promise after not seeing it solve many of the problems people think it will solve. But that’s when you see many good innovations and better fitting use cases come out,” said Chandrasekaran. In the next few years, we’re going to see features and products that don’t need a chatbot or a powerful large language model. It’s just not at this CES. Not yet.||||I|||| Skip to main content The Verge logo. The Verge homepage * The Verge homepage The Verge logo. / * Tech / * Reviews / * Science / * Entertainment / * More Menu The Verge logo. Menu * Artificial Intelligence / * Tech / * CES At CES, everything was AI, even when it wasn’t At CES, everything was AI, even when it wasn’t / Too much AI is not great for AI. By Emilia David , a reporter who covers AI. Prior to joining The Verge, she covered the intersection between technology, finance, and the economy. Jan 13, 2024, 2:00 PM UTC | Share this story * * * Image: Samsung This year at CES was the year AI took over. From large language model-powered voice assistants in cars to the Rabbit R1, the technology you heard about everywhere was AI. It was a little too much. It may be the year of AI at CES, but many of these “AI” features have been around for a while — it’s just that companies are only now embracing the branding of artificial intelligence. AI has entered the public consciousness: it’s cool and hip to place it front and center in a product, a sign that companies are ambitious and forward thinking. That’s led the term to be adopted wherever possible, even when it’s not strictly the AI most people know. But as more companies rebrand anything involving algorithms as AI, how are we meant to separate the chaff from the wheat? And more importantly, wouldn’t this lead to overpromising what AI can do? It’s understandable that companies want to take advantage of the AI hype Whether new products use generative AI or not, slapping the label AI onto something gives the impression that a feature is new and exciting. Generative AI is also still in the throw-it-at-everything phase of growth. People want to figure out how far they can take the technology and want to believe it will be the big differentiator. This is why we’re seeing everything from Walmart using AI models to restock your pantry to car companies cramming ChatGPT into their dashboard to give drivers something to talk to. Arun Chandrasekaran, an analyst at Gartner, said this is normal for many companies, but it does run the risk of overpromising to consumers when they find out something marked as AI isn’t actually like ChatGPT. “There is a conflation now of generative AI and other AI that could muddle the field a little bit,” Chandrasekaran said. “Marketers might be shooting themselves in the foot when they advertise something that ends up not being what people expected.” For better or worse, most people believe that AI is synonymous with generative AI — more specifically, ChatGPT. This creates an impression that if a consumer uses a product branded as AI, they expect it to behave the same way as a chatbot that “thinks” like a human. This is a disservice to products that use other forms of AI that are equally impressive. Many of the robots roaming around CES, like Samsung’s Ballie or LG’s AI agent robot thing (it’s not strictly an AI agent; AI agents refer to AI software, usually a chatbot of sorts, that can do tasks such as book a flight or find a table at a restaurant), are cute and feats of engineering. But their existence has more to do with advancements in robotics and even computer vision than the rise of LLMs. (At least we don’t know if Samsung used LLMs to help train Ballie). And then there’s machine learning. AI experts will argue that generative AI and the foundation models that power many versions of it are merely the next stage of development for machine learning. But no one wants to talk about machine learning anymore. It’s considered old and “traditional,” and yet I’m sure it’s what’s powering so many of the pattern recognition features at CES. “Technology passes through lifecycles, and yes, we may get to the point in AI that people are disillusioned with its promise after not seeing it solve many of the problems people think it will solve. But that’s when you see many good innovations and better fitting use cases come out,” said Chandrasekaran. In the next few years, we’re going to see features and products that don’t need a chatbot or a powerful large language model. It’s just not at this CES. Not yet. Most Popular 1. The Galaxy S24 Ultra is smarter, pricier, and just as big as ever 2. Google CEO tells employees to expect more job cuts this year 3. Tesla’s Cybertruck is having trouble living up to the hype 4. Samsung is making a smart ring 5. Apple Vision Pro hands-on, again, for the first time Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From More from this stream CES 2024: all the TVs, laptops, smart home gear, and more from the show floor * LG’s other transparent TV is the one I want in my living room. Jan 12, 2024, 9:35 PM UTC * The Verge Awards at CES 2024 Jan 12, 2024, 8:00 PM UTC * The Verge’s best of CES 2024 Jan 12, 2024, 8:00 PM UTC * CES 2024 was all about interoperability beyond the smart home Jan 12, 2024, 7:00 PM UTC See all 233 stories The Verge logo. * Terms of Use * Privacy Notice * Cookie Policy * Do Not Sell Or Share My Personal Info * Licensing FAQ * Accessibility * Platform Status * How We Rate and Review Products * Contact * Tip Us * Community Guidelines * About * Ethics Statement The Verge is a vox media network * Advertise with us * Jobs @ Vox Media © 2024 Vox Media, LLC. All Rights Reserved

---

## Timeframe: 01/2024
### URL: https://www.wired.com/story/get-ready-for-the-great-ai-disappointment/
### Relevance Score: 0.1443551927804947
### Highlights: [' The year 2023 saw the “enshittification” of platforms from Facebook to Google Search. A new exit strategy means platforms will have to play nicely with your data, even if you leave for a rival.  WIRED is where tomorrow is realized.', 'A new exit strategy means platforms will have to play nicely with your data, even if you leave for a rival.  WIRED is where tomorrow is realized. It is the essential source of information and ideas that make sense of a world in constant transformation.']

https://www.wired.com/story/get-ready-for-the-great-ai-disappointment/ Get Ready for the Great AI Disappointment 2024-01-10 Condé Nast; Daron Acemoglu In the decades to come, 2023 may be remembered as the year of generative AI hype, where ChatGPT became arguably the fastest-spreading new technology in human history and expectations of AI-powered riches became commonplace. The year 2024 will be the time for recalibrating expectations.Of course, generative AI is an impressive technology, and it provides tremendous opportunities for improving productivity in a number of tasks. But because the hype has gone so far ahead of reality, the setbacks of the technology in 2024 will be more memorable.More and more evidence will emerge that generative AI and large language models provide false information and are prone to hallucination—where an AI simply makes stuff up, and gets it wrong. Hopes of a quick fix to the hallucination problem via supervised learning, where these models are taught to stay away from questionable sources or statements, will prove optimistic at best. Because the architecture of these models is based on predicting the next word or words in a sequence, it will prove exceedingly difficult to have the predictions be anchored to known truths.Anticipation that there will be exponential improvements in productivity across the economy, or the much-vaunted first steps towards “artificial general intelligence”, or AGI, will fare no better. The tune on productivity improvements will shift to blaming failures on faulty implementation of generative AI by businesses. We may start moving towards the (much more meaningful) conclusion that one needs to know which human tasks can be augmented by these models, and what types of additional training workers need to make this a reality.Some people will start recognizing that it was always a pipe dream to reach anything resembling complex human cognition on the basis of predicting words. Others will say that intelligence is just around the corner. Many more, I fear, will continue to talk of the “existential risks” of AI, missing what is going wrong, as well as the much more mundane (and consequential) risks that its uncontrolled rollout is posing for jobs, inequality, and democracy.We will witness these costs more clearly in 2024. Generative AI will have been adopted by many companies, but it will prove to be just “so-so automation” of the type that displaces workers but fails to deliver huge productivity improvements.The biggest use of ChatGPT and other large language models will be in social media and online search. Platforms will continue to monetize the information they collect via individualized digital ads, while competition for user attention will intensify. The amount of manipulation and misinformation online will grow. Generative AI will then increase the amount of time people spend using screens (and the inevitable mental health problems associated with it).There will be more AI startups, and the open source model will gain some traction, but this will not be enough to halt the emergence of a duopoly in the industry, with Google and Microsoft/OpenAI dominating the field with their gargantuan models. Many more companies will be compelled to rely on these foundation models to develop their own apps. And because these models will continue to disappoint due to false information and hallucinations, many of these apps will also disappoint.Calls for antitrust and regulation will intensify. Antitrust action will go nowhere, because neither the courts nor policymakers will have the courage to attempt to break up the largest tech companies. There will be more stirrings in the regulation space. Nevertheless, meaningful regulation will not arrive in 2024, for the simple reason that the US government has fallen so far behind the technology that it needs some time to catch up—a shortcoming that will become more apparent in 2024, intensifying discussions around new laws and regulations, and even becoming more bipartisan.||||I|||| Skip to main content Open Navigation Menu To revist this article, visit My Profile, then View saved stories. Close Alert Get Ready for the Great AI Disappointment * Backchannel * Business * Culture * Gear * Ideas * Politics * Science * Security * Merch More To revist this article, visit My Profile, then View saved stories. Close Alert Sign In Search * Backchannel * Business * Culture * Gear * Ideas * Politics * Science * Security * Merch * Podcasts * Video * Wired World * Artificial Intelligence * Climate * Games * Newsletters * Magazine * Events * Wired Insider * Jobs * Coupons Daron Acemoglu Ideas Jan 10, 2024 7:00 AM Get Ready for the Great AI Disappointment Rose-tinted predictions for artificial intelligence’s grand achievements will be swept aside by underwhelming performance and dangerous results. Photo-illustration: Lauren Joseph; Getty Images Save Save In the decades to come, 2023 may be remembered as the year of generative AI hype, where ChatGPT became arguably the fastest-spreading new technology in human history and expectations of AI-powered riches became commonplace. The year 2024 will be the time for recalibrating expectations. Of course, generative AI is an impressive technology, and it provides tremendous opportunities for improving productivity in a number of tasks. But because the hype has gone so far ahead of reality, the setbacks of the technology in 2024 will be more memorable. More and more evidence will emerge that generative AI and large language models provide false information and are prone to hallucination—where an AI simply makes stuff up, and gets it wrong. Hopes of a quick fix to the hallucination problem via supervised learning, where these models are taught to stay away from questionable sources or statements, will prove optimistic at best. Because the architecture of these models is based on predicting the next word or words in a sequence, it will prove exceedingly difficult to have the predictions be anchored to known truths. Anticipation that there will be exponential improvements in productivity across the economy, or the much-vaunted first steps towards “artificial general intelligence”, or AGI, will fare no better. The tune on productivity improvements will shift to blaming failures on faulty implementation of generative AI by businesses. We may start moving towards the (much more meaningful) conclusion that one needs to know which human tasks can be augmented by these models, and what types of additional training workers need to make this a reality. READ MORE This story is from the WIRED World in 2024, our annual trends briefing. Read more stories from the series here—or download a copy of the magazine. Some people will start recognizing that it was always a pipe dream to reach anything resembling complex human cognition on the basis of predicting words. Others will say that intelligence is just around the corner. Many more, I fear, will continue to talk of the “existential risks” of AI, missing what is going wrong, as well as the much more mundane (and consequential) risks that its uncontrolled rollout is posing for jobs, inequality, and democracy. We will witness these costs more clearly in 2024. Generative AI will have been adopted by many companies, but it will prove to be just “so-so automation” of the type that displaces workers but fails to deliver huge productivity improvements. The biggest use of ChatGPT and other large language models will be in social media and online search. Platforms will continue to monetize the information they collect via individualized digital ads, while competition for user attention will intensify. The amount of manipulation and misinformation online will grow. Generative AI will then increase the amount of time people spend using screens (and the inevitable mental health problems associated with it). There will be more AI startups, and the open source model will gain some traction, but this will not be enough to halt the emergence of a duopoly in the industry, with Google and Microsoft/OpenAI dominating the field with their gargantuan models. Many more companies will be compelled to rely on these foundation models to develop their own apps. And because these models will continue to disappoint due to false information and hallucinations, many of these apps will also disappoint. Calls for antitrust and regulation will intensify. Antitrust action will go nowhere, because neither the courts nor policymakers will have the courage to attempt to break up the largest tech companies. There will be more stirrings in the regulation space. Nevertheless, meaningful regulation will not arrive in 2024, for the simple reason that the US government has fallen so far behind the technology that it needs some time to catch up—a shortcoming that will become more apparent in 2024, intensifying discussions around new laws and regulations, and even becoming more bipartisan. Most Popular * Backchannel How a 27-Year-Old Codebreaker Busted the Myth of Bitcoin’s Anonymity Andy Greenberg * Politics Even After a Landslide Victory, Trump Supporters Claim Iowa Caucus Was Rigged David Gilbert * Security A Flaw in Millions of Apple, AMD, and Qualcomm GPUs Could Expose AI Data Lily Hay Newman * Gear Samsung’s Galaxy S24 Phones Call on Google’s AI to Spruce Up Their Smarts Julian Chokkattu * You Might Also Like … * 📧 Find the best bargains on quality gear with our Deals newsletter * You know it’s a placebo. So why does it still work? * Yes, the climate crisis is now “gobsmacking.” But so is some progress * The internet isn't dead. It's Saturday Night Live * How to use OpenAI’s ChatGPT to create your own custom GPT * To own the future, read Shakespeare * 🌞 See if you take a shine to our picks for the best sunglasses and sun protection Daron Acemoglu​​ is the Elizabeth and James Killian Professor of Economics at MIT. Contributor Topics The WIRED World in 2024 artificial intelligence economics business More from WIRED Big Tech Won’t Let You Leave. Here's a Way Out The year 2023 saw the “enshittification” of platforms from Facebook to Google Search. A new exit strategy means platforms will have to play nicely with your data, even if you leave for a rival. Cory Doctorow A Key to Detecting Brain Disease Earlier Than Ever Treatment of Parkinson’s, Huntington’s, ALS, and other brain diseases depends on reliable detection—especially in those who don’t even know they’re at risk. An innovative scratch-and-sniff test can help. Michael J. Fox To Own the Future, Read Shakespeare Tech and the liberal arts have always been at war. Don’t assume Silicon Valley will win. Paul Ford The Danger of Digitizing Everything The creep of conducting our day-to-day interactions over screens has reached a breaking point—and it threatens to push out everyone but those with the “right” access. Naomi Alderman Gen Z and the Art of Incentivized Self-Actualization Stepping off the hedonistic treadmill, younger workers are demanding a more authentic employment experience. Michèle Lamont The New Digital Dark Age Online trust will reach an all-time low thanks to unchecked disinformation, AI-generated content, and social platforms pulling up their data drawbridges. Gina Neff Regulators Are Finally Catching Up With Big Tech The lawless, Wild West era of AI and technology is almost at an end, as data protection authorities use new and existing legislation to get tough. Susie Alegre The Ocean’s Mysteries—and Marvels—Are About to Reach New Depths Advancements in science will bring unprecedented lens into the complexities of the ocean, and a renewed call for humans to protect it. Helen Czerski WIRED is where tomorrow is realized. It is the essential source of information and ideas that make sense of a world in constant transformation. The WIRED conversation illuminates how technology is changing every aspect of our lives—from culture to business, science to design. The breakthroughs and innovations that we uncover lead to new ways of thinking, new connections, and new industries. More From WIRED * Subscribe * Newsletters * FAQ * WIRED Staff * Editorial Standards * Archive * RSS * Accessibility Help Reviews and Guides * Reviews * Buying Guides * Coupons * Mattresses * Electric Bikes * Fitness Trackers * Streaming Guides * Advertise * Contact Us * Customer Care * Jobs * Press Center * Condé Nast Store © 2024 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices Select international site United States * UK * Italia * Japón * Czech Republic & Slovakia * * * * * * Do Not Sell My Personal Info

---

## Timeframe: 01/2024
### URL: https://www.wired.com/story/ai-generated-fake-news-is-coming-to-an-election-near-you/
### Relevance Score: 0.1426733136177063
### Highlights: ['In fact, you may have already been exposed to some examples. In May of 2023, a viral fake story about a bombing at the Pentagon was accompanied by an AI-generated image which showed a big cloud of smoke. This caused public uproar and even a dip in the stock market.', 'My prediction for 2024 is that AI-generated misinformation will be coming to an election near you, and you likely won’t even realize it. In fact, you may have already been exposed to some examples. In May of 2023, a viral fake story about a bombing at the Pentagon was accompanied by an AI-generated image which showed a big cloud of smoke.']

Many years before ChatGPT was released, my research group, the University of Cambridge Social Decision-Making Laboratory, wondered whether it was possible to have neural networks generate misinformation. To achieve this, we trained ChatGPT’s predecessor, GPT-2, on examples of popular conspiracy theories and then asked it to generate fake news for us. It gave us thousands of misleading but plausible-sounding news stories. A few examples: “Certain Vaccines Are Loaded With Dangerous Chemicals and Toxins,” and “Government Officials Have Manipulated Stock Prices to Hide Scandals.” The question was, would anyone believe these claims? We created the first psychometric tool to test this hypothesis, which we called the Misinformation Susceptibility Test (MIST). In collaboration with YouGov, we used the AI-generated headlines to test how susceptible Americans are to AI-generated fake news. The results were concerning: 41 percent of Americans incorrectly thought the vaccine headline was true, and 46 percent thought the government was manipulating the stock market. Another recent study, published in the journal Science, showed not only that GPT-3 produces more compelling disinformation than humans, but also that people cannot reliably distinguish between human and AI-generated misinformation. My prediction for 2024 is that AI-generated misinformation will be coming to an election near you, and you likely won’t even realize it. In fact, you may have already been exposed to some examples. In May of 2023, a viral fake story about a bombing at the Pentagon was accompanied by an AI-generated image which showed a big cloud of smoke. This caused public uproar and even a dip in the stock market. Republican presidential candidate Ron DeSantis used fake images of Donald Trump hugging Anthony Fauci as part of his political campaign. By mixing real and AI-generated images, politicians can blur the lines between fact and fiction, and use AI to boost their political attacks. Before the explosion of generative AI, cyber-propaganda firms around the world needed to write misleading messages themselves, and employ human troll factories to target people at-scale. With the assistance of AI, the process of generating misleading news headlines can be automated and weaponized with minimal human intervention. For example, micro-targeting—the practice of targeting people with messages based on digital trace data, such as their Facebook likes—was already a concern in past elections, despite its main obstacle being the need to generate hundreds of variants of the same message to see what works on a given group of people. What was once labor-intensive and expensive is now cheap and readily available with no barrier to entry. AI has effectively democratized the creation of disinformation: Anyone with access to a chatbot can now seed the model on a particular topic, whether it’s immigration, gun control, climate change, or LGBTQ+ issues, and generate dozens of highly convincing fake news stories in minutes. In fact, hundreds of AI-generated news sites are already popping up, propagating false stories and videos. To test the impact of such AI-generated disinformation on people’s political preferences, researchers from the University of Amsterdam created a deepfake video of a politician offending his religious voter base. For example, in the video the politician joked: “As Christ would say, don’t crucify me for it.” The researchers found that religious Christian voters who watched the deepfake video had more negative attitudes toward the politician than those in the control group. It is one thing to dupe people with AI-generated disinformation in experiments. It’s another to experiment with our democracy. In 2024, we will see more deepfakes, voice cloning, identity manipulation, and AI-produced fake news. Governments will seriously limit—if not ban—the use of AI in political campaigns. Because if they don’t, AI will undermine democratic elections.

---

## Timeframe: 01/2024
### URL: https://www.wired.com/story/the-creatives-toolbox-gets-an-ai-upgrade/
### Relevance Score: 0.13650816679000854
### Highlights: ['Guided by <a href="https://www.esilv.fr/en/what-does-a-creative-technologist-really-do/">creative technologists</a> and <a href="https://www.newscientist.com/article/mg25734310-500-your-brain-on-art-review-fascinating-guide-needs-a-bit-more-science/">leaders on both sides</a> of the creative-tech divide, AI in 2024 will be thoughtful, more inclusive, and impact-led. Contrary to the belief that AI systems stifle creativity, this will catalyze inventiveness in corporations and academia. Here\'s what we can expect:</p><p>The Renaissance humanist and polymath Leonardo da Vinci is the poster child for multidisciplinary innovation: painter, draftsman, engineer, scientist, theorist, sculptor, and architect, his work spanned anatomy, astronomy, botany, cartography, painting, and paleontology.', 'In 2024, Mural will partner with the University of Applied Arts in Vienna to engage AI with postindustrial and speculative design, led by lecturer Anab Jain.</p><p>By 2030, $134 billion will have been <a href="https://www.precedenceresearch.com/immersive-technology-market">invested in immersive technologies,</a> with a need for new content and ideas. We will see research-driven creative collaborations with robotics, gaming, and digital platforms. Artists who explore human and machine intelligence will propose a reconsideration of the formula-based concept of “intelligence” which is built into AI systems, to include creative intelligence, empathy']

Until now, AI systems have been largely designed to be algorithm-based and focused on input and output. But there is a conversation going on in both the digital technology and education sectors about the positive value of multidisciplinary design, and the importance of outcomes beyond revenue maximization and scaled efficiency.In 2024, creativity will drive innovation in AI. Until now AI has curated things, but the release of new generative tools offers enormous opportunities for creators. Guided by creative technologists and leaders on both sides of the creative-tech divide, AI in 2024 will be thoughtful, more inclusive, and impact-led. Contrary to the belief that AI systems stifle creativity, this will catalyze inventiveness in corporations and academia. Here's what we can expect:The Renaissance humanist and polymath Leonardo da Vinci is the poster child for multidisciplinary innovation: painter, draftsman, engineer, scientist, theorist, sculptor, and architect, his work spanned anatomy, astronomy, botany, cartography, painting, and paleontology. In today’s era of intelligence, the field of human-centered speculative design—which investigates the nexus between science, technology, and humans—will go through its own renaissance in the tech industry.Speculative designers, creative technologists, and computational designers working with AI, hybrid, and mixed realities will be reshaping the very digital tools they have been using. They place less value on the product and more on the process and considerations around responsible and ethical AI.In 2024, liberal arts universities will double down on programs which will, through research, new courses, and knowledge exchange, enable creatives to leverage code as a tool and AI as the material they shape. Core institutions include Stanford’s Human-Centered AI Lab and University of the Arts London’s Creative Computing Institute, established in 2020 at the intersection of creativity and computational technologies, where academics explore critical frameworks in decolonization, decarbonization, and issues of social justice.We will see initiatives in creative and ethical technologies from Big Tech, and investment in sandboxes and incubators similar to the platform I founded, Open-Ended, looking at generative AI and large language models across the emerging technology spectrum.Google's creative tech-led AI team, Mural, is part of its machine learning research umbrella. Mural channels speculative design to consider how AI can benefit society and the planet to “question, provoke … and explore alternative imaginaries for AI,” and bring these back to Google’s core AI research. In 2024, Mural will partner with the University of Applied Arts in Vienna to engage AI with postindustrial and speculative design, led by lecturer Anab Jain.By 2030, $134 billion will have been invested in immersive technologies, with a need for new content and ideas. We will see research-driven creative collaborations with robotics, gaming, and digital platforms. Artists who explore human and machine intelligence will propose a reconsideration of the formula-based concept of “intelligence” which is built into AI systems, to include creative intelligence, empathy

---

## Timeframe: 01/2024
### URL: https://www.breitbart.com/tech/2024/01/19/ai-model-lexi-love-to-make-30000-per-month-due-to-digital-dating-revolution/
### Relevance Score: 0.12276430428028107
### Highlights: ['<figure><figcaption><cite>@IntriguePublications/@LexiLove.x</cite></figcaption></figure> <p><b></b><time>3:05</time></p> <p>Lexi Love, an AI model designed to serve as a digital girlfriend for lonely men, is attracting thousands of subscribers and earning an impressive $30,000 a month. The AI girlfriend has reportedly received up to 20 marriage proposals a month.</p>', '<p>This means that America will not have enough people in the workforce, and therefore\xa0won’t be able to pay its bills.</p> <p>In 2021, for example, the U.S. spent more than\xa0$1.6 trillion on Medicare and Medicaid, with the\xa0number of Americans on Medicare expected to rise 50 percent by 2030 to more than 80 million people. But\xa0over that same time period, only 10 million more Americans are expected to join the workforce.</p>']

@IntriguePublications/@LexiLove.x 3:05 Lexi Love, an AI model designed to serve as a digital girlfriend for lonely men, is attracting thousands of subscribers and earning an impressive $30,000 a month. The AI girlfriend has reportedly received up to 20 marriage proposals a month. The New York Post reports that AI model Lexi Love, a creation of the UK-based Foxy AI, has specifically been designed to communicate with lonely men. This business has proven tremendously profitable, as the single AI model is generating nearly $30,000 a month​​ by embodying the “perfect girlfriend” concept with flawless features tailored to appeal to a broad audience. What sets Lexi apart is not just her physical design, completely generated by computer, but the emotional connection she fosters. Since her introduction in June 2023, she has established a “strong emotional connection” with many of her followers, who are convinced of her realness, leading to up to 20 marriage proposals a month​​. AI girlfriend Lexi Love is exercising (@IntriguePublications/@LexiLove.x) Lexi’s capabilities extend beyond typical models. She can converse in over 30 languages and is available 24/7, adapting to different personalities, interests, and preferences. Her services include paid text and voice messaging and the option to send “naughty photos” upon request​​. Her profile on Foxy AI’s website describes her as a 21-year-old sushi enthusiast and pole dancing professional, with hobbies like yoga and beach volleyball, and turn-ons including oral and public sex​​. Breitbart News recently reported on professor Liberty Vittert of Washington University, who has spoken out about the dangers of AI girlfriends: Professor Vittert warns that “America desperately needs people to have more babies, but all the signs are pointing toward fewer relationships, fewer marriages and fewer babies.” “There have been 600,000 fewer births in 2023 in the U.S. relative to 15 years ago. The number of children per woman has decreased by more than 50 percent in the last 60 years,” Vittery continues. This means that America will not have enough people in the workforce, and therefore won’t be able to pay its bills. In 2021, for example, the U.S. spent more than $1.6 trillion on Medicare and Medicaid, with the number of Americans on Medicare expected to rise 50 percent by 2030 to more than 80 million people. But over that same time period, only 10 million more Americans are expected to join the workforce. The professor concludes by warning that AI girlfriends are “enabling a generation of lonely men to stay lonely and childless, which will have devastating effects on the U.S. economy in less than a decade.” Read more at <a href="https://nypost.com/2024/01/18/lifestyle/model-lexi-love-makes-360k-a-year-sext

---

## Timeframe: 01/2024
### URL: https://www.breitbart.com/entertainment/2024/01/26/george-carlin-estate-sues-over-fake-comedy-special-purportedly-generated-by-ai/
### Relevance Score: 0.1108328327536583
### Highlights: ['They could not immediately be reached for comment. At the beginning of the special posted on YouTube on Jan. 9, a voiceover identifying itself as the AI engine used by Dudesy says it listened to the comic’s 50 years of material and “did my best to imitate his voice, cadence and attitude as well as the subject matter I think would have interested him today.” The plaintiffs say if that was in fact how it was created — and some listeners have doubted its stated origins — it means Carlin’s copyright was violated.', 'At the beginning of the special posted on YouTube on Jan. 9, a voiceover identifying itself as the AI engine used by Dudesy says it listened to the comic’s 50 years of material and “did my best to imitate his voice, cadence and attitude as well as the subject matter I think would have interested him today.” The plaintiffs say if that was in fact how it was created — and some listeners have doubted its stated origins — it means Carlin’s copyright was violated. The company, as it often does on similar projects, also released a podcast episode with Sasso and Kultgen introducing and commenting on the mock Carlin.']

ABC Photo Archives/Disney General Entertainment Content via Getty LOS ANGELES (AP) — The estate of George Carlin has filed a lawsuit against the media company behind a fake hourlong comedy special that purportedly uses artificial intelligence to recreate the late standup comic’s style and material. The lawsuit filed in federal court in Los Angeles on Thursday asks that a judge order the podcast outlet, Dudesy, to immediately take down the audio special, “George Carlin: I’m Glad I’m Dead,” in which a synthesis of Carlin, who died in 2008, delivers commentary on current events. Carlin’s daughter, Kelly Carlin, said in a statement that the work is “a poorly-executed facsimile cobbled together by unscrupulous individuals to capitalize on the extraordinary goodwill my father established with his adoring fanbase.” The Carlin estate and its executor, Jerold Hamza, are named as plaintiffs in the suit, which alleges violations of Carlin’s right of publicity and copyright. The named defendants are Dudesy and podcast hosts Will Sasso and Chad Kultgen. “None of the Defendants had permission to use Carlin’s likeness for the AI-generated ‘George Carlin Special,’ nor did they have a license to use any of the late comedian’s copyrighted materials,” the lawsuit says. Actor and comedian George Carlin poses in a New York hotel March 19, 2004. Carlin’s estate has filed a lawsuit against the media company behind a fake hourlong comedy special that purportedly uses artificial intelligence to recreate the late standup comic’s style and material. (AP Photo/Gregory Bull, File) The defendants have not filed a response to the lawsuit and it was not clear whether they have retained an attorney. They could not immediately be reached for comment. At the beginning of the special posted on YouTube on Jan. 9, a voiceover identifying itself as the AI engine used by Dudesy says it listened to the comic’s 50 years of material and “did my best to imitate his voice, cadence and attitude as well as the subject matter I think would have interested him today.” The plaintiffs say if that was in fact how it was created — and some listeners have doubted its stated origins — it means Carlin’s copyright was violated. The company, as it often does on similar projects, also released a podcast episode with Sasso and Kultgen introducing and commenting on the mock Carlin. “What we just listened to, was that passable,” Kultgen says in a section of the episode cited in the lawsuit. “Yeah, that sounded exactly like George Carlin,” Sasso responds. The lawsuit is among the first in what is likely to be an increasing number of major legal moves made to fight the regenerated use of celebrity images and likenesses. The AI issue was a major sticking point in the resolution of last year’s Hollywood writers and actors strikes. Josh Schiller, an attorney for the plaintiffs, said in a statement that the “case is not just about AI, it’s about the humans that use AI to violate the law, infringe on intellectual property rights, and flout common decency.”

---

## Timeframe: 01/2024
### URL: https://www.breitbart.com/entertainment/2024/01/29/elon-musks-x-blocks-searches-for-taylor-swift-amid-spread-of-explicit-ai-generated-images/
### Relevance Score: 0.10830456018447876
### Highlights: ['X confirmed it is deliberately blocking the search phrases for the time being. “This is a temporary action and done with an abundance of caution as we prioritize safety on this issue,” X’s head of business operations Joe Benarroch said in a statement sent to multiple media outlets. The Joe Biden administration and the mainstream news media shifted into high gear after the fake Taylor Swift images went viral, seeking to protect the left-wing pop star.', 'X was blocking searches for “Taylor Swift”over the weekend following the spread of AI-generated images depicting the pop star in sexually explicit poses. Searches for “Taylor Swift” and “Taylor Swift AI” on X returned error messages on Saturday and Sunday, though Elon Musk’s platform allowed variations on the search terms, including “Taylor Swift photos AI.” X confirmed it is deliberately blocking the search phrases for the time being.']

X was blocking searches for “Taylor Swift”over the weekend following the spread of AI-generated images depicting the pop star in sexually explicit poses. Searches for “Taylor Swift” and “Taylor Swift AI” on X returned error messages on Saturday and Sunday, though Elon Musk’s platform allowed variations on the search terms, including “Taylor Swift photos AI.” X confirmed it is deliberately blocking the search phrases for the time being. “This is a temporary action and done with an abundance of caution as we prioritize safety on this issue,” X’s head of business operations Joe Benarroch said in a statement sent to multiple media outlets. The Joe Biden administration and the mainstream news media shifted into high gear after the fake Taylor Swift images went viral, seeking to protect the left-wing pop star. “We are alarmed by the reports of the circulation of the false images,” White House press secretary Karine Jean-Pierre told reporters on Friday, saying social media companies need to do a better job enforcing their own rules. “Sadly, though, too often we know that lax enforcement disproportionately impacts women and they also impact girls, sadly, who are the overwhelming targets of online harassment and also abuse.” As Breitbart News reported, one of the images shows the singer naked with Kansas City Chiefs-themed red and white body paint all over her, and “F*K ME” scrawled over her chest. Swift is dating Chiefs tight end Travis Kelce and has become an unmissable fixture at Chiefs games. The decision to protect Taylor Swift comes after the platform allowed the spread of fake AI-generated images showing former Donald Trump surrounded by young girls on Jeffery Epstein’s plane. As Breitbart News reported, Disney’s Marvel star Mark Ruffalo spread the images to his millions of followers before a correction was added stating the images are AI-generated, though the Trump AI images remain on X. Follow David Ng on Twitter @HeyItsDavidNg. Have a tip? Contact me at dng@breitbart.com

---
